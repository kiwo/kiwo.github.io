<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>

    <meta name="description" content="Software development using .NET, C#, SQL, Javascript and related technologies" />

    <title>Mikhail Shilkov</title>
    <meta name="author" content="Mikhail Shilkov">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="twitter:card" content="summary_large_image"></meta>
    <meta name="twitter:creator" content="@MikhailShilkov"></meta>
    <meta name="twitter:title" content="Mikhail Shilkov"></meta>

    <meta property="og:type" content="article" />
    <meta property="og:title" content="Mikhail Shilkov" />
    <meta property="og:url" content="https://mikhail.io/" />




    <link href="/feed/" rel="alternate" title="mikhail.io" type="application/atom+xml">
    <link href="/favicon.ico?v=2" rel="shortcut icon">

    <!-- Bootstrap -->
    <link href="/styles/site.css" rel="stylesheet" media="screen">
    <link href="/vendor/prism.css" rel="stylesheet" media="screen">

    <meta name="generator" content="DocPad v6.80.6" />
    
</head>
<body>

<div class="navbar navbar-default navbar-static-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">
                <span class="text-primary">Mikhail Shilkov</span><br />
                <span class="elevator-pitch">Serverless, Azure, FP, F# and more</span>
            </a>
        </div>
        <div class="collapse navbar-collapse navbar-right">
            <ul class="nav navbar-nav">
                <!--<li><a href="/">Blog</a></li>-->
                
                    <li><a href="/tags/">Topics</a></li>
                
                    <li><a href="/archives/">Archives</a></li>
                
                    <li><a href="/talks/">Talks</a></li>
                
                    <li><a href="/about/">About</a></li>
                
                <li class="hidden-xs">
                    <a href="/feed/" class="rss"><span class="icon icon-feed"></span></a>
                    <a href="https://www.linkedin.com/in/mikhailshilkov" class="linkedin"><span class="icon icon-linkedin"></span></a>
                    <a href="https://twitter.com/mikhailshilkov" class="twitter"><span class="icon icon-twitter"></span></a>
                    <a href="https://github.com/mikhailshilkov" class="github"><span class="icon icon-github"></span></a>
                </li>
            </ul>
            <form class="navbar-form navbar-right hidden-xs" role="search" action="https://google.com/search"
                  method="get">
                <div class="form-group">
                    <input type="search" name="q" class="form-control" placeholder="Search">
                    <input type="hidden" name="q" value="site:mikhail.io">
                </div>
            </form>
        </div>
    </div>
</div>
<div class="container">
    
    <article class="post">
    <div class="post-date">Nov 19th, 2018</div>
    
    <h1><a href='/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/'>From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a></h1>
    

    
        <div class="remark">Originally published at <a href='https://blog.binaris.com/from-0-to-1000-instances/'>Binaris Blog</a></div>
    

    <div class="post-content">
        <p>Whenever I see a &quot;Getting Started with Function-as-a-Service&quot; tutorial, it usually shows off 
a synchronous HTTP-triggered scenario. In my projects, though, I use a lot of asynchronous 
functions triggered by a queue or an event stream.</p>
<p>Quite often, the number of messages passing through a queue isn&#39;t uniform over time. I might 
drop batches of work now and then. My app may get piles of queue items arriving from upstream 
systems that were down or under maintenance for an extended period. The system might see some 
rush-hour peaks every day or only a few busy days per month.</p>
<p>This is where serverless tech shines: You pay per execution, and then the promise is that the 
provider takes care of scaling up or down for you. Today, I want to put this scalability under 
test.</p>
<p>The goal of this article is to explore queue-triggered serverless functions and hopefully distill 
some practical advice regarding asynchronous functions for real projects. I will be evaluating 
the problem:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different types of workloads</li>
<li>For different performance tiers</li>
</ul>
<p>Let&#39;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some crucial factors and parameters 
that influence the outcome. My interpretation might be wrong. The results might change over time. 
If you happen to know a way to improve my tests, please let me know, and I will re-run them and 
re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>In this article I analyze the execution results of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via SQS queues</li>
<li>Azure Function triggered via Storage queues</li>
<li>Google Cloud Function triggered via Cloud Pub/Sub</li>
</ul>
<p>All functions are implemented in Javascript and are running on GA runtime.</p>
<p>At the beginning of each test, I threw 100,000 messages into a queue that was previously idle. 
Enqueuing never took longer than one minute (I sent the messages from multiple clients in 
parallel).</p>
<p>I disabled any batch processing, so each message was consumed by a separate function invocation.</p>
<p>I then analyzed the logs (AWS CloudWatch, Azure Application Insights, and GCP Stackdriver 
Logging) to generate charts of execution distribution over time.</p>
<h2 id="how-scaling-actually-works">How Scaling Actually Works</h2>
<p>To understand the experiment better, let&#39;s look at a very simplistic but still useful model of 
how cloud providers scale serverless applications.</p>
<p>All providers handle the increased load by 
<a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">scaling out</a>, i.e., 
by creating multiple instances of the same application that execute the chunks of work in 
parallel.</p>
<p>In theory, a cloud provider could spin up an instance for each message in the queue as soon as 
the messages arrive. The backlog processing time would then stay very close to zero.</p>
<p>In practice, allocating instances is not cheap. The Cloud provider has to boot up the function 
runtime, hit a <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">cold start</a>, and waste 
expensive resources on a job that potentially will take just a few milliseconds.</p>
<p>So the providers are trying to find a sweet spot between handling the work as soon as possible 
and using resources efficiently. The outcomes differ, which is the point of my article.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda defines scale out with a notion of Concurrent Executions. Each instance of your AWS 
Lambda is handling a single execution at any given time. In our case, it&#39;s processing a single 
SQS message.</p>
<p>It&#39;s helpful to think of a function instance as a container working on a single task. If execution 
pauses or waits for an external I/O operation, the instance is on hold.</p>
<p>The model of concurrent executions is universal to all trigger types supported by Lambdas. An 
instance doesn&#39;t work with event sources directly; it just receives an event to work on.</p>
<p>There is a central element in the system, let&#39;s call it &quot;Orchestrator&quot;. The Orchestrator is the 
component talking to an SQS queue and getting the messages from it. It&#39;s then the job of the 
Orchestrator and related infrastructure to provision the required number of instances for working 
on concurrent executions:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-queue-scaling.png" alt="Model of AWS Lambda Scale-Out"></p>
<center class="img-caption">Model of AWS Lambda Scale-Out</center>

<p>As to scaling behavior, here is what the official 
<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/scaling.html">AWS docs</a> say:</p>
<blockquote>
<p>AWS Lambda automatically scales up ... until the number of concurrent function executions 
reaches 1000 ... Amazon Simple Queue Service supports an initial burst of 5 concurrent function 
invocations and increases concurrency by 60 concurrent invocations per minute.</p>
</blockquote>
<h3 id="gcp">GCP</h3>
<p>The model of Google Cloud Functions is very similar to what AWS does. It runs a single 
simultaneous execution per instance and routes the messages centrally.</p>
<p>I wasn&#39;t able to find any scaling specifics except the definition of 
<a href="https://cloud.google.com/functions/quotas">Function Quotas</a>.</p>
<h3 id="azure">Azure</h3>
<p>Experiments with Azure Functions were run on 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
&mdash;the dynamically scaled and billed-per-execution runtime. The concurrency model of Azure Functions 
is different from the counterparts of AWS/GCP.</p>
<p>Function App instance is closer to a VM than a single-task container. It runs multiple concurrent 
executions in parallel. Equally importantly, it pulls messages from the queue on its own instead of 
getting them pushed from a central Orchestrator.</p>
<p>There is still a central coordinator called Scale Controller, but its role is a bit more subtle. It 
connects to the same data source (the queue) and needs to determine how many instances to provision 
based on the metrics from that queue:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-scaling.png" alt="Model of Azure Function Scale-Out"></p>
<center class="img-caption">Model of Azure Function Scale-Out</center>

<p>This model has pros and cons. If one execution is idle, waiting for some I/O operation such as an 
HTTP request to finish, the instance might become busy processing other messages, thus being more 
efficient. Running multiple executions is useful in terms of shared resource utilization, e.g., 
keeping database connection pools and reusing HTTP connections.</p>
<p>On the flip side, the Scale Controller now needs to be smarter: to know not only the queue backlog 
but also how instances are doing and at what pace they are processing the messages. It&#39;s probably 
achievable based on queue telemetry though.</p>
<p>Let&#39;s start applying this knowledge in practical experiments.</p>
<h2 id="pause-the-world-workload">Pause-the-World Workload</h2>
<p>My first serverless function is aimed to simulate I/O-bound workloads without using external 
dependencies to keep the experiment clean. Therefore, the implementation is extremely 
straightforward: pause for 500 ms and return.</p>
<p>It could be loading data from a scalable third-party API. It could be running a database query. 
Instead, it just runs <code>setTimeout</code>.</p>
<p>I sent 100k messages to queues of all three cloud providers and observed the result.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda allows multiple instance sizes to be provisioned. Since the workload is neither CPU- 
nor memory-intensive, I was using the smallest memory allocation of 128 MB.</p>
<p>Here comes the first chart of many, so let&#39;s learn to read it. The horizontal axis shows time in 
minutes since all the messages were sent to the queue.</p>
<p>The line going from top-left to bottom-right shows the decreasing queue backlog. Accordingly, the 
left vertical axis denotes the number of items still-to-be-handled.</p>
<p>The bars show the number of concurrent executions crunching the messages at a given time. Every 
execution logs the instance ID so that I could derive the instance count from the logs. The right 
vertical axis shows the instance number.</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-iobound-scaling.png" alt="AWS Lambda processing 100k SQS messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">AWS Lambda processing 100k SQS messages with "Pause" handler</center>

<p>It took AWS Lambda 5.5 minutes to process the whole batch of 100k messages. For comparison, the 
same batch processed sequentially would take about 14 hours.</p>
<p>Notice how linear the growth of instance count is. If I apply the official scaling formula:</p>
<pre><code>Instance Count = 5 + Minutes * 60 = 5 + 5.5 * 60  = 335</code></pre><p>We get a very close result! Promises kept.</p>
<h3 id="gcp">GCP</h3>
<p>Same function, same chart, same instance size of 128 MB of RAM&mdash;but this time for Google Cloud Functions:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-iobound-scaling.png" alt="Google Cloud Function processing 100k Pub/Sub messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">Google Cloud Function processing 100k Pub/Sub messages with "Pause" handler</center>

<p>Coincidentally, the total amount of instances, in the end, was very close to AWS. The scaling
pattern looks entirely different though: Within the very first minute, there was a burst of 
scaling close to 300 instances, and then the growth got very modest.</p>
<p>Thanks to this initial jump, GCP managed to finish processing almost one minute earlier than AWS.</p>
<h3 id="azure">Azure</h3>
<p>Azure Function doesn&#39;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>The shape of the chart for Azure Functions is very similar, but the instance number growth is 
significantly different:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-iobound-scaling.png" alt="Azure Function processing 100k queue messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">Azure Function processing 100k queue messages with "Pause" handler</center>

<p>The total processing time was a bit faster than AWS and somewhat slower than GCP. Azure Function 
instances process several messages in parallel, so it takes much less of them to do the same amount 
of work.</p>
<p>Instance number growth seems far more linear than bursty.</p>
<h3 id="what-we-learned">What we learned</h3>
<p>Based on this simple test, it&#39;s hard to say if one cloud provider handles scale-out better than 
the others.</p>
<p>It looks like all serverless platforms under stress are making decisions at the resolution of 5-15 
seconds, so the backlog processing delays are likely to be measured in minutes. It sounds quite far 
from the theoretical &quot;close to zero&quot; target but is most likely good enough for the majority of 
applications.</p>
<h2 id="crunching-numbers">Crunching Numbers</h2>
<p>That was an easy job though. Let&#39;s give cloud providers a hard time by executing CPU-heavy workloads 
and see if they survive!</p>
<p>This time, each message handler calculates a <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hash with a cost of 10. One such calculation takes about 200 ms on my laptop.</p>
<h3 id="aws">AWS</h3>
<p>Once again, I sent 100k messages to an SQS queue and recorded the processing speed and instance count.</p>
<p>Since the workload is CPU-bound, and AWS allocates CPU shares proportionally to the allocated 
memory, the instance size might have a significant influence on the result.</p>
<p>I started with the smallest memory allocation of 128 MB:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-cpubound-scaling.png" alt="AWS Lambda (128 MB) processing 100k SQS messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">AWS Lambda (128 MB) processing 100k SQS messages with "Bcrypt" handler</center>

<p>This time it took almost 10 minutes to complete the experiment.</p>
<p>The scaling shape is pretty much the same as last time, still correctly described by the formula 
<code>60 * Minutes + 5</code>. However, because AWS allocates a small fraction of a full CPU to each 128 MB 
execution, one message takes around 1,700 ms to complete. Thus, the total work increased 
approximately by the factor of 3 (47 hours if done sequentially).</p>
<p>At the peak, 612 concurrent executions were running, nearly double the amount in our initial 
experiment. So, the total processing time increased only by the factor of 2&mdash;up to 10 minutes.</p>
<p>Let&#39;s see if larger Lambda instances would improve the outcome. Here is the chart for 512 MB of 
allocated memory:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-cpubound-scaling-512.png" alt="AWS Lambda (512 MB) processing 100k SQS messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">AWS Lambda (512 MB) processing 100k SQS messages with "Bcrypt" handler</center>

<p>And yes it does. The average execution duration is down to 400 ms: 4 times less, as expected. 
The scaling shape still holds, so the entire batch was done in less than four minutes.</p>
<h3 id="gcp">GCP</h3>
<p>I executed the same experiment on Google Cloud Functions. I started with 128 MB, and it looks 
impressive:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling.png" alt="Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (128 MB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>The average execution duration is very close to Amazon&#39;s: 1,600 ms. However, GCP scaled more 
aggressively&mdash;to a staggering 1,169 parallel executions! Scaling also has a different shape: 
It&#39;s not linear but grows in steep jumps. As a result, it took less than six minutes on the 
lowest CPU profile&mdash;very close to AWS&#39;s time on a 4x more powerful CPU.</p>
<p>What will GCP achieve on a faster CPU? Let&#39;s provision 512 MB. It must absolutely crush the 
test. Umm, wait, look at that:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling-512.png" alt="Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (512 MB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>It actually... got slower. Yes, the average execution time is 4x lower: 400 ms, but the scaling 
got much less aggressive too, which canceled the speedup.</p>
<p>I confirmed it with the largest instance size of 2,048 MB:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling-2048.png" alt="Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (2 GB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>CPU is fast: 160 ms average execution time, but the total time to process 100k messages went up 
to eight minutes. Beyond the initial spike at the first minute, it failed to scale up any further 
and stayed at about 110 concurrent executions.</p>
<p>It seems that GCP is not that keen to scale out larger instances. It&#39;s probably easier to find 
many small instances available on the pool rather than a similar number of giant instances.</p>
<h3 id="azure">Azure</h3>
<p>A single invocation takes about 400 ms to complete on Azure Function. Here is the burndown chart:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-cpubound-scaling.png" alt="Azure Function processing 100k queue messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Azure Function processing 100k queue messages with "Bcrypt" handler</center>

<p>Azure spent 21 minutes to process the whole backlog. The scaling was linear, similarly to AWS, 
but with a much slower pace regarding instance size growth, about <code>2.5 * Minutes</code>.</p>
<p>As a reminder, each instance could process multiple queue messages in parallel, but each such 
execution would be competing for the same CPU resource, which doesn&#39;t help for the purely 
CPU-bound workload.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<p>Time for some conclusions and pieces of advice to apply in real serverless applications.</p>
<h3 id="serverless-is-great-for-async-data-processing">Serverless is great for async data processing</h3>
<p>If you are already using cloud services, such as managed queues and topics, serverless functions 
are the easiest way to consume them. </p>
<p>Moreover, the scalability is there too. When was the last time you ran 1,200 copies of your 
application?</p>
<h3 id="serverless-is-not-infinitely-scalable">Serverless is not infinitely scalable</h3>
<p>There are limits. Your functions won&#39;t scale perfectly to accommodate your spike&mdash;a 
provider-specific algorithm will determine the scaling pattern.</p>
<p>If you have large spikes in queue workloads, which is quite likely for medium- to high-load 
scenarios, you can and should expect delays up to several minutes before the backlog is fully 
digested.</p>
<p>All cloud providers have quotas and limits that define an upper boundary of scalability.</p>
<h3 id="cloud-providers-have-different-implementations">Cloud providers have different implementations</h3>
<p><strong>AWS Lambda</strong> seems to have a very consistent and well-documented linear scale growth for 
SQS-triggered Lambda functions. It will happily scale to 1,000 instances, or whatever other 
limit you hit first.</p>
<p><strong>Google Cloud Functions</strong> has the most aggressive scale-out strategy for the smallest instance 
sizes. It can be a cost-efficient and scalable way to run your queue-based workloads. Larger 
instances seem to scale in a more limited way, so a further investigation is required if you 
need those.</p>
<p><strong>Azure Functions</strong> share instances for multiple concurrent executions, which works better 
for I/O-bound workloads than for CPU-bound ones. Depending on the exact scenario that you 
have, it might help to play with instance-level settings.</p>
<h3 id="don-t-forget-batching">Don&#39;t forget batching</h3>
<p>For the tests, I was handling queue messages in the 1-by-1 fashion. In practice, it helps 
if you can batch several messages together and execute a single action for all of them in one go.</p>
<p>If the destination for your data supports batched operations, the throughput will usually 
increase immensely. 
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2017/09/19/processing-100000-events-per-second-on-azure-functions/">Processing 100,000 Events Per Second on Azure Functions</a>
is an excellent case to prove the point.</p>
<h3 id="you-might-get-too-much-scale">You might get too much scale</h3>
<p>A month ago, Troy Hunt published a great post 
<a href="https://www.troyhunt.com/breaking-azure-functions-with-too-many-connections/">Breaking Azure Functions with Too Many Connections</a>. 
His scenario looks very familiar: He uses queue-triggered Azure Functions to notify 
subscribers about data breaches. One day, he dropped 126 million items into the queue, and Azure 
scaled out, which overloaded Mozilla&#39;s servers and caused them to go all-timeouts.</p>
<p>Another consideration is that non-serverless dependencies limit the scalability of your serverless 
application. If you call a legacy HTTP endpoint, a SQL database, or a third-party web service&mdash;be
sure to test how they react when your serverless function scales out to hundreds of concurrent 
executions.</p>
<p>Stay tuned for more serverless performance goodness!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/scalability/'>Scalability</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/gcp/'>GCP</a>, <a href='/tags/google-cloud-functions/'>Google Cloud Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Oct 10th, 2018</div>
    
    <h1><a href='/2018/10/azure-functions-v2-released-how-performant-is-it/'>Azure Functions V2 Is Released, How Performant Is It?</a></h1>
    

    

    <div class="post-content">
        <p>Azure Functions major version 2.0 was released into GA a few days back during Microsoft Ignite. The runtime is now
based on .NET Core and thus is cross-platform and more interoperable. It has a nice extensibility story too.</p>
<p>In theory, .NET Core runtime is more lean and performant.
But last time <a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">I checked back in April</a>,
the preview version of Azure Functions V2 had some serious issues with cold start durations.</p>
<p>I decided to give the new and shiny version another try and ran several benchmarks. All tests were conducted on 
Consumption plan.</p>
<p><strong>TL;DR</strong>: it&#39;s not perfect just yet.</p>
<h2 id="cold-starts">Cold Starts</h2>
<p>Cold starts happen when a new instance handles its first request, see my other posts:
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">one</a>,
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">two</a>,
<a href="https://mikhail.io/2018/08/serverless-cold-start-war/">three</a>.</p>
<h3 id="hello-world">Hello World</h3>
<p>The following chart gives a comparison of V1 vs V2 cold starts for the two most popular runtimes:
.NET and Javascript. The dark bar shows the most probable range of values, while the light ones
are possible but less frequent:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-dotnet-js.png" alt="Cold Starts V1 vs V2: .NET and Javascript"></p>
<p>Apparently, V2 is slower to start for both runtimes. V2 on .NET is slower by 10% on average and seems 
to have higher variation. V2 on Javascript is massively slower: 2 times on average, and the slowest startup
time goes above 10 seconds.</p>
<h3 id="dependencies-on-board">Dependencies On Board</h3>
<p>The values for the previous chart were calculated for Hello-World type of functions with no extra dependencies.</p>
<p>The chart below shows two more Javascript functions, this time with a decent number of dependencies:</p>
<ul>
<li>Referencing 3 NPM packages - 5MB zipped</li>
<li>Referencing 38 NPM packages - 35 MB zipped</li>
</ul>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-js-dependencies.png" alt="Cold Starts V1 vs V2: Javascript with NPM dependencies"></p>
<p>V2 clearly loses on both samples, but V2-V1 difference seems to be consistently within 2.5-3
seconds for any amount of dependencies.</p>
<p>All the functions were deployed with the Run-from-Package method which promises faster startup times.</p>
<h3 id="java">Java</h3>
<p>Functions V2 come with a preview of a new runtime: Java / JVM. It utilizes the same extensibility model 
as Javascript, and thus it seems to be a first-class citizen now.</p>
<p>Cold starts are not first-class though: </p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-java.png" alt="Cold Starts Java"></p>
<p>If you are a Java developer, be prepared for 20-25 seconds of initial startup time. That will probably 
be resolved when the Java runtime becomes generally available:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">That matches some of our internal data. We are looking into it.</p>&mdash; Paul Batum (@paulbatum) <a href="https://twitter.com/paulbatum/status/1048391445386735616?ref_src=twsrc%5Etfw">October 6, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="queue-processor">Queue Processor</h2>
<p>Cold starts are most problematic for synchronous triggers like HTTP requests. They are less relevant
for queue-based workloads, where scale out is of higher importance.</p>
<p>Last year I ran some tests around the ability of Functions to keep up with variable queue load:
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">one</a>,
<a href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/">two</a>.</p>
<p>Today I ran two simple tests to compare the scalability of V1 vs. V2 runtimes.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>In my first tests, a lightweight Javascript Function processed messages from an Azure Storage Queue. For
each message, it just pauses for 500 msec and then completes. This is supposed to simulate I/O-bound 
Functions.</p>
<p>I&#39;ve sent 100,000 messages to the queue and measured how fast they went away. Batch size (degree of parallelism
on each instance) was set to 16.</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/queue-scaling-io-based.png" alt="Processing Queue Messages with Lightweight I/O Workload"></p>
<p>Two lines show the queue backlogs of two runtimes, while the bars indicate the number of instances working
in parallel at a given minute.</p>
<p>We see that V2 was a bit faster to complete, probably due to more instances provisioned to it at any moment.
The difference is not big though and might be statistically insignificant.</p>
<h3 id="cpu-at-work">CPU at Work</h3>
<p>Functions in my second experiment are CPU-bound. Each message invokes calculation of a 10-stage Bcrypt
hash. On a very quiet moment, 1 such function call takes about 300-400 ms to complete, consuming 100% CPU 
load on a single core.</p>
<p>Both Functions are precompiled .NET and both are using <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.NET</a>.</p>
<p>Batch size (degree of parallelism on each instance) was set to 2 to avoid too much fighting for the same CPU. Yet, 
the average call duration is about 1.5 seconds (3x slower than possible).</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/queue-scaling-cpu-bound.png" alt="Processing Queue Messages with CPU-bound Workload"></p>
<p>The first thing to notice: it&#39;s the same number of messages with comparable &quot;sequential&quot; execution time, but 
the total time to complete the job increased 3-fold. That&#39;s because the workload is much more demanding to
the resources of application instances, and they struggle to parallelize work more aggressively.</p>
<p>V1 and V2 are again close to each other. One more time, V2 got more instances allocated to it most of the time.
And yet, it seemed to be <em>consistently</em> slower and lost about 2.5 minutes on 25 minutes interval (~10%).</p>
<h2 id="http-scalability">HTTP Scalability</h2>
<p>I ran two similar Functions &mdash; I/O-bound &quot;Pause&quot; (~100 ms) and CPU-bound Bcrypt (9 stages, ~150ms) &mdash; under a stress test.
But this time they were triggered by HTTP requests. Then I compared the results for V1 and V2.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>The grey bars on the following charts represent the rate of requests sent and processed within a given minute.</p>
<p>The lines are percentiles of response time: green lines for V2 and orange lines for V1.</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-io-based.png" alt="Processing HTTP Requests with Lightweight I/O Workload"></p>
<p>Yes, you saw it right, my Azure Functions were processing 100,000 messages per minute at peak. That&#39;s a lot of
messages.</p>
<p>Apart from the initial spike at minutes 2 and 3, both versions performed pretty close to each other.</p>
<p>50th percentile is flat close to the theoretic minimum of 100 ms, while the 95th percentile fluctuates a bit, but still
mostly stays quite low. </p>
<p>Note that the response time is measured from the client perspective, not by looking at the statistics provided by Azure.</p>
<h3 id="cpu-fanout">CPU Fanout</h3>
<p>How did CPU-heavy workload perform?</p>
<p>To skip ahead, I must say that the response time increased much more significantly, so my sample clients were
not able to generate request rates of 100k per minute. They &quot;only&quot; did about 48k per minute at peak, which still
seems massive to me.</p>
<p>I&#39;ve run the same test twice: one for Bcrypt implemented in .NET, and one for Javascript.</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-dotnet.png" alt="Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>V2 had a real struggle during the first minute, where response time got terribly slow up to 9 seconds.</p>
<p>Looking at the bold-green 50th percentile, we can see that it&#39;s consistently higher than the orange one throughout
the load growth period of the first 10 minutes. V2 seemed to have a harder time to adjust.</p>
<p>This might be explainable by slower growth of instance count:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-dotnet-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>This difference could be totally random, so let&#39;s look at a similar test with Javascript worker. Here is the percentile chart:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-js.png" alt="Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>The original slowness of the first 3 minutes is still there, but after that time V2 and V1 are on-par.</p>
<p>On-par doesn&#39;t sound that great though if you look at the significant edge in the number of allocated instances, in
favor of V2 this time:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-js-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>Massive 147 instances were crunching Bcrypt hashes in Javascript V2, and that made it a bit faster to respond than V1.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As always, be reluctant to make definite conclusions based on simplistic benchmarks. But I see some trends which might
be true as of today:</p>
<ul>
<li>Performance of .NET Functions is comparable across two versions of Functions runtimes;</li>
<li>V1 still has a clear edge in the cold start time of Javascript Functions;</li>
<li>V2 is the only option for Java developers, but be prepared to very slow cold starts;</li>
<li>Scale-out characteristics seem to be independent of the runtime version, although there are blurry signs of
V2 being a bit slower to ramp up or slightly more resource hungry.</li>
</ul>
<p>I hope this helps in your serverless journey!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/cold-start/'>Cold Start</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Aug 30th, 2018</div>
    
    <h1><a href='/2018/08/serverless-cold-start-war/'>Serverless: Cold Start War</a></h1>
    

    

    <div class="post-content">
        <p>Serverless cloud services are hot. Except when they are not :)</p>
<p>AWS Lambda, Azure Functions, Google Cloud Functions are all similar in their attempt
to enable rapid development of cloud-native serverless applications.</p>
<p>Auto-provisioning and auto-scalability are the killer features of those Function-as-a-Service
cloud offerings. No management required, cloud providers will deliver infrastructure for the user
based on the actual incoming load.</p>
<p>One drawback of such dynamic provisioning is a phenomenon called &quot;cold start&quot;. Basically,
applications that haven&#39;t been used for a while take longer to startup and to handle the
first request.</p>
<p>Cloud providers keep a bunch of generic unspecialized workers in stock. Whenever a serverless
application needs to scale up, be it from 0 to 1 instances, or from N to N+1 likewise, the runtime
will pick one of the spare workers and will configure it to serve the named application:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart.png" alt="Cold Start"></p>
<p>This procedure takes time, so the latency of the application event handling increases. To avoid
doing this for every event, the specialized worker will be kept intact for some period of time.
When another event comes in, this worker will stand available to process it as soon as possible.
This is a &quot;warm start&quot;:</p>
<p><img src="/2018/08/serverless-cold-start-war//warmstart.png" alt="Warm Start"></p>
<p>The problem of cold start latency was described multiple times, here are the notable links:</p>
<ul>
<li><a href="https://blogs.msdn.microsoft.com/appserviceteam/2018/02/07/understanding-serverless-cold-start/">Understanding Serverless Cold Start</a></li>
<li><a href="https://hackernoon.com/cold-starts-in-aws-lambda-f9e3432adbf0">Everything you need to know about cold starts in AWS Lambda</a></li>
<li><a href="https://serverless.com/blog/keep-your-lambdas-warm/">Keeping Functions Warm</a></li>
<li><a href="https://theburningmonk.com/2018/01/im-afraid-youre-thinking-about-aws-lambda-cold-starts-all-wrong/">I&#39;m afraid you&#39;re thinking about AWS Lambda cold starts all wrong</a></li>
</ul>
<p>The goal of my article today is to explore how cold starts compare:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different languages and runtimes</li>
<li>For smaller vs larger applications (including dependencies)</li>
<li>How often cold starts happen</li>
<li>What can be done to optimize the cold starts</li>
</ul>
<p>Let&#39;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some important factors and parameters that
influence the outcome. My interpretation might be wrong. The results might change over time. If you happen 
to know a way to improve my tests, please let me know and I will re-run them and re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>All tests were run against HTTP Functions because that&#39;s where cold start matters the most. </p>
<p>All the functions were returning a simple JSON reporting their current instance ID, language etc.
Some functions were also loading extra dependencies, see below.</p>
<p>I did not rely on execution time reported by a cloud provider. Instead, I measured end-to-end duration from
the client perspective. This means that durations of HTTP gateway (e.g. API Gateway in case of AWS) are included
into the total duration. However, all calls were made from within the same region, so network latency should 
have minimal impact:</p>
<p><img src="/2018/08/serverless-cold-start-war//test-setup.png" alt="Test Setup"></p>
<p>Important note: I ran all my tests on GA (generally available) versions of services/languages, so e.g.
Azure tests were done with version 1 of Functions runtime (.NET Framework), and GCP tests were only made for
Javascript runtime.</p>
<h2 id="when-does-cold-start-happen-">When Does Cold Start Happen?</h2>
<p>Obviously, cold start happens when the very first request comes in. After that request is processed,
the instance is kept alive in case subsequent requests arrive. But for how long?</p>
<p>The answer differs between cloud providers.</p>
<p>To help you read the charts in this section, I&#39;ve marked cold starts with blue color dots, and warm starts
with orange color dots.</p>
<h3 id="azure">Azure</h3>
<p>Here is the chart for Azure. It shows the values of normalized request durations across
different languages and runtime versions (Y-axis) depending on the time since the previous
request in minutes (X-axis):</p>
<p><img src="/2018/08/serverless-cold-start-war//azure-coldstart-threshold.png" alt="Azure Cold Start Threshold"></p>
<p>Clearly, an idle instance lives for 20 minutes and then gets recycled. All requests after 20 minutes
threshold hit another cold start.</p>
<h3 id="aws">AWS</h3>
<p>AWS is more tricky. Here is the same kind of chart, relative durations vs time since the last request, 
measured for AWS Lambda:</p>
<p><img src="/2018/08/serverless-cold-start-war//aws-coldstart-threshold.png" alt="AWS Cold Start vs Warm Start"></p>
<p>There&#39;s no clear threshold here... For this sample, no cold starts happened within 28 minutes after the previous 
invocation. Afterward, the frequency of cold starts slowly rises. But even after 1 hour of inactivity, there&#39;s still a
good chance that your instance is alive and ready to take requests.</p>
<p>This doesn&#39;t match the official information that AWS Lambdas stay alive for just 5 minutes after the last
invocation. I reached out to Chris Munns, and he confirmed:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">
So what you are seeing is very much possible as the team plays with certain knobs/levers for execution environment lifecycle. 
let me know if you have concerns about it, but it should be just fine</p>&mdash; chrismunns (@chrismunns) 
<a href="https://twitter.com/chrismunns/status/1021452964630851585?ref_src=twsrc%5Etfw">July 23, 2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>A couple learning points here:</p>
<ul>
<li>AWS is working on improving cold start experience (and probably Azure/GCP do too)</li>
<li>My results might not be reliably reproducible in your application since it&#39;s affected by recent adjustments</li>
</ul>
<h3 id="gcp">GCP</h3>
<p>Google Cloud Functions left me completely puzzled. Here is the same chart for GCP cold starts (again,
orange dots are warm and blue ones are cold):</p>
<p><img src="/2018/08/serverless-cold-start-war//gcp-coldstart-threshold.png" alt="GCP Cold Start vs Warm Start"></p>
<p>This looks totally random to me. A cold start can happen in 3 minutes after the previous request, or an instance
can be kept alive for the whole hour. The probability of a cold start doesn&#39;t seem to depend on the interval,
at least just by looking at this chart.</p>
<p>Any ideas about what&#39;s going on are welcome!</p>
<h3 id="parallel-requests">Parallel requests</h3>
<p>Cold starts happen not only when the first instance of an application is provisioned. The same issue will happen whenever
all the provisioned instances are busy handling incoming events, and yet another event comes in (at scale out).</p>
<p>As far as I&#39;m aware, this behavior is common to all 3 providers, so I haven&#39;t prepared any comparison charts
for N+1 cold starts. Yet, be aware of them!</p>
<h2 id="reading-candle-charts">Reading Candle Charts</h2>
<p>In the following sections, you will see charts that represent statistical distribution of cold start time as
measured during my experiments. I repeated experiments multiple times and then grouped the metric values, e.g.
by the cloud provider or by language.</p>
<p>Each group will be represented by a &quot;candle&quot; on the chart. This is how you should read each candle:</p>
<p><img src="/2018/08/serverless-cold-start-war//sample-coldstart-chart.png" alt="How to Read Cold Start Charts"></p>
<h2 id="memory-allocation">Memory Allocation</h2>
<p>AWS Lambda and Google Cloud Functions have a setting to define the memory size that gets allocated to a single
instance of a function. A user can select a value from 128MB to 2GB and above at creation time.</p>
<p>More importantly, the virtual CPU cycles get allocated proportionally to this provisioned memory size. This means
that an instance of 512 MB will have twice as much CPU speed as an instance of 256MB.</p>
<p>Does this affect the cold start time?</p>
<p>I&#39;ve run a series of tests to compare cold start latency across the board of memory/CPU sizes. The results are
somewhat mixed.</p>
<p>AWS Lambda Javascript doesn&#39;t seem to have significant differences. This probably means that not so much CPU load
is required to start a Node.js &quot;Hello World&quot; application:</p>
<p><img src="/2018/08/serverless-cold-start-war//aws-coldstart-js-by-memory.png" alt="AWS Javascript Cold Start by Memory"></p>
<p>AWS Lambda .NET Core runtime does depend on memory size though. Cold start time drops dramatically with every increase
in allocated memory and CPU:</p>
<p><img src="/2018/08/serverless-cold-start-war//aws-coldstart-csharp-by-memory.png" alt="AWS C# Cold Start by Memory"></p>
<p>GCP Cloud Functions expose a similar effect even for Javascript runtime:</p>
<p><img src="/2018/08/serverless-cold-start-war//gcp-coldstart-js-by-memory.png" alt="GCP Javascript Cold Start by Memory"></p>
<p>In contrast to Amazon and Google, Microsoft doesn&#39;t ask to select a memory limit. Azure will charge Functions based 
on the actual memory usage. More importantly, it will always dedicate a full vCore for a given Function execution.</p>
<p>It&#39;s not exactly apples-to-apples, but I chose to fix the memory allocations of AWS Lambda and GCF to 1024 MB.
This feels the closest to Azure&#39;s vCore capacity, although I haven&#39;t tried a formal CPU performance comparison.</p>
<p>Given that, let&#39;s see how the 3 cloud providers compare in cold start time.</p>
<h2 id="javascript-baseline">Javascript Baseline</h2>
<p>Node.js is the only runtime supported in production by Google Cloud Functions right now. Javascript is also
probably by far the most popular language for serverless applications across the board.</p>
<p>Thus, it makes sense to compare the 3 cloud providers on how they perform in Javascript. The
base test measures the cold starts of &quot;Hello World&quot; type of functions. Functions have no 
dependencies, so deployment package is really small.</p>
<p>Here are the numbers for cold starts:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart-js-baseline.png" alt="Cold Start for Basic Javascript Functions"></p>
<p>AWS is clearly doing the best job here. GCP takes the second place, and Azure is the slowest. The rivals are
sort of close though, seemingly playing in the same league so the exact disposition might change over time.</p>
<h2 id="how-do-languages-compare-">How Do Languages Compare?</h2>
<p>I&#39;ve written Hello World HTTP function in all supported languages of the cloud platforms: </p>
<ul>
<li>AWS: Javascript, Python, Java, Go and C# (.NET Core)</li>
<li>Azure: Javascript and C# (precompiled .NET assembly)</li>
<li>GCP: Javascript</li>
</ul>
<p>Azure kind of supports much more languages, including Python and Java, but they are still considered
experimental / preview, so the cold starts are not fully optimized. See 
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">my previous article</a> for exact numbers.</p>
<p>Same applies to Python on GCP.</p>
<p>The following chart shows some intuition about the cold start duration per language. The languages
are ordered based on mean response time, from lowest to highest:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart-per-language.png" alt="Cold Start per Language per Cloud and Language"></p>
<p>AWS provides the richest selection of runtimes, and 4 out of 5 are faster than the other two cloud providers.
C# / .NET seems to be the least optimized (Amazon, why is that?).</p>
<h2 id="does-size-matter-">Does Size Matter?</h2>
<p>OK, enough of Hello World. A real-life function might be more heavy, mainly because it would
depend on other third-party libraries.</p>
<p>To simulate such scenario, I&#39;ve measured cold starts for functions with extra dependencies:</p>
<ul>
<li>Javascript referencing 3 NPM packages - 5MB zipped</li>
<li>Javascript referencing 38 NPM packages - 35 MB zipped</li>
<li>C# function referencing 5 NuGet packages - 2 MB zipped</li>
<li>Java function referencing 5 Maven packages - 15 MB zipped</li>
</ul>
<p>Here are the results:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart-dependencies.png" alt="Cold Start Dependencies"></p>
<p>As expected, the dependencies slow the loading down. You should keep your Functions lean,
otherwise, you will pay in seconds for every cold start.</p>
<p>However, the increase in cold start seems quite low, especially for precompiled languages.</p>
<p>A very cool feature of GCP Cloud Functions is that you don&#39;t have to include NPM packages into
the deployment archive. You just add <code>package.json</code> file and the runtime will restore them for you.
This makes the deployment artifact ridiculously small, but doesn&#39;t seem to slow down the cold
starts either. Obviously, Google pre-restores the packages in advance, before the actual request 
comes in.</p>
<h2 id="avoiding-cold-starts">Avoiding Cold Starts</h2>
<p>The overall impression is that cold start delays aren&#39;t that high, so most applications can tolerate
them just fine.</p>
<p>If that&#39;s not the case, some tricks can be implemented to keep function instances warm.
The approach is universal for all 3 providers: once in X minutes, make an artificial call to
the function to prevent it from expiring.</p>
<p>Implementation details will differ since the expiration policies are different, as we explored
above.</p>
<p>For applications with higher load profile, you might want to fire several parallel &quot;warming&quot;
requests in order to make sure that enough instances are kept in warm stock.</p>
<p>For further reading, have a look at my 
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">Cold Starts Beyond First Request in Azure Functions</a>
and <a href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/">AWS Lambda Warmer as Pulumi Component</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Here are some lessons learned from all the experiments above:</p>
<ul>
<li>Be prepared for 1-3 seconds cold starts even for the smallest Functions</li>
<li>Different languages and runtimes have roughly comparable cold start time within the same platform</li>
<li>Minimize the number of dependencies, only bring what&#39;s needed</li>
<li>AWS keeps cold starts below 1 second most of the time, which is pretty amazing</li>
<li>All cloud providers are aware of the problem and are actively optimizing the cold start experience</li>
<li>It&#39;s likely that in middle term these optimizations will make cold starts a non-issue for the
vast majority of applications</li>
</ul>
<p>Do you see anything weird or unexpected in my results? Do you need me to dig deeper into other aspects?
Please leave a comment below or ping me on <a href="https://twitter.com/MikhailShilkov">twitter</a>, and let&#39;s 
sort it all out.</p>
<p>Stay tuned for more serverless perf goodness!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/cold-start/'>Cold Start</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/gcp/'>GCP</a>, <a href='/tags/google-cloud-functions/'>Google Cloud Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Aug 2nd, 2018</div>
    
    <h1><a href='/2018/08/aws-lambda-warmer-as-pulumi-component/'>AWS Lambda Warmer as Pulumi Component</a></h1>
    

    

    <div class="post-content">
        <p>Out of curiosity, I&#39;m currently investigating cold starts of Function-as-a-Service platforms of major cloud providers. Basically,
if a function is not called for several minutes, the cloud instance behind it might be recycled, and then the next request will
take longer because a new instance will need to be provisioned.</p>
<p>Recently, Jeremy Daly <a href="https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/">posted</a> a nice
article about the proper way to keep AWS Lambda instances &quot;warm&quot; to (mostly) prevent cold starts with minimal overhead.
Chris Munns <a href="https://twitter.com/chrismunns/status/1017777028274294784">endorsed</a> the article, so we know it&#39;s the right way.</p>
<p>The amount of actions to be taken is quite significant:</p>
<ul>
<li>Define a CloudWatch event which would fire every 5 minutes</li>
<li>Bind this event as another trigger for your Lambda</li>
<li>Inside the Lambda, detect whether current invocation is triggered by our CloudWatch event</li>
<li>If so, short-circuit the execution and return immediately; otherwise, run the normal workload</li>
<li>(Bonus point) If you want to keep multiple instances alive, do some extra dancing with calling itself N times in parallel,
provided by an extra permission to do so.</li>
</ul>
<h2 id="pursuing-reusability">Pursuing Reusability</h2>
<p>To simplify this for his readers, Jeremy was so kind to</p>
<ul>
<li>Create an NPM package which you can install and then call from a function-to-be-warmed</li>
<li>Provide SAM and Serverless Framework templates to automate Cloud Watch integration</li>
</ul>
<p>Those are still two distinct steps: writing the code (JS + NPM) and provisioning the cloud resources (YAML + CLI). There are some
drawbacks to that:</p>
<ul>
<li>You need to change two parts, which don&#39;t look like each other</li>
<li>They have to work in sync, e.g. Cloud Watch event must provide the right payload for the handler</li>
<li>There&#39;s still some boilerplate for every new Lambda</li>
</ul>
<h2 id="pulumi-components">Pulumi Components</h2>
<p>Pulumi takes a different approach. You can blend the application code and infrastructure management code
into one cohesive cloud application.</p>
<p>Related resources can be combined together into reusable components, which hide repetitive stuff behind code abstractions.</p>
<p>One way to define an AWS Lambda with Typescript in Pulumi is the following:</p>
<pre><code class="language-typescript">const handler = (event: any, context: any, callback: (error: any, result: any) =&gt; void) =&gt; {
    const response = {
        statusCode: 200,
        body: &quot;Cheers, how are things?&quot;
      };

    callback(null, response);
};

const lambda = new aws.serverless.Function(&quot;my-function&quot;, { /* options */ }, handler);</code></pre>
<p>The processing code <code>handler</code> is just passed to infrastructure code as a parameter.</p>
<p>So, if I wanted to make reusable API for an &quot;always warm&quot; function, how would it look like?</p>
<p>From the client code perspective, I just want to be able to do the same thing:</p>
<pre><code class="language-typescript">const lambda = new mylibrary.WarmLambda(&quot;my-warm-function&quot;, { /* options */ }, handler);</code></pre>
<p>CloudWatch? Event subscription? Short-circuiting? They are implementation details!</p>
<h2 id="warm-lambda">Warm Lambda</h2>
<p>Here is how to implement such component. The declaration starts with a Typescript class:</p>
<pre><code class="language-typescript">export class WarmLambda extends pulumi.ComponentResource {
    public lambda: aws.lambda.Function;

    // Implementation goes here...
}</code></pre>
<p>We expose the raw Lambda Function object, so that it could be used for further bindings and retrieving outputs.</p>
<p>The constructor accepts the same parameters as <code>aws.serverless.Function</code> provided by Pulumi:</p>
<pre><code class="language-typescript">constructor(name: string,
        options: aws.serverless.FunctionOptions,
        handler: aws.serverless.Handler,
        opts?: pulumi.ResourceOptions) {

    // Subresources are created here...
}</code></pre>
<p>We start resource provisioning by creating the CloudWatch rule to be triggered every 5 minutes:</p>
<pre><code class="language-typescript">const eventRule = new aws.cloudwatch.EventRule(`${name}-warming-rule`, 
    { scheduleExpression: &quot;rate(5 minutes)&quot; },
    { parent: this, ...opts }
);</code></pre>
<p>Then goes the cool trick. We substitute the user-provided handler with our own &quot;outer&quot; handler. This handler closes
over <code>eventRule</code>, so it can use the rule to identify the warm-up event coming from CloudWatch. If such is identified,
the handler short-circuits to the callback. Otherwise, it passes the event over to the original handler:</p>
<pre><code class="language-typescript">const outerHandler = (event: any, context: aws.serverless.Context, callback: (error: any, result: any) =&gt; void) =&gt;
{
    if (event.resources &amp;&amp; event.resources[0] &amp;&amp; event.resources[0].includes(eventRule.name.get())) {
        console.log(&#39;Warming...&#39;);
        callback(null, &quot;warmed!&quot;);
    } else {
        console.log(&#39;Running the real handler...&#39;);
        handler(event, context, callback);
    }
};</code></pre>
<p>That&#39;s a great example of synergy enabled by doing both application code and application infrastructure in a
single program. I&#39;m free to mix and match objects from both worlds.</p>
<p>It&#39;s time to bind both <code>eventRule</code> and <code>outerHandler</code> to a new serverless function:</p>
<pre><code class="language-typescript">const func = new aws.serverless.Function(
    `${name}-warmed`, 
    options, 
    outerHandler, 
    { parent: this, ...opts });
this.lambda = func.lambda;            </code></pre>
<p>Finally, I create an event subscription from CloudWatch schedule to Lambda:</p>
<pre><code class="language-typescript">this.subscription = new serverless.cloudwatch.CloudwatchEventSubscription(
    `${name}-warming-subscription`, 
    eventRule,
    this.lambda,
    { },
    { parent: this, ...opts });</code></pre>
<p>And that&#39;s all we need for now! See the full code 
<a href="https://github.com/mikhailshilkov/pulumi-serverless-examples/blob/master/WarmedLambda-TypeScript/warmLambda.ts">here</a>.</p>
<p>Here is the output of <code>pulumi update</code> command for my sample &quot;warm&quot; lambda application:</p>
<pre><code>     Type                                                      Name                            Plan
 +   pulumi:pulumi:Stack                                       WarmLambda-WarmLambda-dev       create
 +    samples:WarmLambda                                       i-am-warm                       create
 +      aws-serverless:cloudwatch:CloudwatchEventSubscription  i-am-warm-warming-subscription  create
 +        aws:lambda:Permission                                i-am-warm-warming-subscription  create
 +        aws:cloudwatch:EventTarget                           i-am-warm-warming-subscription  create
 +      aws:cloudwatch:EventRule                               i-am-warm-warming-rule          create
 +      aws:serverless:Function                                i-am-warm-warmed                create
 +         aws:lambda:Function                                 i-am-warm-warmed                create</code></pre><p>7 Pulumi components and 4 AWS cloud resources are provisioned by one <code>new WarmLambda()</code> line.</p>
<h2 id="multi-instance-warming">Multi-Instance Warming</h2>
<p>Jeremy&#39;s library supports warming several instances of Lambda by issuing parallel self-calls.</p>
<p>Reproducing the same with Pulumi component should be fairly straightforward:</p>
<ul>
<li>Add an extra constructor option to accept the number of instances to keep warm</li>
<li>Add a permission to call Lambda from itself</li>
<li>Fire N calls when warming event is triggered</li>
<li>Short-circuit those calls in each instance</li>
</ul>
<p>Note that only the first item would be visible to the client code. That&#39;s the power of componentization
and code reuse.</p>
<p>I didn&#39;t need multi-instance warming, so I&#39;ll leave the implementation as exercise for the reader.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Obligatory note: most probably, you don&#39;t need to add warming to your AWS Lambdas.</p>
<p>But whatever advanced scenario you might have, it&#39;s likely that it is easier to express the scenario
in terms of general-purpose reusable component, rather than a set of guidelines or templates.</p>
<p>Happy hacking!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/pulumi/'>Pulumi</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/cold-starts/'>Cold Starts</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Jul 12th, 2018</div>
    
    <h1><a href='/2018/07/getting-started-with-aws-lambda-in-pulumi/'>Getting Started with AWS Lambda in Pulumi</a></h1>
    

    

    <div class="post-content">
        <p>For a small research project of mine, I needed to create HTTP triggered
AWS Lambda&#39;s in all supported programming languages.</p>
<p>I&#39;m not a power AWS user, so I get easily confused about the configuration
of things like IAM roles or API Gateway. Moreover, I wanted my environment to
be reproducible, so manual AWS Console wasn&#39;t a good option.</p>
<p>I decided it was a good job for Pulumi. They pay a lot of attention to
serverless and especially AWS Lambda, and I love the power of 
configuration as code.</p>
<p>I created a Pulumi program which provisions Lambda&#39;s running on Javascript,
.NET, Python, Java and Go. Pulumi program itself is written in Javascript.</p>
<p>I&#39;m describing the resulting code below in case folks need to do the same thing.
The code itself is on <a href="https://github.com/mikhailshilkov/pulumi-aws-serverless-examples">my github</a>.</p>
<h2 id="javascript">Javascript</h2>
<p>Probably, the vast majority of Pulumi + AWS Lambda users will be using
Javascript as programming language for their serverless functions.</p>
<p>No wonder that this scenario is the easiest to start with. There is a
high-level package <code>@pulumi/cloud-aws</code> which hides all the AWS machinery from
a developer. </p>
<p>The simplest function will consist of just several lines:</p>
<pre><code class="language-js">const cloud = require(&quot;@pulumi/cloud-aws&quot;);

const api = new cloud.API(&quot;aws-hellolambda-js&quot;);
api.get(&quot;/js&quot;, (req, res) =&gt; {
    res.status(200).json(&quot;Hi from Javascript lambda&quot;);
});

exports.endpointJs = api.publish().url;</code></pre>
<p>Configure your Pulumi stack, run <code>pulumi update</code> and a Lambda 
is up, running and accessible via HTTP.</p>
<h2 id="-net-core">.NET Core</h2>
<p>.NET is my default development environment and AWS Lambda supports .NET Core
as execution runtime.</p>
<p>Pulumi program is still Javascript, so it can&#39;t mix C# code in. Thus, the setup
looks like this:</p>
<ul>
<li>There is a .NET Core 2.0 application written in C# and utilizing
<code>Amazon.Lambda.*</code> NuGet packages</li>
<li>I build and publish this application with <code>dotnet</code> CLI</li>
<li>Pulumi then utilizes the published binaries to create deployment artifacts</li>
</ul>
<p>C# function looks like this:</p>
<pre><code class="language-csharp">public class Functions
{
    public async Task&lt;APIGatewayProxyResponse&gt; GetAsync(APIGatewayProxyRequest request, ILambdaContext context)
    {
        return new APIGatewayProxyResponse
        {
            StatusCode = (int)HttpStatusCode.OK,
            Body = &quot;\&quot;Hi from C# Lambda\&quot;&quot;,
            Headers = new Dictionary&lt;string, string&gt; { { &quot;Content-Type&quot;, &quot;application/json&quot; } }
        };
    }
}</code></pre>
<p>For non-Javascript lambdas I utilize <code>@pulumi/aws</code> package. It&#39;s of lower level
than <code>@pulumi/cloud-aws</code>, so I had to setup IAM first:</p>
<pre><code class="language-js">const aws = require(&quot;@pulumi/aws&quot;);

const policy = {
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
            &quot;Principal&quot;: {
                &quot;Service&quot;: &quot;lambda.amazonaws.com&quot;,
            },
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Sid&quot;: &quot;&quot;,
        },
    ],
};
const role = new aws.iam.Role(&quot;precompiled-lambda-role&quot;, {
    assumeRolePolicy: JSON.stringify(policy),
});</code></pre>
<p>And then I did a raw definition of AWS Lambda:</p>
<pre><code class="language-js">const pulumi = require(&quot;@pulumi/pulumi&quot;);

const csharpLambda = new aws.lambda.Function(&quot;aws-hellolambda-csharp&quot;, {
    runtime: aws.lambda.DotnetCore2d0Runtime,
    code: new pulumi.asset.AssetArchive({
        &quot;.&quot;: new pulumi.asset.FileArchive(&quot;./csharp/bin/Debug/netcoreapp2.0/publish&quot;),
    }),
    timeout: 5,
    handler: &quot;app::app.Functions::GetAsync&quot;,
    role: role.arn
});</code></pre>
<p>Note the path to <code>publish</code> folder, which should match the path created by
<code>dotnet publish</code>, and the handler name matching C# class/method.</p>
<p>Finally, I used <code>@pulumi/aws-serverless</code> to define API Gateway endpoint for
the lambda:</p>
<pre><code class="language-js">const serverless = require(&quot;@pulumi/aws-serverless&quot;);

const precompiledApi = new serverless.apigateway.API(&quot;aws-hellolambda-precompiledapi&quot;, {
    routes: [
        { method: &quot;GET&quot;, path: &quot;/csharp&quot;, handler: csharpLambda },
    ],
});</code></pre>
<p>That&#39;s definitely more ceremony compared to Javascript version. But hey, it&#39;s
code, so if you find yourself repeating the same code, go ahead and make a
higher order component out of it, incapsulating the repetitive logic.</p>
<h2 id="python">Python</h2>
<p>Pulumi supports Python as scripting language, but I&#39;m sticking to Javascript
for uniform experience.</p>
<p>In this case, the flow is similar to .NET but simpler: no compilation step
is required. Just define a <code>handler.py</code>:</p>
<pre><code class="language-python">def handler(event, context): 
    return {
        &#39;statusCode&#39;: 200,
        &#39;headers&#39;: {&#39;Content-Type&#39;: &#39;application/json&#39;},
        &#39;body&#39;: &#39;&quot;Hi from Python lambda&quot;&#39;
    }</code></pre>
<p>and package it into zip in AWS lambda definition:</p>
<pre><code class="language-js">const pythonLambda = new aws.lambda.Function(&quot;aws-hellolambda-python&quot;, {
    runtime: aws.lambda.Python3d6Runtime,
    code: new pulumi.asset.AssetArchive({
        &quot;.&quot;: new pulumi.asset.FileArchive(&quot;./python&quot;),
    }),
    timeout: 5,
    handler: &quot;handler.handler&quot;,
    role: role.arn
});</code></pre>
<p>I&#39;m reusing the <code>role</code> definition from above. The API definition will also
be the same as for .NET.</p>
<h2 id="go">Go</h2>
<p>Golang is a compiled language, so the approach is similar to .NET: write code,
build, reference the built artifact from Pulumi.</p>
<p>My Go function looks like this:</p>
<pre><code class="language-go">func Handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {

 return events.APIGatewayProxyResponse{
  Body:       &quot;\&quot;Hi from Golang lambda\&quot;&quot;,
  StatusCode: 200,
 }, nil

}</code></pre>
<p>Because I&#39;m on Windows but AWS Lambda runs on Linux, I had to use 
<a href="https://github.com/aws/aws-lambda-go"><code>build-lambda-zip</code></a> 
tool to make the package compatible. Here is the PowerShell build script:</p>
<pre><code class="language-powershell">$env:GOOS = &quot;linux&quot;
$env:GOARCH = &quot;amd64&quot;
go build -o main main.go
~\Go\bin\build-lambda-zip.exe -o main.zip main</code></pre>
<p>and Pulumi function definition:</p>
<pre><code class="language-js">const golangLambda = new aws.lambda.Function(&quot;aws-hellolambda-golang&quot;, {
    runtime: aws.lambda.Go1dxRuntime,
    code: new pulumi.asset.FileArchive(&quot;./go/main.zip&quot;),
    timeout: 5,
    handler: &quot;main&quot;,
    role: role.arn
});</code></pre>
<h2 id="java">Java</h2>
<p>Java class implements an interface from AWS SDK:</p>
<pre><code class="language-java">public class Hello implements RequestStreamHandler {

    public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context) throws IOException {

        JSONObject responseJson = new JSONObject();

        responseJson.put(&quot;isBase64Encoded&quot;, false);
        responseJson.put(&quot;statusCode&quot;, &quot;200&quot;);
        responseJson.put(&quot;body&quot;, &quot;\&quot;Hi from Java lambda\&quot;&quot;);  

        OutputStreamWriter writer = new OutputStreamWriter(outputStream, &quot;UTF-8&quot;);
        writer.write(responseJson.toJSONString());  
        writer.close();
    }
}</code></pre>
<p>I compiled this code with Maven (<code>mvn package</code>), which produced a <code>jar</code> file. AWS Lambda accepts
<code>jar</code> directly, but Pulumi&#39;s <code>FileArchive</code> is unfortunately crashing on trying
to read it.</p>
<p>As a workaround, I had to define a <code>zip</code> file with <code>jar</code> placed inside <code>lib</code>
folder:</p>
<pre><code class="language-js">const javaLambda = new aws.lambda.Function(&quot;aws-coldstart-java&quot;, {
    code: new pulumi.asset.AssetArchive({
        &quot;lib/lambda-java-example-1.0-SNAPSHOT.jar&quot;: new pulumi.asset.FileAsset(&quot;./java/target/lambda-java-example-1.0-SNAPSHOT.jar&quot;),
    }),
    runtime: aws.lambda.Java8Runtime,
    timeout: 5,
    handler: &quot;example.Hello&quot;,
    role: role.arn
});</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>The complete code for 5 lambda functions in 5 different programming languages
can be found in <a href="https://github.com/mikhailshilkov/pulumi-aws-serverless-examples">my github repository</a>.</p>
<p>Running <code>pulumi update</code> provisions 25 AWS resources in a matter of 1 minute,
so I can start playing with my test lambdas in no time.</p>
<p>And the best part: when I don&#39;t need them anymore, I run <code>pulumi destroy</code> and
my AWS Console is clean again!</p>
<p>Happy serverless moments!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/pulumi/'>Pulumi</a>, <a href='/tags/serverless/'>Serverless</a>
    </div>
    
</article>


<div class="page-nav">
    
    
    <a class="page-nav-older" href="/2/index.html">Next page &gt;&gt;</span></a>
    
</div>

<div id="me">
    <p itemscope itemtype="http://data-vocabulary.org/Person">
        <img src="/images/Headshot-Square.jpg" alt="Mikhail Shilkov" itemprop="photo" />
        I'm <b><span itemprop="name">Mikhail Shilkov</span></b>, a <span itemprop="title">software developer</span>. I enjoy F#, C#, Javascript and SQL development, reasoning about distributed systems, data processing pipelines, cloud and web apps. I blog about my experience on this website.
    </p>
    <p>
        <a href="https://www.linkedin.com/in/mikhailshilkov/">LinkedIn</a> &#8226;
        <a href="https://twitter.com/mikhailshilkov">@mikhailshilkov</a> &#8226;
        <a href="https://github.com/mikhailshilkov">GitHub</a> &#8226;
        <a href="https://stackoverflow.com/users/1171619/mikhail">Stack Overflow</a>
    </p>
</div>
</div>
<div class="container">
    <div class="navbar navbar-footer">
        <p class="navbar-center navbar-text">Content copyright &copy; 2018 Mikhail Shilkov</p>
    </div>
</div>



<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.pack.js"></script>
<script src="/vendor/prism.js"></script>
<script src="/site.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59218480-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>