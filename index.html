<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>

    <meta name="description" content="Software development using .NET, C#, SQL, Javascript and related technologies" />

    <title>Mikhail Shilkov</title>
    <meta name="author" content="Mikhail Shilkov">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="twitter:card" content="summary_large_image"></meta>
    <meta name="twitter:creator" content="@MikhailShilkov"></meta>
    <meta name="twitter:title" content="Mikhail Shilkov"></meta>

    <meta property="og:type" content="article" />
    <meta property="og:title" content="Mikhail Shilkov" />
    <meta property="og:url" content="https://mikhail.io/" />




    <link href="/feed/" rel="alternate" title="mikhail.io" type="application/atom+xml">
    <link href="/favicon.ico?v=2" rel="shortcut icon">

    <!-- Bootstrap -->
    <link href="/vendor/prism.css" rel="stylesheet" media="screen">
    <link href="/styles/site.css" rel="stylesheet" media="screen">

    <meta name="generator" content="DocPad v6.80.6" />
    
</head>
<body>

<div class="navbar navbar-default navbar-static-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">
                <span class="text-primary">Mikhail Shilkov</span><br />
                <span class="elevator-pitch">Serverless, Azure, FP, F# and more</span>
            </a>
        </div>
        <div class="collapse navbar-collapse navbar-right">
            <ul class="nav navbar-nav">
                <!--<li><a href="/">Blog</a></li>-->
                
                    <li><a href="/tags/">Topics</a></li>
                
                    <li><a href="/archives/">Archives</a></li>
                
                    <li><a href="/talks/">Talks</a></li>
                
                    <li><a href="/about/">About</a></li>
                
                <li class="hidden-xs">
                    <a href="/feed/" class="rss"><span class="icon icon-feed"></span></a>
                    <a href="https://www.linkedin.com/in/mikhailshilkov" class="linkedin"><span class="icon icon-linkedin"></span></a>
                    <a href="https://twitter.com/mikhailshilkov" class="twitter"><span class="icon icon-twitter"></span></a>
                    <a href="https://github.com/mikhailshilkov" class="github"><span class="icon icon-github"></span></a>
                </li>
            </ul>
            <form class="navbar-form navbar-right hidden-xs" role="search" action="https://google.com/search"
                  method="get">
                <div class="form-group">
                    <input type="search" name="q" class="form-control" placeholder="Search">
                    <input type="hidden" name="q" value="site:mikhail.io">
                </div>
            </form>
        </div>
    </div>
</div>
<div class="container">
    
    <article class="post">
    <div class="post-date">Feb 14th, 2019</div>
    
    <h1><a href='/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/'>From YAML to TypeScript: Developer's View on Cloud Automation</a></h1>
    

    

    <div class="post-content">
        <p>The rise of managed cloud services, cloud-native and serverless applications brings both new possibilities and challenges. More and more practices from software development process like version control, code review, continuous integration, and automated testing are applied to the cloud infrastructure provisioning.</p>
<p>Most existing tools suggest defining infrastructure in text-based markup formats. In this article, I&#39;m making a case for using real programming languages like TypeScript instead. Such a change makes even more software development practices applicable to the infrastructure realm.</p>
<h2 id="sample-application">Sample Application</h2>
<p>It&#39;s easier to make a case given a specific example. For this article, I define a URL Shortener application, a basic clone of tinyurl.com or bit.ly. There is an administrative page where one can define short aliases for long URLs:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/url-shortener.png" alt="URL Shortener sample app"></p>
<figcaption>URL Shortener sample app</figcaption>

<p>Now, whenever somebody goes to the base URL of the application + an existing alias, they get redirected to the full URL.</p>
<p>This app is simple to describe but involves enough moving parts to be representative of some real-world issues. As a bonus, there are many existing implementations on the web to compare with.</p>
<h2 id="serverless-url-shortener">Serverless URL Shortener</h2>
<p>I&#39;m a big proponent of the serverless architecture: the style of cloud applications being a combination of serverless functions and managed cloud services. They are fast to develop, effortless to run and cost pennies unless the application gets lots of users. However, even serverless applications require infrastructure provisioning, like databases, queues, and other sources of events and destinations of data.</p>
<p>My examples are going to use Amazon AWS, but this could be Microsoft Azure or Google Cloud Platform too.</p>
<p>So, the gist is to store URLs with short names as key-value pairs in Amazon DynamoDB and use AWS Lambdas to run the application code. Here is the initial sketch:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/lambda-dynamodb.png" alt="URL Shortener with AWS Lambda and DynamoDB"></p>
<figcaption>URL Shortener with AWS Lambda and DynamoDB</figcaption>

<p>The Lambda on the top receives an event when somebody decides to add a new URL. It extracts the name and the URL from the request and saves them as an item in the DynamoDB table.</p>
<p>The Lambda at the bottom is called when a user navigates to a short URL. The code reads the full URL based on the requested path and returns a 301 response with the corresponding location.</p>
<p>Here is the implementation of the <code>Open URL</code> Lambda in JavaScript:</p>
<pre><code class="language-javascript">const aws = require(&#39;aws-sdk&#39;);
const table = new aws.DynamoDB.DocumentClient();

exports.handler = async (event) =&gt; {
  const name = event.path.substring(1);

  const params = { TableName: &quot;urls&quot;, Key: { &quot;name&quot;: name } };
  const value = await table.get(params).promise();

  const url = value &amp;&amp; value.Item &amp;&amp; value.Item.url;
  return url 
    ? { statusCode: 301, body: &quot;&quot;, headers: { &quot;Location&quot;: url } }
    : { statusCode: 404, body: name + &quot; not found&quot; };
};</code></pre>
<p>That&#39;s 11 lines of code. I&#39;ll skip the implementation of <code>Add URL</code> function because it&#39;s very similar. Considering a third function to list the existing URLs for UI, we might have 30-40 lines of JavaScript in total.</p>
<p>So, how do we deploy the application?</p>
<p>Well, before we do that, we should realize that the above picture was an over-simplification.</p>
<ul>
<li>AWS Lambda can&#39;t handle HTTP requests directly, so we need to add AWS API Gateway in front of it.</li>
<li>We also need to serve some static files for the UI, which we&#39;ll put into AWS S3 and front it with the same API Gateway.</li>
</ul>
<p>Here is the updated diagram:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/apigateway-lambda-dynamodb-s3.png" alt="API Gateway, Lambda, DynamoDB, and S3"></p>
<figcaption>API Gateway, Lambda, DynamoDB, and S3</figcaption>

<p>This is a viable design, but the details are even more complicated:</p>
<ul>
<li>API Gateway is a complex beast which needs Stages, Deployments, and REST Endpoints to be appropriately configured.</li>
<li>Permissions and policies need to be defined so that API Gateway could call Lambda and Lambda could access DynamoDB.</li>
<li>Static Files should go to S3 Bucket Objects.</li>
</ul>
<p>So, the actual setup involves a couple of dozen objects to be configured in AWS:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/apigateway-lambda-dynamodb-s3-details.png" alt="Cloud resources to be provisioned"></p>
<figcaption>Cloud resources to be provisioned</figcaption>

<p>How do we approach this task?</p>
<h2 id="options-to-provision-the-infrastructure">Options to Provision the Infrastructure</h2>
<p>There are many options to provision a cloud application, each one has its trade-offs. Let&#39;s quickly go through the list of possibilities to understand the landscape.</p>
<h2 id="aws-web-console">AWS Web Console</h2>
<p>AWS, like any other cloud, has a <a href="https://console.aws.amazon.com">web user interface</a> to configure its resources:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/aws-web-console.png" alt="AWS Web Console"></p>
<figcaption>AWS Web Console</figcaption>

<p>That&#39;s a decent place to start&mdash;good for experimenting, figuring out the available options, following the tutorials, i.e., for exploration.</p>
<p>However, it doesn&#39;t suit particularly well for long-lived ever-changing applications developed in teams. A manually clicked deployment is pretty hard to reproduce in the exact manner, which becomes a maintainability issue pretty fast.</p>
<h2 id="aws-command-line-interface">AWS Command Line Interface</h2>
<p>The <a href="https://aws.amazon.com/cli/">AWS Command Line Interface</a> (CLI) is a unified tool to manage all AWS services from a command prompt. You write the calls like</p>
<pre><code class="language-console">aws apigateway create-rest-api --name &#39;My First API&#39; --description &#39;This is my first API&#39;

aws apigateway create-stage --rest-api-id 1234123412 --stage-name &#39;dev&#39; --description &#39;Development stage&#39; --deployment-id a1b2c3</code></pre>
<p>The initial experience might not be as smooth as clicking buttons in the browser, but the huge benefit is that you can <em>reuse</em> commands that you once wrote. You can build scripts by combining many commands into cohesive scenarios. So, your colleague can benefit from the same script that you created. You can provision multiple environments by parameterizing the scripts.</p>
<p>Frankly speaking, I&#39;ve never done that for several reasons:</p>
<ul>
<li>CLI scripts feel too imperative to me. I have to describe &quot;how&quot; to do things, not &quot;what&quot; I want to get in the end.</li>
<li>There seems to be no good story for updating existing resources. Do I write small delta scripts for each change? Do I have to keep them forever and run the full suite every time I need a new environment?</li>
<li>If a failure occurs mid-way through the script, I need to manually repair everything to a consistent state. This gets messy real quick, and I have no desire to exercise this process, especially in production.</li>
</ul>
<p>To overcome such limitations, the notion of the <strong>Desired State Configuration</strong> (DSC) was invented. Under this paradigm, one describes the desired layout of the infrastructure, and then the tooling takes care of either provisioning it from scratch or applying the required changes to an existing environment.</p>
<p>Which tool provides DSC model for AWS? There are legions.</p>
<h2 id="aws-cloudformation">AWS CloudFormation</h2>
<p><a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> is the first-party tool for Desired State Configuration management from Amazon. CloudFormation templates use YAML to describe all the infrastructure resources of AWS.</p>
<p>Here is a snippet from <a href="https://aws.amazon.com/blogs/compute/build-a-serverless-private-url-shortener/">a private URL shortener example</a> kindly provided at AWS blog:</p>
<pre><code class="language-yaml">Resources:
  S3BucketForURLs:
    Type: &quot;AWS::S3::Bucket&quot;
    DeletionPolicy: Delete
    Properties:
      BucketName: !If [ &quot;CreateNewBucket&quot;, !Ref &quot;AWS::NoValue&quot;, !Ref S3BucketName ]
      WebsiteConfiguration:
        IndexDocument: &quot;index.html&quot;
      LifecycleConfiguration:
        Rules:
          -
            Id: DisposeShortUrls
            ExpirationInDays: !Ref URLExpiration
            Prefix: &quot;u&quot;
            Status: Enabled</code></pre>
<p>This is just a very short fragment: the complete example consists of 317 lines YAML. That&#39;s an order of magnitude more than the actual JavaScript code that we have in the application!</p>
<p>CloudFormation is a powerful tool, but it demands quite some learning to be done to master it. Moreover, it&#39;s specific to AWS: you won&#39;t be able to transfer the skill to other cloud providers.</p>
<p>Wouldn&#39;t it be great if there was a universal DSC format? Meet Terraform.</p>
<h2 id="terraform">Terraform</h2>
<p><a href="https://www.terraform.io/">HashiCorp Terraform</a> is an open source tool to define infrastructure in declarative configuration files. It has a pluggable architecture, so the tool supports all major clouds and even hybrid scenarios.</p>
<p>The custom text-based Terraform <code>.tf</code> format is used to define the configurations. The templating language is quite powerful, and once you learn it, you can use it for different cloud providers.</p>
<p>Here is a snippet from <a href="https://github.com/jamesridgway/aws-lambda-short-url">AWS Lambda Short URL Generator</a> example:</p>
<pre><code class="language-tf">resource &quot;aws_api_gateway_rest_api&quot; &quot;short_urls_api_gateway&quot; {
  name        = &quot;Short URLs API&quot;
  description = &quot;API for managing short URLs.&quot;
}
resource &quot;aws_api_gateway_usage_plan&quot; &quot;short_urls_admin_api_key_usage_plan&quot; {
  name         = &quot;Short URLs admin API key usage plan&quot;
  description  = &quot;Usage plan for the admin API key for Short URLS.&quot;
  api_stages {
    api_id = &quot;${aws_api_gateway_rest_api.short_urls_api_gateway.id}&quot;
    stage  = &quot;${aws_api_gateway_deployment.short_url_api_deployment.stage_name}&quot;
  }
}</code></pre>
<p>This time, the complete example is around 450 lines of textual templates. Are there ways to reduce the size of the infrastructure definition?</p>
<p>Yes, by raising the level of abstraction. It&#39;s possible with Terraform&#39;s modules, or by using other, more specialized tools.</p>
<h2 id="serverless-framework-and-sam">Serverless Framework and SAM</h2>
<p><a href="https://serverless.com/">The Serverless Framework</a> is an infrastructure management tool focused on serverless applications. It works across cloud providers (AWS support is the strongest though) and only exposes features related to building applications with cloud functions.</p>
<p>The benefit is that it&#39;s much more concise. Once again, the tool is using YAML to define the templates, here is the snippet from <a href="https://github.com/danielireson/serverless-url-shortener">Serverless URL Shortener</a> example:</p>
<pre><code class="language-yaml">functions:
  store:
    handler: api/store.handle
    events:
      - http:
          path: /
          method: post
          cors: true</code></pre>
<p>The domain-specific language yields a shorter definition: this example has 45 lines of YAML + 123 lines of JavaScript functions.</p>
<p>However, the conciseness has a flip side: as soon as you veer outside of the fairly &quot;thin&quot; golden path&mdash;the cloud functions and an incomplete list of event sources&mdash;you have to fall back to more generic tools like CloudFormation. As soon as your landscape includes lower-level infrastructure work or some container-based components, you&#39;re stuck using multiple config languages and tools again.</p>
<p>Amazon&#39;s <a href="https://docs.aws.amazon.com/serverless-application-model/index.html">AWS Serverless Application Model</a> (SAM) looks very similar to the Serverless Framework but tailored to be AWS-specific.</p>
<p>Is that the end game? I don&#39;t think so.</p>
<h2 id="desired-properties-of-infrastructure-definition-tool">Desired Properties of Infrastructure Definition Tool</h2>
<p>So what have we learned while going through the existing landscape? The perfect infrastructure tools should:</p>
<ul>
<li>Provide <strong>reproducible</strong> results of deployments</li>
<li>Be <strong>scriptable</strong>, i.e., require no human intervention after the definition is complete</li>
<li>Define <strong>desired state</strong> rather than exact steps to achieve it</li>
<li>Support <strong>multiple cloud providers</strong> and hybrid scenarios</li>
<li>Be <strong>universal</strong> in the sense of using the same tool to define any type of resource</li>
<li>Be <strong>succinct</strong> and <strong>concise</strong> to stay readable and manageable</li>
<li><del>Use YAML-based format</del></li>
</ul>
<p>Nah, I crossed out the last item. YAML seems to be the most popular language among this class of tools (and I haven&#39;t even touched Kubernetes yet!), but I&#39;m not convinced it works well for me. <a href="https://noyaml.com/">YAML has many flaws, and I just don&#39;t want to use it</a>.</p>
<p>Have you noticed that I haven&#39;t mentioned <strong>Infrastructure as code</strong> a single time yet? Well, here we go (from <a href="https://en.wikipedia.org/wiki/Infrastructure_as_code">Wikipedia</a>):</p>
<blockquote>
<p>Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.</p>
</blockquote>
<p>Shouldn&#39;t it be called &quot;Infrastructure as definition files&quot;, or &quot;Infrastructure as YAML&quot;?</p>
<p>As a software developer, what I really want is &quot;Infrastructure as actual code, you know, the program thing&quot;. I want to use <strong>the same language</strong> that I already know. I want to stay in the same editor. I want to get IntelliSense <strong>auto-completion</strong> when I type. I want to see the <strong>compilation errors</strong> when what I typed is not syntactically correct. I want to reuse the <strong>developer skills</strong> that I already have. I want to come up with <strong>abstractions</strong> to generalize my code and create <strong>reusable components</strong>. I want to <strong>leverage open-source community</strong> who would create much better components than I ever could. I want to <strong>combine the code and infrastructure</strong> in one code project.</p>
<p>If you are with me on that, keep reading. You get all of that with <a href="https://pulumi.io/">Pulumi</a>.</p>
<h2 id="pulumi">Pulumi</h2>
<p><a href="https://pulumi.io/">Pulumi</a> is a tool to build cloud-based software using real programming languages. They support all major cloud providers, plus Kubernetes.</p>
<p>Pulumi programming model supports Go and Python too, but I&#39;m going to use TypeScript for the rest of the article.</p>
<p>While prototyping a URL shortener, I explain the fundamental way of working and illustrate the benefits and some trade-offs. If you want to follow along, <a href="https://pulumi.io/quickstart/install.html">install Pulumi</a>.</p>
<h2 id="how-pulumi-works">How Pulumi Works</h2>
<p>Let&#39;s start defining our URL shortener application in TypeScript. I installed <code>@pulumi/pulumi</code> and <code>@pulumi/aws</code> NPM modules so that I can start the program. The first resource to create is a DynamoDB table:</p>
<pre><code class="language-typescript">import * as aws from &quot;@pulumi/aws&quot;;

// A DynamoDB table with a single primary key
let counterTable = new aws.dynamodb.Table(&quot;urls&quot;, {
    name: &quot;urls&quot;,
    attributes: [
        { name: &quot;name&quot;, type: &quot;S&quot; },
    ],
    hashKey: &quot;name&quot;,
    readCapacity: 1,
    writeCapacity: 1
});</code></pre>
<p>I use <code>pulumi</code> CLI to run this program to provision the actual resource in AWS:</p>
<pre><code class="language-console">&gt; pulumi up

Previewing update (urlshortener):

     Type                   Name             Plan
 +   pulumi:pulumi:Stack    urlshortener     create
 +    aws:dynamodb:Table    urls             create

Resources:
    + 2 to create

Do you want to perform this update? yes
Updating (urlshortener):

     Type                   Name             Status
 +   pulumi:pulumi:Stack    urlshortener     created
 +    aws:dynamodb:Table    urls             created

Resources:
    + 2 created</code></pre>
<p>The CLI first shows the preview of the changes to be made, and when I confirm, it creates the resource. It also creates a <em>stack</em>&mdash;a container for all the resources of the application.</p>
<p>This code might look like an imperative command to create a DynamoDB table, but it actually isn&#39;t. If I go ahead and change <code>readCapacity</code> to <code>2</code> and then re-run <code>pulumi up</code>, it produces a different outcome:</p>
<pre><code class="language-console">&gt; pulumi up

Previewing update (urlshortener):

     Type                   Name             Plan
     pulumi:pulumi:Stack    urlshortener        
 ~   aws:dynamodb:Table     urls             update  [diff: ~readCapacity]

Resources:
    ~ 1 to update
    1 unchanged</code></pre>
<p>It detects the exact change that I made and suggests an update. The following picture illustrates how Pulumi works:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/how-pulumi-works.png" alt="How Pulumi works"></p>
<figcaption>How Pulumi works</figcaption>

<p><code>index.ts</code> in the red square is my program. Pulumi&#39;s language host understands TypeScript and translates the code to commands to the internal engine. As a result, the engine builds a tree of resources-to-be-provisioned, the desired state of the infrastructure.</p>
<p>The end state of the last deployment is persisted in the storage (can be in pulumi.com backend or a file on disk). The engine then compares the current state of the system with the desired state of the program and calculates the delta in terms of create-update-delete commands to the cloud provider.</p>
<h2 id="help-of-types">Help Of Types</h2>
<p>Now I can proceed to the code that defines a Lambda function:</p>
<pre><code class="language-typescript">// Create a Role giving our Lambda access.
let policy: aws.iam.PolicyDocument = { /* Redacted for brevity */ };
let role = new aws.iam.Role(&quot;lambda-role&quot;, {
    assumeRolePolicy: JSON.stringify(policy),
});
let fullAccess = new aws.iam.RolePolicyAttachment(&quot;lambda-access&quot;, {
    role: role,
    policyArn: aws.iam.AWSLambdaFullAccess,
});

// Create a Lambda function, using code from the `./app` folder.
let lambda = new aws.lambda.Function(&quot;lambda-get&quot;, {
    runtime: aws.lambda.NodeJS8d10Runtime,
    code: new pulumi.asset.AssetArchive({
        &quot;.&quot;: new pulumi.asset.FileArchive(&quot;./app&quot;),
    }),
    timeout: 300,
    handler: &quot;read.handler&quot;,
    role: role.arn,
    environment: { 
        variables: {
            &quot;COUNTER_TABLE&quot;: counterTable.name
        }
    },
}, { dependsOn: [fullAccess] });</code></pre>
<p>You can see that the complexity kicked in and the code size is growing. However, now I start to gain real benefits from using a typed programming language:</p>
<ul>
<li>I&#39;m using objects in the definitions of other object&#39;s parameters. If I misspell their name, I don&#39;t get a runtime failure but an immediate error message from the editor.</li>
<li>If I don&#39;t know which options I need to provide, I can go to the type definition and look it up (or use IntelliSense).</li>
<li>If I forget to specify a mandatory option, I get a clear error.</li>
<li>If the type of the input parameter doesn&#39;t match the type of the object I&#39;m passing, I get an error again.</li>
<li>I can use language features like <code>JSON.stringify</code> right inside my program. In fact, I can reference and use any NPM module.</li>
</ul>
<p>You can see the code for API Gateway <a href="https://github.com/mikhailshilkov/fosdem2019/blob/master/samples/1-raw/index.ts#L60-L118">here</a>. It looks too verbose, doesn&#39;t it? Moreover, I&#39;m only half-way through with only one Lambda function defined.</p>
<h2 id="reusable-components">Reusable Components</h2>
<p>We can do better than that. Here is the improved definition of the same Lambda function:</p>
<pre><code class="language-typescript">import { Lambda } from &quot;./lambda&quot;;

const func = new Lambda(&quot;lambda-get&quot;, {
    path: &quot;./app&quot;,
    file: &quot;read&quot;,
    environment: { 
       &quot;COUNTER_TABLE&quot;: counterTable.name
    },
});</code></pre>
<p>Now, isn&#39;t that beautiful? Only the essential options remained, while all the machinery is gone. Well, it&#39;s not completely gone, it&#39;s been hidden behind an <em>abstraction</em>.</p>
<p>I defined a <strong>custom component</strong> called <code>Lambda</code>:</p>
<pre><code class="language-typescript">export interface LambdaOptions {
    readonly path: string;
    readonly file: string;

    readonly environment?:  pulumi.Input&lt;{
        [key: string]: pulumi.Input&lt;string&gt;;
    }&gt;;    
}

export class Lambda extends pulumi.ComponentResource {
    public readonly lambda: aws.lambda.Function;

    constructor(name: string,
        options: LambdaOptions,
        opts?: pulumi.ResourceOptions) {

        super(&quot;my:Lambda&quot;, name, opts);

        const role = //... Role as defined in the last snippet
        const fullAccess = //... RolePolicyAttachment as defined in the last snippet

        this.lambda = new aws.lambda.Function(`${name}-func`, {
            runtime: aws.lambda.NodeJS8d10Runtime,
            code: new pulumi.asset.AssetArchive({
                &quot;.&quot;: new pulumi.asset.FileArchive(options.path),
            }),
            timeout: 300,
            handler: `${options.file}.handler`,
            role: role.arn,
            environment: {
                variables: options.environment
            }
        }, { dependsOn: [fullAccess], parent: this });
    }
}</code></pre>
<p>The interface <code>LambdaOptions</code> defines options that are important for my abstraction. The class <code>Lambda</code> derives from <code>pulumi.ComponentResource</code> and creates all the child resources in its constructor. </p>
<p>A nice effect is that one can see the structure in <code>pulumi</code> preview:</p>
<pre><code class="language-console">Previewing update (fosdem-component-urlshortener):

     Type                                Name                  Plan
 +   pulumi:pulumi:Stack                 urlshortener          create
 +     my:Lambda                         lambda-get            create
 +       aws:iam:Role                    lambda-get-role       create
 +       aws:iam:RolePolicyAttachment    lambda-get-access     create
 +       aws:lambda:Function             lambda-get-func       create
 +     aws:dynamodb:Table                urls                  create</code></pre>
<p>The <code>Endpoint</code> component simplifies definition of API Gateway (see <a href="https://github.com/mikhailshilkov/fosdem2019/blob/master/samples/2-components/endpoint.ts">the source</a>):</p>
<pre><code class="language-typescript">const api = new Endpoint(&quot;urlapi&quot;, {
    path: &quot;/{proxy+}&quot;,
    lambda: func.lambda
});</code></pre>
<p>The component hides the complexity from the clients; if the abstraction was selected correctly, that is. The component class can be reused in multiple places, in several projects, across teams, etc.</p>
<h2 id="standard-component-library">Standard Component Library</h2>
<p>In fact, Pulumi team came up with lots of high-level components that build abstractions on top of raw resources. The components from <code>@pulumi/cloud-aws</code> package are particularly useful for serverless applications.</p>
<p>Here is the full URL shortener application with DynamoDB table, Lambdas, API Gateway, and S3-based static files:</p>
<pre><code class="language-typescript">import * as aws from &quot;@pulumi/cloud-aws&quot;;

// Create a table `urls`, with `name` as primary key.
let urlTable = new aws.Table(&quot;urls&quot;, &quot;name&quot;);

// Create a web server.
let endpoint = new aws.API(&quot;urlshortener&quot;);

// Serve all files in the www directory to the root.
endpoint.static(&quot;/&quot;, &quot;www&quot;);

// GET /url/{name} redirects to the target URL based on a short-name.
endpoint.get(&quot;/url/{name}&quot;, async (req, res) =&gt; {
    let name = req.params[&quot;name&quot;];
    let value = await urlTable.get({name});
    let url = value &amp;&amp; value.url;

    // If we found an entry, 301 redirect to it; else, 404.
    if (url) {
        res.setHeader(&quot;Location&quot;, url);
        res.status(301);
        res.end(&quot;&quot;);
    }
    else {
        res.status(404);
        res.end(&quot;&quot;);
    }
});

// POST /url registers a new URL with a given short-name.
endpoint.post(&quot;/url&quot;, async (req, res) =&gt; {
    let url = req.query[&quot;url&quot;];
    let name = req.query[&quot;name&quot;];
    await urlTable.insert({ name, url });
    res.json({ shortenedURLName: name });
});

export let endpointUrl = endpoint.publish().url;</code></pre>
<p>The coolest thing here is that the actual <em>implementation code</em> of AWS Lambdas is <a href="https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions">intertwined</a> with the <em>definition of resources</em>. The code looks very similar to an Express application. AWS Lambdas are defined as TypeScript lambdas. All strongly typed and compile-time checked.</p>
<p>It&#39;s worth noting that at the moment such high-level components only exist in TypeScript. One could create their custom components in Python or Go, but there is no standard library available. Pulumi folks <a href="https://github.com/pulumi/pulumi/issues/2430">are actively trying to figure out a way to bridge this gap</a>.</p>
<h2 id="avoiding-vendor-lock-in-">Avoiding Vendor Lock-in?</h2>
<p>If you look closely at the previous code block, you notice that only one line is AWS-specific: the <code>import</code> statement. The rest is just naming. </p>
<p>We can get rid of that one too: just change the import to <code>import * as cloud from &quot;@pulumi/cloud&quot;;</code> and replace <code>aws.</code> with <code>cloud.</code> everywhere. Now, we&#39;d have to go to the stack configuration file and specify the cloud provider there:</p>
<pre><code class="language-yaml">config:
  cloud:provider: aws</code></pre>
<p>Which is enough to make the application work again!</p>
<p>Vendor lock-in seems to be a big concern among many people when it comes to cloud architectures heavily relying on managed cloud services, including serverless applications. While I don&#39;t necessarily share those concerns and am not sure if generic abstractions are the right way to go, Pulumi Cloud library can be one direction for the exploration.</p>
<p>The following picture illustrates the choice of the level of abstraction that Pulumi provides:</p>
<p><img src="/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/pulumi-layers.png" alt="Pulumi abstraction layers"></p>
<figcaption>Pulumi abstraction layers</figcaption>

<p>Working on top of the cloud provider&#39;s API and internal resource provider, you can choose to work with raw components with maximum flexibility, or opt-in for higher-level abstractions. Mix-and-match in the same program is possible too.</p>
<h2 id="infrastructure-as-real-code">Infrastructure as Real Code</h2>
<p>Designing applications for the modern cloud means utilizing multiple cloud services which have to be configured to play nicely together. The Infrastructure as Code approach is almost a requirement to keep the management of such applications reliable in a team setting and over the extended period.</p>
<p>Application code and supporting infrastructure become more and more blended, so it&#39;s natural that software developers take the responsibility to define both. The next logical step is to use the same set of languages, tooling, and practices for both software and infrastructure.</p>
<p>Pulumi exposes cloud resources as APIs in several popular general-purpose programming languages. Developers can directly transfer their skills and experience to define, build, compose, and deploy modern cloud-native and serverless applications more efficiently than ever.</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/pulumi/'>Pulumi</a>, <a href='/tags/typescript/'>TypeScript</a>, <a href='/tags/infrastructure-as-code/'>Infrastructure as Code</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Jan 21st, 2019</div>
    
    <h1><a href='/2019/serverless-at-scale-serving-stackoverflow-like-traffic/'>Serverless at Scale: Serving StackOverflow-like Traffic</a></h1>
    

    
        <div class="remark">Originally published at <a href='https://blog.binaris.com/serverless-at-scale/'>Binaris Blog</a></div>
    

    <div class="post-content">
        <p>Serverless compute is a very productive and quick way to get an application up and running. A developer writes a piece of code that solves a particular task and uploads it to the cloud. The provider handles code deployment and the ops burden of managing all the required infrastructure, so that the Function is always available, secure and performant.</p>
<p>Performance is a feature, and the ability to run the same application for 10 users or 10 million users is very appealing. Unfortunately, FaaS is not magical, so scalability limits do exist. That&#39;s why I spend time testing the existing FaaS services to highlight the cases where performance might not be perfect. For some background, you can read my previous articles:  <a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/">From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a> for queue-based workloads and <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">Serverless: Cold Start War</a> for exploring cold start latencies.</p>
<p>Today, I want to dig into the scalability of serverless HTTP-based functions. HTTP-based functions are a popular use case that most developers can relate to, and they are also heavily impacted by the ability to scale. When your app goes viral on social networks, scores the Hacker News front page, or gets featured on TV, the last thing you want is slow responses and timeouts.</p>
<p>I implemented a simple HTTP-triggered function and deployed it across the Big-3 cloud providers&mdash;Amazon, Microsoft, and Google. Next, I ran a load test issuing hundreds of requests per second to each function. In this article, I present the design and the results of these experiments.</p>
<p><em>DISCLAIMER: Performance testing is hard. I might be missing some crucial factors and parameters that influence the outcome. My interpretation might be wrong. The results might change over time. If you happen to know a way to improve my tests, please let me know, and I will re-run them and re-publish the results.</em></p>
<h2 id="stackoverflow-on-faas">StackOverflow on FaaS</h2>
<p>Every developer knows <a href="https://stackoverflow.com/">StackOverflow</a> and uses it pretty much every day. I&#39;ve made the goal to serve traffic comparable to what StackOverflow sees, solely from a serverless function.</p>
<p>StackOverflow is an excellent target for many reasons:</p>
<ul>
<li>Publish the actual request statistics from the site (see <a href="https://nickcraver.com/blog/2016/02/17/stack-overflow-the-architecture-2016-edition/">the data from 2016</a>)</li>
<li>Very transparent about their tech stack (same link above)</li>
<li>Publish  <a href="https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/">the full database</a> and provide <a href="https://data.stackexchange.com/stackoverflow/queries">a tool to query the data online</a></li>
</ul>
<p>StackOverflow runs on .NET Core, SQL Server, Redis, Elastic, etc. Obviously, my goal is not to replicate the whole site. I just want to serve the comparable traffic to the outside world.</p>
<p>Here are some important metrics for my experiment:</p>
<ul>
<li>StackOverflow served 66 million pages per day, which is 760 pages/sec on average.</li>
<li>We’ll make the assumption that the vast majority of those pageviews are question pages, so I will ignore everything else.</li>
<li>We know they serve the whole page as one server-rendered HTML, so we’ll do something comparable.</li>
<li>Each page should be ~ 100kb size before compression.</li>
</ul>
<p>With this in mind, I came up with the following experiment design:</p>
<ul>
<li>Create a HTML template for the whole question page with question and answer markup replaced by placeholders.</li>
<li>Download the data of about 1000 questions and their respective answers from the <a href="https://data.stackexchange.com/stackoverflow/query/new">the data explorer</a>.</li>
<li>Save the HTML templates and JSON data in blob storage of each cloud provider.</li>
<li>Implement a serverless function that retrieves the question data, populates the template, and returns the HTML in response.</li>
</ul>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/stackoverflow-test-setup.png" alt="Serving StackOverflow Traffic from a Serverless Function"></p>
<figcaption>Serving StackOverflow Traffic from a Serverless Function</figcaption>

<p>The HTML template is loaded at the first request and then cached in memory. The question/answers data file is loaded from the blob storage for every request. Template population is accomplished with string concatenation.</p>
<p>In my view, this setup is a simple but fair approximation of StackOverflow’s front-end. In addition, it is somewhat representative of many real-world web applications.</p>
<h2 id="metrics-setup">Metrics Setup</h2>
<p>I analyzed the scalability of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via Amazon API Gateway (<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/with-on-demand-https.html">docs</a>)</li>
<li>Azure Function with an HTTP trigger (<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook">docs</a>)</li>
<li>Google Cloud HTTP Function (<a href="https://cloud.google.com/functions/docs/writing/http">docs</a>)</li>
</ul>
<p>All functions were implemented in JavaScript (Node.js) and were running on the latest GA runtime.</p>
<p>Since built-in monitoring tools, such as CloudWatch, would only report the function execution duration, which does not include other potential delays in the HTTP pipeline, I instead measured end-to-end latency from the client perspective. This means that the latency of the network and HTTP gateway (e.g., API Gateway in the case of AWS) were included in the total duration.</p>
<p>Requests were sent from multiple VMs outside the target cloud provider&#39;s region but in geographical proximity. Network latency was present in the metrics, but I estimated it to be 20-30 milliseconds at most.</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/measuring-response-time.png" alt="Measuring Response Time of a Serverless Function"></p>
<figcaption>Measuring Response Time of a Serverless Function</figcaption>

<p>Blob storage services of all cloud providers have enough throughput to serve one blob per HTTP request. However, the latencies differ among the clouds, so I included blob fetch duration measurements in the performance baseline.</p>
<p>Each measurement was then saved to persistent storage and analyzed afterward.</p>
<p>The charts below show <a href="https://en.wikipedia.org/wiki/Percentile">percentile</a> values. For instance, the 95th percentile (written as P95) value of 100ms means that 95% of the requests were faster than 100ms while 5% were slower than that. P50 is the median.</p>
<h2 id="load-pattern">Load Pattern</h2>
<p>I wanted to test the ability of serverless functions to scale up rapidly in response to the growth in request rate, so I came up with a dynamic load scenario.</p>
<p>The experiments started with a baseline 10% of the target load. The goal of the baseline was to make sure that the app was overall healthy, to evaluate the basic latency and the impact of blob storage on it.</p>
<p>At some point (around minute 0 of the charts), the load began to grow and reached 1000 RPS within 8 minutes. After the peak, the cooldown period started, and the load steadily decreased to zero in 8 more minutes.</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/request-distribution.png" alt="Request Distribution during the Load Test"></p>
<figcaption>Request Distribution during the Load Test</figcaption>

<p>Even though the growth period on the left and the decline period on the right represented the same number of requests, the hypothesis was that the first half might be more challenging because of the need to provision new resources rapidly.</p>
<p>In total, 600,000 requests were served within 17 minutes with the total outbound traffic of 70 GB.</p>
<p>Finally, we’ve made it through all the mechanics, and now it&#39;s time to present the actual results.</p>
<h2 id="aws-lambda">AWS Lambda</h2>
<p>AWS Lambda was our first target for the experiment. I provisioned 512 MB size for Lambda instances, which is a medium-range value. I expected larger instances to be slightly faster, and smaller instances to be a bit slower, but the load is not very demanding to CPU, so the overall results should be comparable across the spectrum.</p>
<p>During the low-load baseline, the median response time was about 70 ms with a minimum of 50 ms. The median response time from the S3 bucket was 50 ms.</p>
<p>Here is the P50-P95 latency chart during the load test:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/aws-lambda-p50-p95.png" alt="AWS Lambda Response Time Distribution (P50-P95)"></p>
<figcaption>AWS Lambda Response Time Distribution (P50-P95)</figcaption>

<p>The percentiles were very consistent and flat. The median response time was still around 70 ms with no variance observed. P90 and P95 were quite stable too.</p>
<p>Only the 99th percentile displayed the difference between the ramp-up period on the left and the cooldown period on the right:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/aws-lambda-p99.png" alt="AWS Lambda Response Time Distribution (P99)"></p>
<figcaption>AWS Lambda Response Time Distribution (P99)</figcaption>

<p>AWS Lambda scales by creating multiple instances of the same function that handle the requests in parallel. Each Lambda instance is handling a single request at any given time, which is why the scale is measured in &quot;concurrent executions.&quot; When the current request is done being processed, the same instance can be reused for a subsequent request.</p>
<p>Instance identifier can be retrieved from <code>/proc/self/cgroup</code> of a lambda, so I recorded this value for each execution. The following chart shows the number of instances throughout the experiment:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/aws-lambda-concurrent-executions.png" alt="AWS Lambda Concurrent Executions"></p>
<figcaption>AWS Lambda Concurrent Executions</figcaption>There were about 80 concurrent executions at peak. That&#39;s quite a few, but still, almost an order of magnitude fewer instances compared to <a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/#crunching-numbers">my queue processing experiment</a>. It felt that AWS was capable of scaling even further.<p></p>
<p>P99.9 showed slowness of the least lucky 0.1% requests. Most probably, it had lots of cold starts in it:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/aws-lambda-p999.png" alt="AWS Lambda Response Time Distribution (P99.9)"></p>
<figcaption>AWS Lambda Response Time Distribution (P99.9)</figcaption>Still, even those requests were mostly served within 2-3 seconds.<p></p>
<h2 id="google-cloud-functions">Google Cloud Functions</h2>
<p>Now, let&#39;s look at the results of Google Cloud Functions. Once again I provisioned 512 MB instance size (same as for Lambda).</p>
<p>During the low-load baseline, the median response time was about 150 ms with a minimum of 100 ms. Almost all of that time was spent fetching blobs from Cloud Storage: Its median latency was over 130 ms! I haven&#39;t spent too much time investigating the reason, but I assume that Google Cloud Storage has higher latency for small files than S3. Zach Bjornson published  <a href="http://blog.zachbjornson.com/2015/12/29/cloud-storage-performance.html">the comparison of storage latencies</a>. Although it&#39;s 3 years old, the conclusion was that &quot;GCS averaged more than three times higher latency&quot; when compared to Azure and AWS.</p>
<p>That&#39;s an important observation because the Function execution times were twice as big as those recorded on AWS. Keeping this difference in mind, here is the P50-P95 latency chart during the GCP load test:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/google-cloud-function-p50-p95.png" alt="Google Cloud Function Response Time Distribution (P50-P95)"></p>
<figcaption>Google Cloud Function Response Time Distribution (P50-P95)</figcaption>The median value was stable and flat at 150-180 ms. P90 and P95 had some spikes during the first 3 minutes. Google passed the test, but the lower percentiles were not perfect.<p></p>
<p>The 99th percentile was relatively solid though. It was higher on the left, but it stayed within 1 second most of the time:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/google-cloud-function-p99.png" alt="Google Cloud Function Response Time Distribution (P99)"></p>
<figcaption>Google Cloud Function Response Time Distribution (P99)</figcaption>The scaling model of Google Functions appeared to be very similar to the one of AWS Lambda. This means that 2x duration of the average execution required 2x more concurrent executions to run and 2x more instances to be provisioned:<p></p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/google-cloud-function-instances.png" alt="Google Cloud Function Concurrent Executions"></p>
<figcaption>Google Cloud Function Concurrent Executions</figcaption>Indeed, there were about 160 concurrent executions at peak. GCP had to work twice as hard because of the storage latency, which might explain some of the additional variations of response time.<p></p>
<p>Besides, Google seems to manage instance lifecycle differently. It provisioned a larger batch of instances during the first two minutes, which was in line with <a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/#pause-the-world-workload">my previous findings</a>. It also kept instances for longer when the traffic went down (or at least, reused the existing instances more evenly).</p>
<p>For completeness, here are the P99.9 values:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/google-cloud-function-p999.png" alt="Google Cloud Function Response Time Distribution (P99.9)"></p>
<figcaption>Google Cloud Function Response Time Distribution (P99.9)</figcaption>They fluctuated between 1 and 5 seconds on the left and were incredibly stable on the right.<p></p>
<h2 id="azure">Azure</h2>
<p>Experiments with Azure Functions were run on <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
&mdash;the dynamically scaled and billed-per-execution runtime. Consumption Plan doesn&#39;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>During the low-load baseline, the median response time was about 95 ms with a minimum of 45 ms, which is close to AWS and considerably faster than GCP. This time, JSON file retrieval was not the main contributor to the end-to-end latency: The median response time of Azure Blob Storage was an amazing 8 ms.</p>
<p>However, it turns out that the scaling model of Azure Functions doesn&#39;t work well for my experiment. Very high latencies were observed during the load test on Azure:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/azure-function-js-p50-p95.png" alt="Azure Function (Node.js) Response Time Distribution (P50-P95)"></p>
<figcaption>Azure Function (Node.js) Response Time Distribution (P50-P95)</figcaption>The concurrency model of Azure Functions is different from the counterparts of AWS/GCP. Function App instance is closer to a VM than a single-task container. It runs multiple concurrent executions in parallel. A central coordinator called Scale Controller monitors the metrics from existing instances and determines how many instances to provision on top. Instance identifier can be retrieved from environment variables of a function, so I recorded this value for each execution.<p></p>
<p>The multiple-requests-at-one-instance model didn&#39;t help in terms of the total instances required to process the traffic:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/azure-function-js-instances.png" alt="Azure Function (Node.js) Instances"></p>
<figcaption>Azure Function (Node.js) Instances</figcaption>At peak, 90 instances were required, which is almost the same as the number of current executions of AWS Lambda. Given the I/O bound nature of my function, this was surprising to me.<p></p>
<p>Puzzled by the moderate results, I decided to run the same application as a .NET Azure Function and compare the performance. The same function ported to C# got much faster:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/azure-function-dotnet-p50-p95.png" alt="Azure Function (.NET) Response Time Distribution (P50-P95)"></p>
<figcaption>Azure Function (.NET) Response Time Distribution (P50-P95)</figcaption>P50 was extremely good: It stayed below 50 ms (leveraging the blazingly fast Blob Storage) for the whole period except for one point when it was 140 ms. P90 and P95 were stable except for three data points.<p></p>
<p>The chart of instance growth was very different from the JavaScript one too:</p>
<p><img src="/2019/serverless-at-scale-serving-stackoverflow-like-traffic/azure-function-dotnet-instances.png" alt="Azure Function (.NET) Instances"></p>
<figcaption>Azure Function (.NET) Instances</figcaption>Basically, it spiked to 20 instances at the third minute, and that was enough for the rest of the test. I concluded that the .NET worker was more efficient compared to Node.js worker, at least for my scenario.<p></p>
<p>If I compare the percentile charts with the instance charts, it looks as if the latency spikes happen at the time when new instances get provisioned. For some reason, the performance suffers during the scale out. It&#39;s not just cold starts at the new instances: P90 and even P50 are affected. It might be a good topic for a separate investigation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>During the experiment, sample StackOverflow pages were built and served from AWS Lambda, Google Cloud Functions, and Azure Functions at the rate of up to 1000 pageviews per second. Each Function call served a single pageview and was a combination of I/O workload (reading blob storage) and CPU usage (for parsing JSON and rendering HTML).</p>
<p>All cloud providers were able to scale up and serve the traffic. However, the latency distributions were quite different.</p>
<p>AWS Lambda was solid: Median response time was always below 100 ms, 95th percentile was below 200 ms, and 99th percentile exceeded 500 ms just once.</p>
<p>Google Cloud Storage seemed to have the highest latency out of the three cloud providers. Google Cloud Functions had a bit of a slowdown during the first two minutes of the scale-out but otherwise were quite stable and responsive.</p>
<p>Azure Functions had difficulties during the scale-out period and the response time went up to several seconds. .NET worker appeared to be more performant compared to Node.js one, but both of them show undesirable spikes when new instances are provisioned.</p>
<p>Here is my practical advice to take home:</p>
<ul>
<li>Function-as-a-Service is a great model to build applications that can work for low-usage scenarios, high-load applications, and even spiky workloads.</li>
<li>Scalability limits do exist, so if you anticipate high growth in the application&#39;s usage, run a simple load test to see how it behaves.</li>
<li>Always test in combination with your non-serverless dependencies. I&#39;ve selected scalable-by-definition cloud blob storage, and yet I got some influence of its behavior on the results. If you use a database or a third-party service, it&#39;s quite likely they will hit the scalability limit much earlier than the serverless compute.</li>
</ul>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/scalability/'>Scalability</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/gcp/'>GCP</a>, <a href='/tags/google-cloud-functions/'>Google Cloud Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Dec 20th, 2018</div>
    
    <h1><a href='/2018/12/fairy-tale-of-fsharp-and-durable-functions/'>A Fairy Tale of F# and Durable Functions</a></h1>
    

    

    <div class="post-content">
        <p><em>The post is a part of 
<a href="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/">F# Advent Calendar 2018</a>.
It&#39;s Christmas time!</em></p>
<p>This summer I was hired by the office of Santa Claus. Santa is not just a fairy tale
character on his own&mdash;he leads a large organization that supplies gifts and happiness to millions of 
children around the globe. Like any large organization, Santa&#39;s office employs an impressive number of 
IT systems. </p>
<p>As part of its IT modernization
effort, North Pole HQ restructured the whole supply chain of Christmas gifts. Many legacy components were moved from
a self-managed data center at the North Pole&mdash;although the cooling is quite cheap there&mdash;to 
Azure cloud. Azure was an easy sell since Santa&#39;s techy elves use Office 365, SharePoint and
the .NET development stack.</p>
<p>One of the goals of the redesign was to leverage managed cloud services and serverless architecture
wherever possible. Santa has no spare elves to keep reinventing IT wheels.</p>
<h2 id="wish-fulfillment-service">Wish Fulfillment Service</h2>
<p>My assignment was to redesign the <strong>Wish Fulfillment</strong> service. The service receives
wish lists from clients (they call children &quot;clients&quot;):</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/wish-list.png" alt="Christmas Wish List"></p>
<center class="img-caption">Christmas Card with a Wish List &copy; my son Tim</center>

<p>Luckily, the list is already parsed by some other service, and also contains the metadata about
the kid&#39;s background (age, gender, and so on) and preferences.</p>
<p>For each item in the list, our service calls the <strong>Matching</strong> service, which uses machine learning,
Azure Cognitive services, and a bit of magic to determine the actual products (they call gifts &quot;products&quot;)
that best fit the client&#39;s expressed desire and profile. For instance, my son&#39;s wish for &quot;LEGO Draak&quot; matches
to &quot;LEGO NINJAGO Masters of Spinjitzu Firstbourne Red Dragon&quot;. You get the point.</p>
<p>There might be several matches for each desired item, and each result has an estimate of how
likely it is to fulfill the original request and make the child happy.</p>
<p>All the matching products are combined and sent over to the <strong>Gift Picking</strong> service. Gift Picking selects one
of the options based on its price, demand, confidence level, and the Naughty-or-Nice score of the client.</p>
<p>The last step of the workflow is to <strong>Reserve</strong> the selected gift in the warehouse and shipping system
called &quot;Santa&#39;s Archive of Products&quot;, also referred to as SAP.</p>
<p>Here is the whole flow in one picture:</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/gift-fulfillment-service.png" alt="Gift Fulfillment Workflow"></p>
<center class="img-caption">Gift Fulfillment Workflow</center>

<p>How should we implement this service?</p>
<h2 id="original-design">Original Design</h2>
<p>The Wish Fulfillment service should run in the cloud and integrate with other services. It
should be able to process millions of requests in December and stay very cheap to run during the
rest of the year. We decided to leverage serverless architecture with 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview">Azure Functions</a> on the 
<a href="https://azure.microsoft.com/en-us/pricing/details/functions/">Consumption Plan</a>. Serverless
Functions are:</p>
<ul>
<li><p><strong>Fully Managed</strong>: the cloud provider provisions resources, scales them based on the load, takes
care of uptime and reliability;</p>
</li>
<li><p><strong>Event-Driven</strong>: for each serverless Function you have to define a specific trigger&mdash;the 
event type which causes it to run, be it an HTTP endpoint or a queue message;</p>
</li>
<li><p><strong>Changed per Execution</strong>: it costs nothing to run the application if there is no usage,
and the cost of busy applications is proportional to the actual resource utilization.</p>
</li>
</ul>
<p>Here is the diagram of the original design:</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/azure-functions-diagram.png" alt="Workflow Design with Azure Functions and Storage Queues"></p>
<center class="img-caption">Workflow Design with Azure Functions and Storage Queues</center>

<p>We used Azure Storage Queues to keep the whole flow asynchronous and more resilient to failures
and load fluctuation.</p>
<p>This design would mostly work, but we found a couple of problems with it:</p>
<ul>
<li><p>The Functions were manually wired via storage queues and corresponding bindings. The workflow
was spread over infrastructure definition and thus was hard to grasp.</p>
</li>
<li><p>We had to pass all items of each wish list into a single invocation of Matching Function,
otherwise combining the matching results from multiple queue messages would be tricky. </p>
</li>
<li><p>Although not in scope for the initial release, there were plans to add manual elf 
intervention for poorly matched items. This feature would require a change in the flow design:
it&#39;s not trivial to fit long-running processes into the pipeline.</p>
</li>
</ul>
<p>To improve on these points, we decided to try 
<a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Durable Functions</a>&mdash;a library 
that brings workflow orchestration to Azure Functions. It introduces several tools to define stateful,
potentially long-running operations, and handles a lot of the mechanics of reliable communication 
and state management behind the scenes.</p>
<p>If you want to know more about what Durable Functions are and why they might be a good idea,
I invite you to read my article 
<a href="https://mikhail.io/2018/12/making-sense-of-azure-durable-functions/">Making Sense of Azure Durable Functions</a>
(20 minutes read).</p>
<p>For the rest of this post, I will walk you through the implementation of the Wish Fulfillment workflow
with Azure Durable Functions.</p>
<h2 id="domain-model">Domain Model</h2>
<p>A good design starts with a decent domain model. Luckily, the project was built with F#&mdash;the language with
the richest domain modeling capabilities in the .NET ecosystem.</p>
<h3 id="types">Types</h3>
<p>Our service is invoked with a wish list as the input parameter, so let&#39;s start with the type <code>WishList</code>:</p>
<pre><code class="language-fsharp">type WishList = 
{
    Kid: Customer
    Wishes: string list
}</code></pre>
<p>It contains information about the author of the list and recognized &quot;order&quot; items. <code>Customer</code> is a custom type;
for now, it&#39;s not important what&#39;s in it.</p>
<p>For each wish we want to produce a list of possible matches:</p>
<pre><code class="language-fsharp">type Match = 
{
    Product: Product
    Confidence: Probability
}</code></pre>
<p>The product is a specific gift option from Santa&#39;s catalog, and the confidence is a number 
from <code>0.0</code> to <code>1.0</code> of how strong the match is.</p>
<p>The end goal of our service is to produce a <code>Reservation</code>:</p>
<pre><code class="language-fsharp">type Reservation = 
{
    Kid: Customer
    Product: Product
}</code></pre>
<p>It represents the exact product selection for the specific kid.</p>
<h3 id="functions">Functions</h3>
<p>The Wish Fulfillment service needs to perform three actions, which can be
modeled with three strongly-typed asynchronous functions.</p>
<p><em>Note: I use lowercase &quot;function&quot; for F# functions and capitalize &quot;Function&quot; for Azure Functions
throughout the article to minimize confusion.</em></p>
<p>The <strong>first action</strong> finds matches for each wish:</p>
<pre><code class="language-fsharp">// string -&gt; Async&lt;Match list&gt;
let findMatchingGift (wish: string) = async {
    // Call a custom machine learning model
    // The real implementation uses the Customer profile to adjust decisions by age, etc.
    // but we&#39;ll keep the model simple for now.
}</code></pre>
<p>The first line of all my function snippets shows the function type. In this case, it&#39;s a mapping 
from the text of the child&#39;s wish (<code>string</code>) to a list of matches (<code>Match list</code>).</p>
<p>The <strong>second action</strong> takes the <em>combined</em> list of all matches of all wishes and picks one. Its
real implementation is Santa&#39;s secret sauce, but my model just picks the one with the highest
confidence level:</p>
<pre><code class="language-fsharp">// Match list -&gt; Product
let pickGift (candidates: Match list) =
    candidates
    |&gt; List.sortByDescending (fun x -&gt; x.Confidence)
    |&gt; List.head
    |&gt; (fun x -&gt; x.Product)</code></pre>
<p>Given the picked <code>gift</code>, the reservation is merely <code>{ Kid = wishlist.Kid; Product = gift }</code>,
not worthy of a separate action.</p>
<p>The <strong>third action</strong> registers a reservation in the SAP system:</p>
<pre><code class="language-fsharp">// Reservation -&gt; Async&lt;unit&gt;
let reserve (reservation: Reservation) = async {
    // Call Santa&#39;s Archive of Products
}</code></pre>
<h3 id="workflow">Workflow</h3>
<p>The fulfillment service combines the three actions into one workflow:</p>
<pre><code class="language-fsharp">// WishList -&gt; Async&lt;Reservation&gt;
let workflow (wishlist: WishList) = async {

    // 1. Find matches for each wish 
    let! matches = 
        wishlist.Wishes
        |&gt; List.map findMatchingGift
        |&gt; Async.Parallel

    // 2. Pick one product from the combined list of matches
    let gift = pickGift (List.concat matches)

    // 3. Register and return the reservation
    let reservation = { Kid = wishlist.Kid; Product = gift }
    do! reserve reservation
    return reservation
}</code></pre>
<p>The workflow implementation is a nice and concise summary of the actual domain flow.</p>
<p>Note that the Matching service is called multiple times in parallel, and then
the results are easily combined by virtue of the <code>Async.Parallel</code> F# function.</p>
<p>So how do we translate the domain model to the actual implementation on top of
serverless Durable Functions?</p>
<h2 id="classic-durable-functions-api">Classic Durable Functions API</h2>
<p>C# was the first target language for Durable Functions; Javascript is now fully supported too.</p>
<p>F# wasn&#39;t initially declared as officially supported, but since F# runs on top of the same .NET runtime
as C#, it has always worked. I have a blog post about
<a href="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/">Azure Durable Functions in F#</a> and
have added <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/fsharp">F# samples</a>
to the official repository.</p>
<p>Here are two examples from that old F# code of mine (they have nothing to do with our 
gift fulfillment domain):</p>
<pre><code class="language-fsharp">// 1. Simple sequencing of activities
let Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = task {
  let! hello1 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Tokyo&quot;)
  let! hello2 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Seattle&quot;)
  let! hello3 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;London&quot;)
  return [hello1; hello2; hello3]
}     

// 2. Parallel calls snippet
let tasks = Array.map (fun f -&gt; context.CallActivityAsync&lt;int64&gt;(&quot;E2_CopyFileToBlob&quot;, f)) files
let! results = Task.WhenAll tasks</code></pre>
<p>This code works and does its job, but doesn&#39;t look like idiomatic F# code:</p>
<ul>
<li>No strong typing: Activity Functions are called by name and with types manually specified</li>
<li>Functions are not curried, so partial application is hard</li>
<li>The need to pass the <code>context</code> object around for any Durable operation</li>
</ul>
<p>Although not shown here, the other samples read input parameters, handle errors, and enforce
timeouts&mdash;all look too C#-y.</p>
<h2 id="better-durable-functions">Better Durable Functions</h2>
<p>Instead of following the sub-optimal route, we implemented the service with a more F#-idiomatic API.
I&#39;ll show the code first, and then I&#39;ll explain its foundation.</p>
<p>The implementation consists of three parts:</p>
<ul>
<li><strong>Activity</strong> Functions&mdash;one per action from the domain model</li>
<li><strong>Orchestrator</strong> Function defines the workflow</li>
<li><strong><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings">Azure Functions bindings</a></strong> 
to instruct how to run the application in the cloud</li>
</ul>
<h3 id="activity-functions">Activity Functions</h3>
<p>Each Activity Function defines one step of the workflow: Matching, Picking, and Reserving. We
simply reference the F# functions of those actions in one-line definitions:</p>
<pre><code class="language-fsharp">let findMatchingGiftActivity = Activity.defineAsync &quot;FindMatchingGift&quot; findMatchingGift
let pickGiftActivity = Activity.define &quot;PickGift&quot; pickGift
let reserveActivity = Activity.defineAsync &quot;Reserve&quot; reserve</code></pre>
<p>Each activity is defined by a name and a function.</p>
<h3 id="orchestrator">Orchestrator</h3>
<p>The Orchestrator calls Activity Functions to produce the desired outcome of the service. The code
uses a custom computation expression:</p>
<pre><code class="language-fsharp">let workflow wishlist = orchestrator {
    let! matches = 
        wishlist.Wishes
        |&gt; List.map (Activity.call findMatchingGiftActivity)
        |&gt; Activity.all

    let! gift = Activity.call pickGiftActivity (List.concat matches)

    let reservation = { Kid = wishlist.Kid; Product = gift }
    do! Activity.call reserveActivity reservation
    return reservation
}</code></pre>
<p>Notice how closely it matches the workflow definition from our domain model:</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/durable-orchestrator-vs-async.png" alt="Async Function vs. Durable Orchestrator"></p>
<center class="img-caption">Async function vs. Durable Orchestrator</center>

<p>The only differences are:</p>
<ul>
<li><code>orchestrator</code> computation expression is used instead of <code>async</code> because multi-threading is
not allowed in Orchestrators</li>
<li><code>Activity.call</code> replaces of direct invocations of functions</li>
<li><code>Activity.all</code> substitutes <code>Async.Parallel</code></li>
</ul>
<h3 id="hosting-layer">Hosting layer</h3>
<p>An Azure Function trigger needs to be defined to host any piece of code as a cloud Function. This can
be done manually in <code>function.json</code>, or via trigger generation from .NET attributes. In my case
I added the following four definitions:</p>
<pre><code class="language-fsharp">[&lt;FunctionName(&quot;FindMatchingGift&quot;)&gt;]
let FindMatchingGift([&lt;ActivityTrigger&gt;] wish) = 
    Activity.run findMatchingGiftActivity wish

[&lt;FunctionName(&quot;PickGift&quot;)&gt;]
let PickGift([&lt;ActivityTrigger&gt;] matches) = 
    Activity.run pickGiftActivity matches

[&lt;FunctionName(&quot;Reserve&quot;)&gt;]
let Reserve([&lt;ActivityTrigger&gt;] wish) = 
    Activity.run reserveActivity wish

[&lt;FunctionName(&quot;WishlistFulfillment&quot;)&gt;]
let Workflow ([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) =
    Orchestrator.run (workflow, context)</code></pre>
<p>The definitions are very mechanical and, again, strongly typed (apart from Functions&#39; names).</p>
<h3 id="ship-it-">Ship It!</h3>
<p>These are all the bits required to get our Durable Wish Fulfillment service up and running.
From this point, we can leverage all the existing tooling of Azure Functions:</p>
<ul>
<li>Visual Studio and Visual Studio Code for development and debugging</li>
<li><a href="https://github.com/Azure/azure-functions-core-tools">Azure Functions Core Tools</a> to run
the application locally and deploy it to Azure</li>
<li>The latest version of the Core Tools has dedicated commands to 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management">manage instances of Durable Functions</a></li>
</ul>
<p>There is a learning curve in the process of adopting the serverless architecture. However, a small
project like ours is a great way to do the learning. It sets Santa&#39;s IT department on the road
to success, and children will get better gifts more reliably!</p>
<h2 id="durablefunctions-fsharp">DurableFunctions.FSharp</h2>
<p>The above code was implemented with the library 
<a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">DurableFunctions.FSharp</a>. I created
this library as a thin F#-friendly wrapper around Durable Functions.</p>
<p>Frankly speaking, the whole purpose of this article is to introduce the library and make you curious
enough to give it a try. DurableFunctions.FSharp has several pieces in the toolbox:</p>
<ul>
<li><p><code>OrchestratorBuilder</code> and <code>orchestrator</code> computation expression which encapsulates proper 
usage of <code>Task</code>-based API of <code>DurableOrchestrationContext</code></p>
</li>
<li><p><code>Activity</code> generic type to define activities as first-class values</p>
</li>
<li><p><code>Activity</code> module with helper functions to call activities</p>
</li>
<li><p>Adapters for Azure Functions definition for <code>Async</code> and <code>Orchestrator</code></p>
</li>
<li><p>API of the original Durable Extensions is still available, so you can fall back to them if needed</p>
</li>
</ul>
<p>In my opinion, F# is a great language to develop serverless Functions. The simplicity of working with functions,
immutability by default, strong type system, focus on data pipelines are all useful in the world of
event-driven cloud applications.</p>
<p>Azure Durable Functions brings higher-level abstractions to compose workflows out of simple building
blocks. The goal of DurableFunctions.FSharp is to make such composition natural and enjoyable for F#
developers.</p>
<p><a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp#getting-started">Getting Started</a> 
is as easy as creating a new .NET Core project and referencing a NuGet package. </p>
<p>I&#39;d love to get as much feedback as possible! Leave comments below, create issues
on the <a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">GitHub repository</a>, or open a PR. 
This would be super awesome!</p>
<p>Happy coding, and Merry Christmas!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Many thanks to <a href="https://twitter.com/kashimizMSFT">Katy Shimizu</a>, <a href="https://twitter.com/DevonBurriss">Devon Burriss</a>,
<a href="https://twitter.com/iwasdavid">Dave Lowe</a>, <a href="https://twitter.com/cgillum">Chris Gillum</a>
for reviewing the draft of this article and their valuable contributions and suggestions.</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/f#/'>F#</a>, <a href='/tags/workflows/'>Workflows</a>, <a href='/tags/azure-durable-functions/'>Azure Durable Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Dec 7th, 2018</div>
    
    <h1><a href='/2018/12/making-sense-of-azure-durable-functions/'>Making Sense of Azure Durable Functions</a></h1>
    

    

    <div class="post-content">
        <p>Stateful Workflows on top of Stateless Serverless Cloud Functions&mdash;this is the essence
of the Azure Durable Functions library. That&#39;s a lot of fancy words in one sentence, and they
might be hard for the majority of readers to understand.</p>
<p>Please join me on the journey where I&#39;ll try to explain how those buzzwords fit
together. I will do this in 3 steps:</p>
<ul>
<li>Describe the context of modern cloud applications relying on serverless architecture;</li>
<li>Identify the limitations of basic approaches to composing applications out of the simple building blocks;</li>
<li>Explain the solutions that Durable Functions offer for those problems.</li>
</ul>
<h2 id="microservices">Microservices</h2>
<p>Traditionally, server-side applications were built in a style which is now referred to as
<strong>Monolith</strong>. If multiple people and teams were developing parts of the same application, they
mostly contributed to the same code base. If the code base were structured well, it would have
some distinct modules or components, and a single team would typically own each module:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/monolith.png" alt="Monolith"></p>
<center class="img-caption">Multiple components of a monolithic application</center>

<p>Usually, the modules would be packaged together at build time and then deployed as a single
unit, so a lot of communication between modules would stay inside the OS process.</p>
<p>Although the modules could stay loosely coupled over time, the coupling almost always occurred
on the level of the data store because all teams would use a single centralized database.</p>
<p>This model works great for small- to medium-size applications, but it turns out that teams
start getting in each other&#39;s way as the application grows since synchronization of contributions
takes more and more effort.</p>
<p>As a complex but viable alternative, the industry came up with a revised service-oriented
approach commonly called <strong>Microservices</strong>. The teams split the big application into &quot;vertical slices&quot; 
structured around the distinct business capabilities:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/microservices.png" alt="Microservices"></p>
<center class="img-caption">Multiple components of a microservice-based application</center>

<p>Each team then owns a whole vertical&mdash;from public communication contracts, or even UIs, down
to the data storage. Explicitly shared databases are strongly discouraged. Services talk to each
other via documented and versioned public contracts.</p>
<p>If the borders for the split were selected well&mdash;and that&#39;s the most tricky part&mdash;the
contracts stay stable over time, and thin enough to avoid too much chattiness. This gives
each team enough autonomy to innovate at their best pace and to make independent technical decisions.</p>
<p>One of the drawbacks of microservices is the change in deployment model. The services are now
deployed to separate servers connected via a network:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/distributed-system.png" alt="Distributed Systems"></p>
<center class="img-caption">Challenges of communication between distributed components</center>

<p>Networks are fundamentally unreliable: they work just fine most of the time, but when they 
fail, they fail in all kinds of unpredictable and least desirable manners. There are books
written on the topic of distributed systems architecture. TL;DR: it&#39;s hard.</p>
<p>A lot of the new adopters of microservices tend to ignore such complications. REST over HTTP(S) is the
dominant style of connecting microservices. Like any other synchronous communication
protocol, it makes the system brittle.</p>
<p>Consider what happens when one service becomes temporary unhealthy: maybe its database goes offline, or it&#39;s struggling to 
keep up with the request load, or a new version of the service is being deployed. All the requests to the problematic service start
failing&mdash;or worse&mdash;become very slow. The dependent service waits for the response, and
thus blocks all incoming requests of its own. The error propagates upstream very quickly causing cascading
failures all over the place:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/cascading-failures.png" alt="Cascading Failures"></p>
<center class="img-caption">Error in one component causes cascading failures</center>

<p>The application is down. Everybody screams and starts the blame war.</p>
<h2 id="event-driven-applications">Event-Driven Applications</h2>
<p>While cascading failures of HTTP communication can be mitigated with patterns like a circuit breaker
and graceful degradation, a better solution is to switch to the asynchronous style of communication
as the default. Some kind of persistent queueing service is used as an intermediary.</p>
<p>The style of application architecture which is based on sending events between services
is known as <strong>Event-Driven</strong>. When a service does something useful, it publishes an event&mdash;a record 
about the fact which happened to its business domain. Another service listens to the published events and 
executes its own duty in response to those facts:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/event-driven.png" alt="Event-Driven Application"></p>
<center class="img-caption">Communication in event-driven applications</center>

<p>The service that produces events might not know about the consumers. New event subscribers can
be introduced over time. This works better in theory than in practice, but the services tend to
get coupled less.</p>
<p>More importantly, if one service is down, other services don&#39;t catch fire immediately. The
upstream services keep publishing the events, which build up in the queue but can be stored safely
for hours or days. The downstream services might not be doing anything useful for this particular
flow, but it can stay healthy otherwise.</p>
<p>However, another potential issue comes hand-in-hand with loose coupling: low cohesion.
As Martin Fowler notices in his essay
<a href="https://martinfowler.com/articles/201701-event-driven.html">What do you mean by &quot;Event-Driven&quot;</a>:</p>
<blockquote>
<p>It&#39;s very easy to make nicely decoupled systems with event notification, without realizing 
that you&#39;re losing sight of the larger-scale flow.</p>
</blockquote>
<p>Given many components that publish and subscribe to a large number of event types, it&#39;s easy to stop
seeing the forest for the trees. Combinations of events usually constitute gradual workflows executed 
in time. A workflow is more than the sum of its parts, and understanding of the high-level flow is 
paramount to controlling the system behavior.</p>
<p>Hold this thought for a minute; we&#39;ll get back to it later. Now it&#39;s time to talk <em>cloud</em>.</p>
<h2 id="cloud">Cloud</h2>
<p>The birth of public cloud changed the way we architect applications. It made many things
much more straightforward: provisioning of new resources in minutes instead of months, scaling elastically
based on demand, and resiliency and disaster recovery at the global scale.</p>
<p>It made other things more complicated. Here is the picture of the global Azure network:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/azure-network.png" alt="Azure Network"></p>
<center class="img-caption">Azure locations with network connections</center>

<p>There are good reasons to deploy applications to more than one geographical location:
among others, to reduce network latency by staying close to the customer, and to achieve resilience through 
geographical redundancy. Public Cloud is the ultimate distributed system. As you remember,
distributed systems are hard.</p>
<p>There&#39;s more to that. Each cloud provider has dozens and dozens of managed services, which is
the curse and the blessing. Specialized services are great to provide off-the-shelf solutions 
to common complex problems. On the flip side, each service has distinct properties regarding 
consistency, resiliency and fault tolerance.</p>
<p>In my opinion, at this point developers have to embrace the public cloud and apply the distributed 
system design on top of it. If you agree, there is an excellent way to approach it.</p>
<h2 id="serverless">Serverless</h2>
<p>The slightly provocative term <strong>serverless</strong> is used to describe cloud services that do not
require provisioning of VMs, instances, workers, or any other fixed capacity to run
custom applications on top of them. Resources are allocated dynamically and transparently,
and the cost is based on their actual consumption, rather than on pre-purchased capacity.</p>
<p>Serverless is more about operational and economical properties of the system than about the
technology per se. Servers do exist, but they are someone else&#39;s concern. You don&#39;t manage
the uptime of serverless applications: the cloud provider does.</p>
<p>On top of that, you pay for what you use, similar to the consumption of other commodity resources
like electricity. Instead of buying a generator to power up your house, you just purchase energy
from the power company. You lose some control (e.g., no way to select the voltage), but this is fine
in most cases. The great benefit is no need to buy and maintain the hardware.</p>
<p>Serverless compute does the same: it supplies standard services on a pay-per-use basis.</p>
<p>If we talk more specifically about Function-as-a-Service offerings like Azure Functions, they
provide a standard model to run small pieces of code in the cloud.
You zip up the code or binaries and send it to Azure; Microsoft takes care of all the
hardware and software required to run it. The infrastructure automatically scales up or down based
on demand, and you pay per request, CPU time and memory that the application consumed. 
No usage&mdash;no bill.</p>
<p>However, there&#39;s always a &quot;but&quot;. FaaS services come with an opinionated development model that
applications have to follow:</p>
<ul>
<li><p><strong>Event-Driven</strong>: for each serverless function you have to define a specific trigger&mdash;the 
event type which causes it to run, be it an HTTP endpoint or a queue message;</p>
</li>
<li><p><strong>Short-Lived</strong>: functions can only run up to several minutes, and preferably for a few seconds 
or less;</p>
</li>
<li><p><strong>Stateless</strong>: as you don&#39;t control where and when function instances are provisioned or
deprovisioned, there is no way to store data within the process between requests reliably;
external storage has to be utilized.</p>
</li>
</ul>
<p>Frankly speaking, the majority of existing applications don&#39;t really fit into this model.
If you are lucky to work on a new application (or a new module of it), you are in better shape.</p>
<p>A lot of the serverless applications may be designed to look somewhat similar to this example
from <a href="https://www.serverless360.com/blog/building-reactive-solution-with-azure-event-grid">the Serverless360 blog</a>:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/serviceful-example.png" alt="Serviceful Serverless Application"></p>
<center class="img-caption">Sample application utilizing "serviceful" serverless architecture</center>

<p>There are 9 managed Azure services working together in this app. Most of them have a unique purpose, but
the services are all glued together with Azure Functions. An image is uploaded to Blob Storage, an
Azure Function calls Vision API to recognize the license plate and send the result to Event Grid, another 
Azure Function puts that event to Cosmos DB, and so on.</p>
<p>This style of cloud applications is sometimes referred to as <strong>Serviceful</strong> to emphasize the heavy usage
of managed services &quot;glued&quot; together by serverless functions.</p>
<p>Creating a comparable application without any managed services would be a much harder task,
even more so, if the application has to run at scale. Moreover, there&#39;s no way to keep the pay-as-you-go 
pricing model in the self-service world.</p>
<p>The application pictured above is still pretty straightforward. The processes
in enterprise applications are often much more sophisticated.</p>
<p>Remember the quote from Martin Fowler about losing sight of the large-scale flow. That was
true for microservices, but it&#39;s even more true for the &quot;nanoservices&quot; of cloud functions.</p>
<p>I want to dive deeper and give you several examples of related problems.</p>
<h2 id="challenges-of-serverless-composition">Challenges of Serverless Composition</h2>
<p>For the rest of the article, I&#39;ll define an imaginary business application for booking trips to software
conferences. In order to go to a conference, I need to buy tickets to the conference itself,
purchase the flights, and book a room at a hotel.</p>
<p>In this scenario, it makes sense to create three Azure Functions, each one responsible for one step
of the booking process. As we prefer message passing, each Function emits an event which
the next function can listen for:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/conference-booking.png" alt="Conference Booking Application"></p>
<center class="img-caption">Conference booking application</center>

<p>This approach works, however, problems do exist.</p>
<h3 id="flexible-sequencing">Flexible Sequencing</h3>
<p>As we need to execute the whole booking process in sequence, the Azure Functions are wired
one after another by configuring the output of one function to match with the event source of
the downstream function.</p>
<p>In the picture above, the functions&#39; sequence is hard-defined. If we were to swap the order of booking 
the flights and reserving the hotel, that would require a code change&mdash;at least of the 
input/output wiring definitions, but probably also the functions&#39; parameter types.</p>
<p>In this case, are the functions <em>really</em> decoupled?</p>
<h3 id="error-handling">Error Handling</h3>
<p>What happens if the Book Flight function becomes unhealthy, perhaps due to the
outage of the third-party flight-booking service? Well, that&#39;s why we use asynchronous messaging:
after the function execution fails, the message returns to the queue and is picked
up again by another execution.</p>
<p>However, such retries happen almost immediately for most event sources. This might not
be what we want: an exponential back-off policy could be a smarter idea. At this point,
the retry logic becomes <strong>stateful</strong>: the next attempt should &quot;know&quot; the history of previous attempts
to make a decision about retry timing.</p>
<p>There are more advanced error-handling patterns too. If executions failures are not
intermittent, we may decide to cancel the whole process and run compensating actions
against the already completed steps.</p>
<p>An example of this is a fallback action: if the flight is not possible (e.g., no routes for this
origin-destination combination), the flow could choose to book a train instead:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/fallback-on-error.png" alt="Fallback On Error"></p>
<center class="img-caption">Fallback after 3 consecutive failures</center>

<p>This scenario is not trivial to implement with stateless functions. We could wait until a
message goes to the dead-letter queue and then route it from there, but this is brittle and
not expressive enough.</p>
<h3 id="parallel-actions">Parallel Actions</h3>
<p>Sometimes the business process doesn&#39;t have to be sequential. In our reservation scenario,
there might be no difference whether we book a flight before a hotel or vice versa. It could
be desirable to run those actions in parallel.</p>
<p>Parallel execution of actions is easy with the pub-sub capabilities of an event bus: both functions
should subscribe to the same event and act on it independently.</p>
<p>The problem comes when we need to reconcile the outcomes of parallel actions, e.g., calculate the
final price for expense reporting purposes:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/fanout-fanin.png" alt="Fan-out / Fan-in"></p>
<center class="img-caption">Fan-out / fan-in pattern</center>

<p>There is no way to implement the Report Expenses block as a single Azure Function: functions
can&#39;t be triggered by two events, let alone correlate two <em>related</em> events.</p>
<p>The solution would probably include two functions, one per event, and the shared storage
between them to pass information about the first completed booking to the one who
completes last. All this wiring has to be implemented in custom code. The complexity grows
if more than two functions need to run in parallel.</p>
<p>Also, don&#39;t forget the edge cases. What if one of the function fails? How do you make sure there is
no race condition when writing and reading to/from the shared storage?</p>
<h3 id="missing-orchestrator">Missing Orchestrator</h3>
<p>All these examples give us a hint that we need an additional tool to organize low-level
single-purpose independent functions into high-level workflows.</p>
<p>Such a tool can be called an <strong>Orchestrator</strong> because its sole mission is to delegate work
to stateless actions while maintaining the big picture and history of the flow.</p>
<p>Azure Durable Functions aims to provide such a tool.</p>
<h2 id="introducing-azure-durable-functions">Introducing Azure Durable Functions</h2>
<h3 id="azure-functions">Azure Functions</h3>
<p>Azure Functions is the serverless compute service from Microsoft. Functions are event-driven:
each function defines a <strong>trigger</strong>&mdash;the exact definition of the event source, for instance,
the name of a storage queue.</p>
<p>Azure Functions can be programmed in <a href="https://docs.microsoft.com/en-us/azure/azure-functions/supported-languages">several languages</a>. 
A basic Function with a
<a href="https://docs.microsoft.com/azure/azure-functions/functions-bindings-storage-queue">Storage Queue trigger</a>
implemented in C# would look like this: </p>
<pre><code class="language-csharp">[FunctionName(&quot;MyFirstFunction&quot;)]
public static void QueueTrigger(
    [QueueTrigger(&quot;myqueue-items&quot;)] string myQueueItem, 
    ILogger log)
{
    log.LogInformation($&quot;C# function processed: {myQueueItem}&quot;);
}</code></pre>
<p>The <code>FunctionName</code> attribute exposes the C# static method as an Azure Function named <code>MyFirstFunction</code>. 
The <code>QueueTrigger</code> attribute defines the name of the storage queue to listen to. The function body
logs the information about the incoming message.</p>
<h3 id="durable-functions">Durable Functions</h3>
<p><a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Durable Functions</a>
is a library that brings workflow orchestration abstractions to
Azure Functions. It introduces a number of idioms and tools to define stateful,
potentially long-running operations, and manages a lot of mechanics of reliable communication 
and state management behind the scenes.</p>
<p>The library records the history of all actions in Azure Storage services, enabling durability 
and resilience to failures.</p>
<p>Durable Functions is 
<a href="https://github.com/Azure/azure-functions-durable-extension">open source</a>, 
Microsoft accepts external contributions, and the community is quite active.</p>
<p>Currently, you can write Durable Functions in 3 programming languages: C#, F#, and
Javascript (Node.js). All my examples are going to be in C#. For Javascript,
check <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/quickstart-js-vscode">this quickstart</a>
and <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/javascript">these samples</a>.
For F# see <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/fsharp">the samples</a>,
<a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">the F#-specific library</a> and my article 
<a href="https://mikhail.io/2018/12/fairy-tale-of-fsharp-and-durable-functions/">A Fairy Tale of F# and Durable Functions</a>. </p>
<p>Workflow building functionality is achieved by the introduction of two additional types
of triggers: Activity Functions and Orchestrator Functions.</p>
<h3 id="activity-functions">Activity Functions</h3>
<p>Activity Functions are simple stateless single-purpose building blocks
that do just one task and have no awareness of the bigger workflow. 
A new trigger type,
<code>ActivityTrigger</code>, was introduced to expose functions as workflow steps, as
I explain below.</p>
<p>Here is a simple Activity Function implemented in C#:</p>
<pre><code class="language-csharp">[FunctionName(&quot;BookConference&quot;)]
public static ConfTicket BookConference([ActivityTrigger] string conference)
{
    var ticket = BookingService.Book(conference);
    return new ConfTicket { Code = ticket };
}</code></pre>
<p>It has a common <code>FunctionName</code> attribute to expose the C# static method as an
Azure Function named <code>BookConference</code>. The name is important because it is used to
invoke the activity from orchestrators.</p>
<p>The <code>ActivityTrigger</code> attribute defines the trigger type and points to the input
parameter <code>conference</code> which the activity expects to get for each invocation.</p>
<p>The function can return a result of any serializable type; my sample function
returns a simple property bag called <code>ConfTicket</code>.</p>
<p>Activity Functions can do pretty much anything: call other services, load and
save data from/to databases, and use any .NET libraries.</p>
<h3 id="orchestrator-functions">Orchestrator Functions</h3>
<p>The Orchestrator Function is a unique concept introduced by Durable Functions. Its sole
purpose is to manage the flow of execution and data among several activity functions.</p>
<p>Its most basic form chains multiple independent activities into a single
sequential workflow.</p>
<p>Let&#39;s start with an example which books a conference ticket, a flight itinerary, and
a hotel room one-by-one:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/sequential-workflow.png" alt="Sequential Workflow"></p>
<center class="img-caption">3 steps of a workflow executed in sequence</center>

<p>The implementation of this workflow is defined by another C# Azure Function, this time with
<code>OrchestrationTrigger</code>:</p>
<pre><code class="language-csharp">[FunctionName(&quot;SequentialWorkflow&quot;)]
public static async Task Sequential([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var conference = await context.CallActivityAsync&lt;ConfTicket&gt;(&quot;BookConference&quot;, &quot;ServerlessDays&quot;);
    var flight = await context.CallActivityAsync&lt;FlightTickets&gt;(&quot;BookFlight&quot;, conference.Dates);
    await context.CallActivityAsync(&quot;BookHotel&quot;, flight.Dates);
}</code></pre>
<p>Again, attributes are used to describe the function for the Azure runtime.</p>
<p>The only input parameter has type <code>DurableOrchestrationContext</code>. This context is the tool that
enables the orchestration operations.</p>
<p>In particular, the <code>CallActivityAsync</code> method is used three times to invoke three activities one after the other.
The method body looks very typical for any C# code working with a <code>Task</code>-based API. However,
the behavior is entirely different. Let&#39;s have a look at the implementation details.</p>
<h2 id="behind-the-scenes">Behind the Scenes</h2>
<p>Let&#39;s walk through the lifecycle of one execution of the sequential workflow above.</p>
<p>When the orchestrator starts running, the first <code>CallActivityAsync</code> invocation is made to book the
conference ticket. What actually happens here is that a queue message is sent from the orchestrator
to the activity function. </p>
<p>The corresponding activity function gets triggered by the queue message. It does its job (books the 
ticket) and returns the result. The activity function serializes the result and sends it as a queue 
message back to the orchestrator:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/durable-messaging.png" alt="Durable Functions: Message Passing"></p>
<center class="img-caption">Messaging between the orchestrator and the activity</center>

<p>When the message arrives, the orchestrator gets triggered again and can proceed to the second
activity. The cycle repeats&mdash;a message gets sent to Book Flight activity, it gets triggered, does its
job, and sends a message back to the orchestrator.
The same message flow happens for the third call.</p>
<h3 id="stop-resume-behavior">Stop-resume behavior</h3>
<p>As discussed earlier, message passing is intended to decouple the sender and receiver in time.
For every message in the scenario above, no immediate response is expected. </p>
<p>On the C# level, when the <code>await</code> operator is executed, the code doesn&#39;t block the execution of the whole
orchestrator. Instead, it just quits: the orchestrator stops being active and its current step completes.</p>
<p>Whenever a return message arrives from an activity, the orchestrator code restarts. It always starts
with the first line. Yes, this means that the same line is executed multiple times: up to the number of
messages to the orchestrator.</p>
<p>However, the orchestrator stores the history of its past executions in Azure Storage, so the effect of the second pass
of the first line is different: instead of sending a message to the activity it already knows
the result of that activity, so <code>await</code> returns this result back and assigns it to the <code>conference</code> variable.</p>
<p>Because of these &quot;replays&quot;, the orchestrator&#39;s implementation has to be deterministic: don&#39;t use
<code>DateTime.Now</code>, random numbers or multi-thread operations; more details
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints">here</a>.</p>
<h3 id="event-sourcing">Event Sourcing</h3>
<p>Azure Functions are stateless, while workflows require a state to keep track of their progress. Every time a new
action towards the workflow&#39;s execution happens, the framework automatically records an event in table storage.</p>
<p>Whenever an orchestrator restarts the execution because a new message arrives from its activity,
it loads the complete history of this particular execution from storage. Durable Context uses
this history to make decisions whether to call the activity or return the previously stored result.</p>
<p>The pattern of storing the complete history of state changes as an append-only event store is
known as Event Sourcing. Event store provides several benefits:</p>
<ul>
<li><strong>Durability</strong>&mdash;if a host running an orchestration fails, the history is retained in
persistent storage and is loaded by the new host where the orchestration restarts;</li>
<li><strong>Scalability</strong>&mdash;append-only writes are fast and easy to spread over multiple storage servers;</li>
<li><strong>Observability</strong>&mdash;no history is ever lost, so it&#39;s straightforward to inspect and
analyze even after the workflow is complete.</li>
</ul>
<p>Here is an illustration of the notable events that get recorded during our sequential workflow:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/event-sourcing.png" alt="Durable Functions: Event Sourcing"></p>
<center class="img-caption">Log of events in the course of orchestrator progression</center>

<h3 id="billing">Billing</h3>
<p>Azure Functions on the serverless consumption-based plan are billed per execution + per duration of
execution.</p>
<p>The stop-replay behavior of durable orchestrators causes the single workflow &quot;instance&quot; to execute
the same orchestrator function multiple times. This also means paying for several short
executions.</p>
<p>However, the total bill usually ends up being much lower compared to the potential cost of blocking
synchronous calls to activities. The price of 5 executions of 100 ms each is significantly lower
than the cost of 1 execution of 30 seconds.</p>
<p>By the way, the first million executions per month are 
<a href="https://azure.microsoft.com/en-us/pricing/details/functions/">at no charge</a>,
so many scenarios incur no cost at all from Azure Functions service.</p>
<p>Another cost component to keep in mind is Azure Storage. Queues and Tables that are used behind the
scenes are charged to the end customer. In my experience, this charge remains close to zero for
low- to medium-load applications.</p>
<p>Beware of unintentional eternal loops or indefinite recursive fan-outs in your orchestrators. Those
can get expensive if you leave them out of control.</p>
<h2 id="error-handling-and-retries">Error-handling and retries</h2>
<p>What happens when an error occurs somewhere in the middle of the workflow? For instance, a third-party
flight booking service might not be able to process the request:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/error-handling.png" alt="Error Handling"></p>
<center class="img-caption">One activity is unhealthy</center>

<p>This situation is expected by Durable Functions. Instead of silently failing, the activity function
sends a message containing the information about the error back to the orchestrator.</p>
<p>The orchestrator deserializes the error details and, at the time of replay, throws a .NET exception
from the corresponding call. The developer is free to put a <code>try .. catch</code> block around the call
and handle the exception:</p>
<pre><code class="language-csharp">[FunctionName(&quot;SequentialWorkflow&quot;)]
public static async Task Sequential([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var conf = await context.CallActivityAsync&lt;ConfTicket&gt;(&quot;BookConference&quot;, &quot;ServerlessDays&quot;);
    try
    {
        var itinerary = MakeItinerary(/* ... */);
        await context.CallActivityAsync(&quot;BookFlight&quot;, itinerary);
    }
    catch (FunctionFailedException)
    {
        var alternativeItinerary = MakeAnotherItinerary(/* ... */);
        await context.CallActivityAsync(&quot;BookFlight&quot;, alternativeItinerary);
    }
    await context.CallActivityAsync(&quot;BookHotel&quot;, flight.Dates);
}</code></pre>
<p>The code above falls back to a &quot;backup plan&quot; of booking another itinerary. Another typical pattern
would be to run a compensating activity to cancel the effects of any previous actions (un-book the
conference in our case) and leave the system in a clean state.</p>
<p>Quite often, the error might be transient, so it might make sense to retry the failed operation
after a pause. It&#39;s a such a common scenario that Durable Functions provides a dedicated API:</p>
<pre><code class="language-csharp">var options = new RetryOptions(
    firstRetryInterval: TimeSpan.FromMinutes(1),                    
    maxNumberOfAttempts: 5);
options.BackoffCoefficient = 2.0;

await context.CallActivityWithRetryAsync(&quot;BookFlight&quot;, options, itinerary);</code></pre>
<p>The above code instructs the library to</p>
<ul>
<li>Retry up to 5 times</li>
<li>Wait for 1 minute before the first retry</li>
<li>Increase delays before every subsequent retry by the factor of 2 (1 min, 2 min, 4 min, etc.)</li>
</ul>
<p>The significant point is that, once again, the orchestrator does not block while
awaiting retries. After a failed call, a message is scheduled for the moment in the future
to re-run the orchestrator and retry the call.</p>
<h2 id="sub-orchestrators">Sub-orchestrators</h2>
<p>Business processes may consist of numerous steps. To keep the code of orchestrators manageable,
Durable Functions allows nested orchestrators. A &quot;parent&quot; orchestrator can call out to child
orchestrators via the <code>context.CallSubOrchestratorAsync</code> method:</p>
<pre><code class="language-csharp">[FunctionName(&quot;CombinedOrchestrator&quot;)]
public static async Task CombinedOrchestrator([OrchestrationTrigger] DurableOrchestrationContext context)
{
    await context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysAmsterdam);
    await context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysHamburg);
}</code></pre>
<p>The code above books two conferences, one after the other.</p>
<h2 id="fan-out-fan-in">Fan-out / Fan-in</h2>
<p>What if we want to run multiple activities in parallel?</p>
<p>For instance, in the example above, we could wish to book two conferences, but the
booking order might not matter. Still, when both bookings are completed, we want to combine the results
to produce an expense report for the finance department:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/parallel-calls.png" alt="Parallel Calls"></p>
<center class="img-caption">Parallel calls followed by a final step</center>

<p>In this scenario, the <code>BookTrip</code> orchestrator accepts an input parameter with the name of the 
conference and returns the expense information. <code>ReportExpenses</code> needs to receive both
expenses combined.</p>
<p>This goal can be easily achieved by scheduling two tasks (i.e., sending two messages) without
awaiting them separately. We use the familiar <code>Task.WhenAll</code> method to await both and combine
the results:</p>
<pre><code class="language-csharp">[FunctionName(&quot;ParallelWorkflow&quot;)]
public static async Task Parallel([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var amsterdam = context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysAmsterdam);
    var hamburg   = context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysHamburg);

    var expenses = await Task.WhenAll(amsterdam, hamburg);

    await context.CallActivityAsync(&quot;ReportExpenses&quot;, expenses);
}</code></pre>
<p>Remember that awaiting the <code>WhenAll</code> method doesn&#39;t synchronously block the orchestrator. It quits
the first time and then restarts two times on reply messages received from activities.
The first restart quits again, and only the second restart makes it past the <code>await</code>.</p>
<p><code>Task.WhenAll</code> returns an array of results (one result per each input task), which is then
passed to the reporting activity.</p>
<p>Another example of parallelization could be a workflow sending e-mails to hundreds of
recipients. Such fan-out wouldn&#39;t be hard with normal queue-triggered functions: simply send
hundreds of messages. However, combining the results, if required for the next step
of the workflow, is quite challenging.</p>
<p>It&#39;s straightforward with a durable orchestrator:</p>
<pre><code class="language-csharp">var emailSendingTasks =
    recepients
    .Select(to =&gt; context.CallActivityAsync&lt;bool&gt;(&quot;SendEmail&quot;, to))
    .ToArray();

var results = await Task.WhenAll(emailSendingTasks);

if (results.All(r =&gt; r)) { /* ... */ }</code></pre>
<p>Making hundreds of roundtrips to activities and back could cause numerous replays
of the orchestrator. As an optimization, if multiple activity functions complete around the same 
time, the orchestrator may internally process several messages as a batch and restart 
the orchestrator function only once per batch.</p>
<h2 id="other-concepts">Other Concepts</h2>
<p>There are many more patterns enabled by Durable Functions. Here is a quick list to give you some perspective:</p>
<ul>
<li>Waiting for the <em>first</em> completed task in a collection (rather than <em>all</em> of them) using the <code>Task.WhenAny</code>
method. Useful for scenarios like timeouts or competing actions.</li>
<li>Pausing the workflow for a given period or until a deadline.</li>
<li>Waiting for external events, e.g., bringing human interaction into the workflow.</li>
<li>Running recurring workflows, when the flow repeats until a certain condition is met.</li>
</ul>
<p>Further explanation and code samples are in 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">the docs</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I firmly believe that serverless applications utilizing a broad range of managed cloud services
are highly beneficial to many companies, due to both rapid development process and
the properly aligned billing model.</p>
<p>Serverless tech is still young; more high-level architectural patterns need to emerge
to enable expressive and composable implementations of large business systems.</p>
<p>Azure Durable Functions suggests some of the possible answers. It combines the clarity and readability
of sequential RPC-style code with the power and resilience of event-driven architecture.</p>
<p><a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">The documentation</a>
for Durable Functions is excellent, with plenty of examples and how-to guides.
Learn it, try it for your real-life scenarios, and let me know your opinion&mdash;I&#39;m
excited about the serverless future!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Many thanks to <a href="https://twitter.com/kashimizMSFT">Katy Shimizu</a>, <a href="https://twitter.com/cgillum">Chris Gillum</a>,
<a href="https://twitter.com/efleming18">Eric Fleming</a>, <a href="https://twitter.com/KevinJonesD">KJ Jones</a>,
<a href="https://twitter.com/William_DotNet">William Liebenberg</a>, <a href="https://twitter.com/ATosato86">Andrea Tosato</a>
for reviewing the draft of this article and their valuable contributions and suggestions. The community 
around Azure Functions and Durable Functions is superb!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/workflows/'>Workflows</a>, <a href='/tags/azure-durable-functions/'>Azure Durable Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Nov 19th, 2018</div>
    
    <h1><a href='/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/'>From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a></h1>
    

    
        <div class="remark">Originally published at <a href='https://blog.binaris.com/from-0-to-1000-instances/'>Binaris Blog</a></div>
    

    <div class="post-content">
        <p>Whenever I see a &quot;Getting Started with Function-as-a-Service&quot; tutorial, it usually shows off 
a synchronous HTTP-triggered scenario. In my projects, though, I use a lot of asynchronous 
functions triggered by a queue or an event stream.</p>
<p>Quite often, the number of messages passing through a queue isn&#39;t uniform over time. I might 
drop batches of work now and then. My app may get piles of queue items arriving from upstream 
systems that were down or under maintenance for an extended period. The system might see some 
rush-hour peaks every day or only a few busy days per month.</p>
<p>This is where serverless tech shines: You pay per execution, and then the promise is that the 
provider takes care of scaling up or down for you. Today, I want to put this scalability under 
test.</p>
<p>The goal of this article is to explore queue-triggered serverless functions and hopefully distill 
some practical advice regarding asynchronous functions for real projects. I will be evaluating 
the problem:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different types of workloads</li>
<li>For different performance tiers</li>
</ul>
<p>Let&#39;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some crucial factors and parameters 
that influence the outcome. My interpretation might be wrong. The results might change over time. 
If you happen to know a way to improve my tests, please let me know, and I will re-run them and 
re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>In this article I analyze the execution results of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via SQS queues</li>
<li>Azure Function triggered via Storage queues</li>
<li>Google Cloud Function triggered via Cloud Pub/Sub</li>
</ul>
<p>All functions are implemented in Javascript and are running on GA runtime.</p>
<p>At the beginning of each test, I threw 100,000 messages into a queue that was previously idle. 
Enqueuing never took longer than one minute (I sent the messages from multiple clients in 
parallel).</p>
<p>I disabled any batch processing, so each message was consumed by a separate function invocation.</p>
<p>I then analyzed the logs (AWS CloudWatch, Azure Application Insights, and GCP Stackdriver 
Logging) to generate charts of execution distribution over time.</p>
<h2 id="how-scaling-actually-works">How Scaling Actually Works</h2>
<p>To understand the experiment better, let&#39;s look at a very simplistic but still useful model of 
how cloud providers scale serverless applications.</p>
<p>All providers handle the increased load by 
<a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">scaling out</a>, i.e., 
by creating multiple instances of the same application that execute the chunks of work in 
parallel.</p>
<p>In theory, a cloud provider could spin up an instance for each message in the queue as soon as 
the messages arrive. The backlog processing time would then stay very close to zero.</p>
<p>In practice, allocating instances is not cheap. The Cloud provider has to boot up the function 
runtime, hit a <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">cold start</a>, and waste 
expensive resources on a job that potentially will take just a few milliseconds.</p>
<p>So the providers are trying to find a sweet spot between handling the work as soon as possible 
and using resources efficiently. The outcomes differ, which is the point of my article.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda defines scale out with a notion of Concurrent Executions. Each instance of your AWS 
Lambda is handling a single execution at any given time. In our case, it&#39;s processing a single 
SQS message.</p>
<p>It&#39;s helpful to think of a function instance as a container working on a single task. If execution 
pauses or waits for an external I/O operation, the instance is on hold.</p>
<p>The model of concurrent executions is universal to all trigger types supported by Lambdas. An 
instance doesn&#39;t work with event sources directly; it just receives an event to work on.</p>
<p>There is a central element in the system, let&#39;s call it &quot;Orchestrator&quot;. The Orchestrator is the 
component talking to an SQS queue and getting the messages from it. It&#39;s then the job of the 
Orchestrator and related infrastructure to provision the required number of instances for working 
on concurrent executions:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-queue-scaling.png" alt="Model of AWS Lambda Scale-Out"></p>
<center class="img-caption">Model of AWS Lambda Scale-Out</center>

<p>As to scaling behavior, here is what the official 
<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/scaling.html">AWS docs</a> say:</p>
<blockquote>
<p>AWS Lambda automatically scales up ... until the number of concurrent function executions 
reaches 1000 ... Amazon Simple Queue Service supports an initial burst of 5 concurrent function 
invocations and increases concurrency by 60 concurrent invocations per minute.</p>
</blockquote>
<h3 id="gcp">GCP</h3>
<p>The model of Google Cloud Functions is very similar to what AWS does. It runs a single 
simultaneous execution per instance and routes the messages centrally.</p>
<p>I wasn&#39;t able to find any scaling specifics except the definition of 
<a href="https://cloud.google.com/functions/quotas">Function Quotas</a>.</p>
<h3 id="azure">Azure</h3>
<p>Experiments with Azure Functions were run on 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
&mdash;the dynamically scaled and billed-per-execution runtime. The concurrency model of Azure Functions 
is different from the counterparts of AWS/GCP.</p>
<p>Function App instance is closer to a VM than a single-task container. It runs multiple concurrent 
executions in parallel. Equally importantly, it pulls messages from the queue on its own instead of 
getting them pushed from a central Orchestrator.</p>
<p>There is still a central coordinator called Scale Controller, but its role is a bit more subtle. It 
connects to the same data source (the queue) and needs to determine how many instances to provision 
based on the metrics from that queue:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-scaling.png" alt="Model of Azure Function Scale-Out"></p>
<center class="img-caption">Model of Azure Function Scale-Out</center>

<p>This model has pros and cons. If one execution is idle, waiting for some I/O operation such as an 
HTTP request to finish, the instance might become busy processing other messages, thus being more 
efficient. Running multiple executions is useful in terms of shared resource utilization, e.g., 
keeping database connection pools and reusing HTTP connections.</p>
<p>On the flip side, the Scale Controller now needs to be smarter: to know not only the queue backlog 
but also how instances are doing and at what pace they are processing the messages. It&#39;s probably 
achievable based on queue telemetry though.</p>
<p>Let&#39;s start applying this knowledge in practical experiments.</p>
<h2 id="pause-the-world-workload">Pause-the-World Workload</h2>
<p>My first serverless function is aimed to simulate I/O-bound workloads without using external 
dependencies to keep the experiment clean. Therefore, the implementation is extremely 
straightforward: pause for 500 ms and return.</p>
<p>It could be loading data from a scalable third-party API. It could be running a database query. 
Instead, it just runs <code>setTimeout</code>.</p>
<p>I sent 100k messages to queues of all three cloud providers and observed the result.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda allows multiple instance sizes to be provisioned. Since the workload is neither CPU- 
nor memory-intensive, I was using the smallest memory allocation of 128 MB.</p>
<p>Here comes the first chart of many, so let&#39;s learn to read it. The horizontal axis shows time in 
minutes since all the messages were sent to the queue.</p>
<p>The line going from top-left to bottom-right shows the decreasing queue backlog. Accordingly, the 
left vertical axis denotes the number of items still-to-be-handled.</p>
<p>The bars show the number of concurrent executions crunching the messages at a given time. Every 
execution logs the instance ID so that I could derive the instance count from the logs. The right 
vertical axis shows the instance number.</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-iobound-scaling.png" alt="AWS Lambda processing 100k SQS messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">AWS Lambda processing 100k SQS messages with "Pause" handler</center>

<p>It took AWS Lambda 5.5 minutes to process the whole batch of 100k messages. For comparison, the 
same batch processed sequentially would take about 14 hours.</p>
<p>Notice how linear the growth of instance count is. If I apply the official scaling formula:</p>
<pre><code>Instance Count = 5 + Minutes * 60 = 5 + 5.5 * 60  = 335</code></pre><p>We get a very close result! Promises kept.</p>
<h3 id="gcp">GCP</h3>
<p>Same function, same chart, same instance size of 128 MB of RAM&mdash;but this time for Google Cloud Functions:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-iobound-scaling.png" alt="Google Cloud Function processing 100k Pub/Sub messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">Google Cloud Function processing 100k Pub/Sub messages with "Pause" handler</center>

<p>Coincidentally, the total amount of instances, in the end, was very close to AWS. The scaling
pattern looks entirely different though: Within the very first minute, there was a burst of 
scaling close to 300 instances, and then the growth got very modest.</p>
<p>Thanks to this initial jump, GCP managed to finish processing almost one minute earlier than AWS.</p>
<h3 id="azure">Azure</h3>
<p>Azure Function doesn&#39;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>The shape of the chart for Azure Functions is very similar, but the instance number growth is 
significantly different:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-iobound-scaling.png" alt="Azure Function processing 100k queue messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">Azure Function processing 100k queue messages with "Pause" handler</center>

<p>The total processing time was a bit faster than AWS and somewhat slower than GCP. Azure Function 
instances process several messages in parallel, so it takes much less of them to do the same amount 
of work.</p>
<p>Instance number growth seems far more linear than bursty.</p>
<h3 id="what-we-learned">What we learned</h3>
<p>Based on this simple test, it&#39;s hard to say if one cloud provider handles scale-out better than 
the others.</p>
<p>It looks like all serverless platforms under stress are making decisions at the resolution of 5-15 
seconds, so the backlog processing delays are likely to be measured in minutes. It sounds quite far 
from the theoretical &quot;close to zero&quot; target but is most likely good enough for the majority of 
applications.</p>
<h2 id="crunching-numbers">Crunching Numbers</h2>
<p>That was an easy job though. Let&#39;s give cloud providers a hard time by executing CPU-heavy workloads 
and see if they survive!</p>
<p>This time, each message handler calculates a <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hash with a cost of 10. One such calculation takes about 200 ms on my laptop.</p>
<h3 id="aws">AWS</h3>
<p>Once again, I sent 100k messages to an SQS queue and recorded the processing speed and instance count.</p>
<p>Since the workload is CPU-bound, and AWS allocates CPU shares proportionally to the allocated 
memory, the instance size might have a significant influence on the result.</p>
<p>I started with the smallest memory allocation of 128 MB:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-cpubound-scaling.png" alt="AWS Lambda (128 MB) processing 100k SQS messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">AWS Lambda (128 MB) processing 100k SQS messages with "Bcrypt" handler</center>

<p>This time it took almost 10 minutes to complete the experiment.</p>
<p>The scaling shape is pretty much the same as last time, still correctly described by the formula 
<code>60 * Minutes + 5</code>. However, because AWS allocates a small fraction of a full CPU to each 128 MB 
execution, one message takes around 1,700 ms to complete. Thus, the total work increased 
approximately by the factor of 3 (47 hours if done sequentially).</p>
<p>At the peak, 612 concurrent executions were running, nearly double the amount in our initial 
experiment. So, the total processing time increased only by the factor of 2&mdash;up to 10 minutes.</p>
<p>Let&#39;s see if larger Lambda instances would improve the outcome. Here is the chart for 512 MB of 
allocated memory:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-cpubound-scaling-512.png" alt="AWS Lambda (512 MB) processing 100k SQS messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">AWS Lambda (512 MB) processing 100k SQS messages with "Bcrypt" handler</center>

<p>And yes it does. The average execution duration is down to 400 ms: 4 times less, as expected. 
The scaling shape still holds, so the entire batch was done in less than four minutes.</p>
<h3 id="gcp">GCP</h3>
<p>I executed the same experiment on Google Cloud Functions. I started with 128 MB, and it looks 
impressive:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling.png" alt="Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (128 MB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>The average execution duration is very close to Amazon&#39;s: 1,600 ms. However, GCP scaled more 
aggressively&mdash;to a staggering 1,169 parallel executions! Scaling also has a different shape: 
It&#39;s not linear but grows in steep jumps. As a result, it took less than six minutes on the 
lowest CPU profile&mdash;very close to AWS&#39;s time on a 4x more powerful CPU.</p>
<p>What will GCP achieve on a faster CPU? Let&#39;s provision 512 MB. It must absolutely crush the 
test. Umm, wait, look at that:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling-512.png" alt="Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (512 MB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>It actually... got slower. Yes, the average execution time is 4x lower: 400 ms, but the scaling 
got much less aggressive too, which canceled the speedup.</p>
<p>I confirmed it with the largest instance size of 2,048 MB:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling-2048.png" alt="Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (2 GB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>CPU is fast: 160 ms average execution time, but the total time to process 100k messages went up 
to eight minutes. Beyond the initial spike at the first minute, it failed to scale up any further 
and stayed at about 110 concurrent executions.</p>
<p>It seems that GCP is not that keen to scale out larger instances. It&#39;s probably easier to find 
many small instances available on the pool rather than a similar number of giant instances.</p>
<h3 id="azure">Azure</h3>
<p>A single invocation takes about 400 ms to complete on Azure Function. Here is the burndown chart:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-cpubound-scaling.png" alt="Azure Function processing 100k queue messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Azure Function processing 100k queue messages with "Bcrypt" handler</center>

<p>Azure spent 21 minutes to process the whole backlog. The scaling was linear, similarly to AWS, 
but with a much slower pace regarding instance size growth, about <code>2.5 * Minutes</code>.</p>
<p>As a reminder, each instance could process multiple queue messages in parallel, but each such 
execution would be competing for the same CPU resource, which doesn&#39;t help for the purely 
CPU-bound workload.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<p>Time for some conclusions and pieces of advice to apply in real serverless applications.</p>
<h3 id="serverless-is-great-for-async-data-processing">Serverless is great for async data processing</h3>
<p>If you are already using cloud services, such as managed queues and topics, serverless functions 
are the easiest way to consume them. </p>
<p>Moreover, the scalability is there too. When was the last time you ran 1,200 copies of your 
application?</p>
<h3 id="serverless-is-not-infinitely-scalable">Serverless is not infinitely scalable</h3>
<p>There are limits. Your functions won&#39;t scale perfectly to accommodate your spike&mdash;a 
provider-specific algorithm will determine the scaling pattern.</p>
<p>If you have large spikes in queue workloads, which is quite likely for medium- to high-load 
scenarios, you can and should expect delays up to several minutes before the backlog is fully 
digested.</p>
<p>All cloud providers have quotas and limits that define an upper boundary of scalability.</p>
<h3 id="cloud-providers-have-different-implementations">Cloud providers have different implementations</h3>
<p><strong>AWS Lambda</strong> seems to have a very consistent and well-documented linear scale growth for 
SQS-triggered Lambda functions. It will happily scale to 1,000 instances, or whatever other 
limit you hit first.</p>
<p><strong>Google Cloud Functions</strong> has the most aggressive scale-out strategy for the smallest instance 
sizes. It can be a cost-efficient and scalable way to run your queue-based workloads. Larger 
instances seem to scale in a more limited way, so a further investigation is required if you 
need those.</p>
<p><strong>Azure Functions</strong> share instances for multiple concurrent executions, which works better 
for I/O-bound workloads than for CPU-bound ones. Depending on the exact scenario that you 
have, it might help to play with instance-level settings.</p>
<h3 id="don-t-forget-batching">Don&#39;t forget batching</h3>
<p>For the tests, I was handling queue messages in the 1-by-1 fashion. In practice, it helps 
if you can batch several messages together and execute a single action for all of them in one go.</p>
<p>If the destination for your data supports batched operations, the throughput will usually 
increase immensely. 
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2017/09/19/processing-100000-events-per-second-on-azure-functions/">Processing 100,000 Events Per Second on Azure Functions</a>
is an excellent case to prove the point.</p>
<h3 id="you-might-get-too-much-scale">You might get too much scale</h3>
<p>A month ago, Troy Hunt published a great post 
<a href="https://www.troyhunt.com/breaking-azure-functions-with-too-many-connections/">Breaking Azure Functions with Too Many Connections</a>. 
His scenario looks very familiar: He uses queue-triggered Azure Functions to notify 
subscribers about data breaches. One day, he dropped 126 million items into the queue, and Azure 
scaled out, which overloaded Mozilla&#39;s servers and caused them to go all-timeouts.</p>
<p>Another consideration is that non-serverless dependencies limit the scalability of your serverless 
application. If you call a legacy HTTP endpoint, a SQL database, or a third-party web service&mdash;be
sure to test how they react when your serverless function scales out to hundreds of concurrent 
executions.</p>
<p>Stay tuned for more serverless performance goodness!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/scalability/'>Scalability</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/gcp/'>GCP</a>, <a href='/tags/google-cloud-functions/'>Google Cloud Functions</a>
    </div>
    
</article>


<div class="page-nav">
    
    
    <a class="page-nav-older" href="/2/index.html">Next page &gt;&gt;</span></a>
    
</div>

<div id="me">
    <p itemscope itemtype="http://data-vocabulary.org/Person">
        <img src="/images/Headshot-Square.jpg" alt="Mikhail Shilkov" itemprop="photo" />
        I'm <b><span itemprop="name">Mikhail Shilkov</span></b>, a <span itemprop="title">software developer and architect</span>,
        a Microsoft Azure MVP, Russian expat living in the Netherlands. I am passionate about cloud technologies, 
        functional programming and the intersection of the two.
    </p>
    <p>
        <a href="https://www.linkedin.com/in/mikhailshilkov/">LinkedIn</a> &#8226;
        <a href="https://twitter.com/mikhailshilkov">@mikhailshilkov</a> &#8226;
        <a href="https://github.com/mikhailshilkov">GitHub</a> &#8226;
        <a href="https://stackoverflow.com/users/1171619/mikhail">Stack Overflow</a>
    </p>
</div>
</div>
<div class="container">
    <div class="navbar navbar-footer">
        <p class="navbar-center navbar-text">Content copyright &copy; 2018 Mikhail Shilkov</p>
    </div>
</div>



<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.pack.js"></script>
<script src="/vendor/prism.js"></script>
<script src="/site.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59218480-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>