<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>

    <meta name="description" content="Software development using .NET, C#, SQL, Javascript and related technologies" />

    <title>Mikhail Shilkov</title>
    <meta name="author" content="Mikhail Shilkov">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="twitter:card" content="summary_large_image"></meta>
    <meta name="twitter:creator" content="@MikhailShilkov"></meta>
    <meta name="twitter:title" content="Mikhail Shilkov"></meta>

    <meta property="og:type" content="article" />
    <meta property="og:title" content="Mikhail Shilkov" />
    <meta property="og:url" content="https://mikhail.io/" />




    <link href="/feed/" rel="alternate" title="mikhail.io" type="application/atom+xml">
    <link href="/favicon.ico?v=2" rel="shortcut icon">

    <!-- Bootstrap -->
    <link href="/vendor/prism.css" rel="stylesheet" media="screen">
    <link href="/styles/site.css" rel="stylesheet" media="screen">

    <meta name="generator" content="DocPad v6.80.6" />
    
</head>
<body>

<div class="navbar navbar-default navbar-static-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">
                <span class="text-primary">Mikhail Shilkov</span><br />
                <span class="elevator-pitch">Serverless, Azure, FP, F# and more</span>
            </a>
        </div>
        <div class="collapse navbar-collapse navbar-right">
            <ul class="nav navbar-nav">
                <!--<li><a href="/">Blog</a></li>-->
                
                    <li><a href="/tags/">Topics</a></li>
                
                    <li><a href="/archives/">Archives</a></li>
                
                    <li><a href="/talks/">Talks</a></li>
                
                    <li><a href="/about/">About</a></li>
                
                <li class="hidden-xs">
                    <a href="/feed/" class="rss"><span class="icon icon-feed"></span></a>
                    <a href="https://www.linkedin.com/in/mikhailshilkov" class="linkedin"><span class="icon icon-linkedin"></span></a>
                    <a href="https://twitter.com/mikhailshilkov" class="twitter"><span class="icon icon-twitter"></span></a>
                    <a href="https://github.com/mikhailshilkov" class="github"><span class="icon icon-github"></span></a>
                </li>
            </ul>
            <form class="navbar-form navbar-right hidden-xs" role="search" action="https://google.com/search"
                  method="get">
                <div class="form-group">
                    <input type="search" name="q" class="form-control" placeholder="Search">
                    <input type="hidden" name="q" value="site:mikhail.io">
                </div>
            </form>
        </div>
    </div>
</div>
<div class="container">
    
    <article class="post">
    <div class="post-date">Dec 20th, 2018</div>
    
    <h1><a href='/2018/12/fairy-tale-of-fsharp-and-durable-functions/'>A Fairy Tale of F# and Durable Functions</a></h1>
    

    

    <div class="post-content">
        <p><em>The post is a part of 
<a href="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/">F# Advent Calendar 2018</a>.
It&#39;s Christmas time!</em></p>
<p>This summer I was hired by the office of Santa Claus. Santa is not just a fairy tale
character on his own&mdash;he leads a large organization that supplies gifts and happiness to millions of 
children around the globe. Like any large organization, Santa&#39;s office employs an impressive number of 
IT systems. </p>
<p>As part of its IT modernization
effort, North Pole HQ restructured the whole supply chain of Christmas gifts. Many legacy components were moved from
a self-managed data center at the North Pole&mdash;although the cooling is quite cheap there&mdash;to 
Azure cloud. Azure was an easy sell since Santa&#39;s techy elves use Office 365, SharePoint and
the .NET development stack.</p>
<p>One of the goals of the redesign was to leverage managed cloud services and serverless architecture
wherever possible. Santa has no spare elves to keep reinventing IT wheels.</p>
<h2 id="wish-fulfillment-service">Wish Fulfillment Service</h2>
<p>My assignment was to redesign the <strong>Wish Fulfillment</strong> service. The service receives
wish lists from clients (they call children &quot;clients&quot;):</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/wish-list.png" alt="Christmas Wish List"></p>
<center class="img-caption">Christmas Card with a Wish List &copy; my son Tim</center>

<p>Luckily, the list is already parsed by some other service, and also contains the metadata about
the kid&#39;s background (age, gender, and so on) and preferences.</p>
<p>For each item in the list, our service calls the <strong>Matching</strong> service, which uses machine learning,
Azure Cognitive services, and a bit of magic to determine the actual products (they call gifts &quot;products&quot;)
that best fit the client&#39;s expressed desire and profile. For instance, my son&#39;s wish for &quot;LEGO Draak&quot; matches
to &quot;LEGO NINJAGO Masters of Spinjitzu Firstbourne Red Dragon&quot;. You get the point.</p>
<p>There might be several matches for each desired item, and each result has an estimate of how
likely it is to fulfill the original request and make the child happy.</p>
<p>All the matching products are combined and sent over to the <strong>Gift Picking</strong> service. Gift Picking selects one
of the options based on its price, demand, confidence level, and the Naughty-or-Nice score of the client.</p>
<p>The last step of the workflow is to <strong>Reserve</strong> the selected gift in the warehouse and shipping system
called &quot;Santa&#39;s Archive of Products&quot;, also referred to as SAP.</p>
<p>Here is the whole flow in one picture:</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/gift-fulfillment-service.png" alt="Gift Fulfillment Workflow"></p>
<center class="img-caption">Gift Fulfillment Workflow</center>

<p>How should we implement this service?</p>
<h2 id="original-design">Original Design</h2>
<p>The Wish Fulfillment service should run in the cloud and integrate with other services. It
should be able to process millions of requests in December and stay very cheap to run during the
rest of the year. We decided to leverage serverless architecture with 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview">Azure Functions</a> on the 
<a href="https://azure.microsoft.com/en-us/pricing/details/functions/">Consumption Plan</a>. Serverless
Functions are:</p>
<ul>
<li><p><strong>Fully Managed</strong>: the cloud provider provisions resources, scales them based on the load, takes
care of uptime and reliability;</p>
</li>
<li><p><strong>Event-Driven</strong>: for each serverless Function you have to define a specific trigger&mdash;the 
event type which causes it to run, be it an HTTP endpoint or a queue message;</p>
</li>
<li><p><strong>Changed per Execution</strong>: it costs nothing to run the application if there is no usage,
and the cost of busy applications is proportional to the actual resource utilization.</p>
</li>
</ul>
<p>Here is the diagram of the original design:</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/azure-functions-diagram.png" alt="Workflow Design with Azure Functions and Storage Queues"></p>
<center class="img-caption">Workflow Design with Azure Functions and Storage Queues</center>

<p>We used Azure Storage Queues to keep the whole flow asynchronous and more resilient to failures
and load fluctuation.</p>
<p>This design would mostly work, but we found a couple of problems with it:</p>
<ul>
<li><p>The Functions were manually wired via storage queues and corresponding bindings. The workflow
was spread over infrastructure definition and thus was hard to grasp.</p>
</li>
<li><p>We had to pass all items of each wish list into a single invocation of Matching Function,
otherwise combining the matching results from multiple queue messages would be tricky. </p>
</li>
<li><p>Although not in scope for the initial release, there were plans to add manual elf 
intervention for poorly matched items. This feature would require a change in the flow design:
it&#39;s not trivial to fit long-running processes into the pipeline.</p>
</li>
</ul>
<p>To improve on these points, we decided to try 
<a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Durable Functions</a>&mdash;a library 
that brings workflow orchestration to Azure Functions. It introduces several tools to define stateful,
potentially long-running operations, and handles a lot of the mechanics of reliable communication 
and state management behind the scenes.</p>
<p>If you want to know more about what Durable Functions are and why they might be a good idea,
I invite you to read my article 
<a href="https://mikhail.io/2018/12/making-sense-of-azure-durable-functions/">Making Sense of Azure Durable Functions</a>
(20 minutes read).</p>
<p>For the rest of this post, I will walk you through the implementation of the Wish Fulfillment workflow
with Azure Durable Functions.</p>
<h2 id="domain-model">Domain Model</h2>
<p>A good design starts with a decent domain model. Luckily, the project was built with F#&mdash;the language with
the richest domain modeling capabilities in the .NET ecosystem.</p>
<h3 id="types">Types</h3>
<p>Our service is invoked with a wish list as the input parameter, so let&#39;s start with the type <code>WishList</code>:</p>
<pre><code class="language-fsharp">type WishList = 
{
    Kid: Customer
    Wishes: string list
}</code></pre>
<p>It contains information about the author of the list and recognized &quot;order&quot; items. <code>Customer</code> is a custom type;
for now, it&#39;s not important what&#39;s in it.</p>
<p>For each wish we want to produce a list of possible matches:</p>
<pre><code class="language-fsharp">type Match = 
{
    Product: Product
    Confidence: Probability
}</code></pre>
<p>The product is a specific gift option from Santa&#39;s catalog, and the confidence is a number 
from <code>0.0</code> to <code>1.0</code> of how strong the match is.</p>
<p>The end goal of our service is to produce a <code>Reservation</code>:</p>
<pre><code class="language-fsharp">type Reservation = 
{
    Kid: Customer
    Product: Product
}</code></pre>
<p>It represents the exact product selection for the specific kid.</p>
<h3 id="functions">Functions</h3>
<p>The Wish Fulfillment service needs to perform three actions, which can be
modeled with three strongly-typed asynchronous functions.</p>
<p><em>Note: I use lowercase &quot;function&quot; for F# functions and capitalize &quot;Function&quot; for Azure Functions
throughout the article to minimize confusion.</em></p>
<p>The <strong>first action</strong> finds matches for each wish:</p>
<pre><code class="language-fsharp">// string -&gt; Async&lt;Match list&gt;
let findMatchingGift (wish: string) = async {
    // Call a custom machine learning model
    // The real implementation uses the Customer profile to adjust decisions by age, etc.
    // but we&#39;ll keep the model simple for now.
}</code></pre>
<p>The first line of all my function snippets shows the function type. In this case, it&#39;s a mapping 
from the text of the child&#39;s wish (<code>string</code>) to a list of matches (<code>Match list</code>).</p>
<p>The <strong>second action</strong> takes the <em>combined</em> list of all matches of all wishes and picks one. Its
real implementation is Santa&#39;s secret sauce, but my model just picks the one with the highest
confidence level:</p>
<pre><code class="language-fsharp">// Match list -&gt; Product
let pickGift (candidates: Match list) =
    candidates
    |&gt; List.sortByDescending (fun x -&gt; x.Confidence)
    |&gt; List.head
    |&gt; (fun x -&gt; x.Product)</code></pre>
<p>Given the picked <code>gift</code>, the reservation is merely <code>{ Kid = wishlist.Kid; Product = gift }</code>,
not worthy of a separate action.</p>
<p>The <strong>third action</strong> registers a reservation in the SAP system:</p>
<pre><code class="language-fsharp">// Reservation -&gt; Async&lt;unit&gt;
let reserve (reservation: Reservation) = async {
    // Call Santa&#39;s Archive of Products
}</code></pre>
<h3 id="workflow">Workflow</h3>
<p>The fulfillment service combines the three actions into one workflow:</p>
<pre><code class="language-fsharp">// WishList -&gt; Async&lt;Reservation&gt;
let workflow (wishlist: WishList) = async {

    // 1. Find matches for each wish 
    let! matches = 
        wishlist.Wishes
        |&gt; List.map findMatchingGift
        |&gt; Async.Parallel

    // 2. Pick one product from the combined list of matches
    let gift = pickGift (List.concat matches)

    // 3. Register and return the reservation
    let reservation = { Kid = wishlist.Kid; Product = gift }
    do! reserve reservation
    return reservation
}</code></pre>
<p>The workflow implementation is a nice and concise summary of the actual domain flow.</p>
<p>Note that the Matching service is called multiple times in parallel, and then
the results are easily combined by virtue of the <code>Async.Parallel</code> F# function.</p>
<p>So how do we translate the domain model to the actual implementation on top of
serverless Durable Functions?</p>
<h2 id="classic-durable-functions-api">Classic Durable Functions API</h2>
<p>C# was the first target language for Durable Functions; Javascript is now fully supported too.</p>
<p>F# wasn&#39;t initially declared as officially supported, but since F# runs on top of the same .NET runtime
as C#, it has always worked. I have a blog post about
<a href="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/">Azure Durable Functions in F#</a> and
have added <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/fsharp">F# samples</a>
to the official repository.</p>
<p>Here are two examples from that old F# code of mine (they have nothing to do with our 
gift fulfillment domain):</p>
<pre><code class="language-fsharp">// 1. Simple sequencing of activities
let Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = task {
  let! hello1 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Tokyo&quot;)
  let! hello2 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Seattle&quot;)
  let! hello3 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;London&quot;)
  return [hello1; hello2; hello3]
}     

// 2. Parallel calls snippet
let tasks = Array.map (fun f -&gt; context.CallActivityAsync&lt;int64&gt;(&quot;E2_CopyFileToBlob&quot;, f)) files
let! results = Task.WhenAll tasks</code></pre>
<p>This code works and does its job, but doesn&#39;t look like idiomatic F# code:</p>
<ul>
<li>No strong typing: Activity Functions are called by name and with types manually specified</li>
<li>Functions are not curried, so partial application is hard</li>
<li>The need to pass the <code>context</code> object around for any Durable operation</li>
</ul>
<p>Although not shown here, the other samples read input parameters, handle errors, and enforce
timeouts&mdash;all look too C#-y.</p>
<h2 id="better-durable-functions">Better Durable Functions</h2>
<p>Instead of following the sub-optimal route, we implemented the service with a more F#-idiomatic API.
I&#39;ll show the code first, and then I&#39;ll explain its foundation.</p>
<p>The implementation consists of three parts:</p>
<ul>
<li><strong>Activity</strong> Functions&mdash;one per action from the domain model</li>
<li><strong>Orchestrator</strong> Function defines the workflow</li>
<li><strong><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings">Azure Functions bindings</a></strong> 
to instruct how to run the application in the cloud</li>
</ul>
<h3 id="activity-functions">Activity Functions</h3>
<p>Each Activity Function defines one step of the workflow: Matching, Picking, and Reserving. We
simply reference the F# functions of those actions in one-line definitions:</p>
<pre><code class="language-fsharp">let findMatchingGiftActivity = Activity.defineAsync &quot;FindMatchingGift&quot; findMatchingGift
let pickGiftActivity = Activity.define &quot;PickGift&quot; pickGift
let reserveActivity = Activity.defineAsync &quot;Reserve&quot; reserve</code></pre>
<p>Each activity is defined by a name and a function.</p>
<h3 id="orchestrator">Orchestrator</h3>
<p>The Orchestrator calls Activity Functions to produce the desired outcome of the service. The code
uses a custom computation expression:</p>
<pre><code class="language-fsharp">let workflow wishlist = orchestrator {
    let! matches = 
        wishlist.Wishes
        |&gt; List.map (Activity.call findMatchingGiftActivity)
        |&gt; Activity.all

    let! gift = Activity.call pickGiftActivity (List.concat matches)

    let reservation = { Kid = wishlist.Kid; Product = gift }
    do! Activity.call reserveActivity reservation
    return reservation
}</code></pre>
<p>Notice how closely it matches the workflow definition from our domain model:</p>
<p><img src="/2018/12/fairy-tale-of-fsharp-and-durable-functions/durable-orchestrator-vs-async.png" alt="Async Function vs. Durable Orchestrator"></p>
<center class="img-caption">Async function vs. Durable Orchestrator</center>

<p>The only differences are:</p>
<ul>
<li><code>orchestrator</code> computation expression is used instead of <code>async</code> because multi-threading is
not allowed in Orchestrators</li>
<li><code>Activity.call</code> replaces of direct invocations of functions</li>
<li><code>Activity.all</code> substitutes <code>Async.Parallel</code></li>
</ul>
<h3 id="hosting-layer">Hosting layer</h3>
<p>An Azure Function trigger needs to be defined to host any piece of code as a cloud Function. This can
be done manually in <code>function.json</code>, or via trigger generation from .NET attributes. In my case
I added the following four definitions:</p>
<pre><code class="language-fsharp">[&lt;FunctionName(&quot;FindMatchingGift&quot;)&gt;]
let FindMatchingGift([&lt;ActivityTrigger&gt;] wish) = 
    Activity.run findMatchingGiftActivity wish

[&lt;FunctionName(&quot;PickGift&quot;)&gt;]
let PickGift([&lt;ActivityTrigger&gt;] matches) = 
    Activity.run pickGiftActivity matches

[&lt;FunctionName(&quot;Reserve&quot;)&gt;]
let Reserve([&lt;ActivityTrigger&gt;] wish) = 
    Activity.run reserveActivity wish

[&lt;FunctionName(&quot;WishlistFulfillment&quot;)&gt;]
let Workflow ([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) =
    Orchestrator.run (workflow, context)</code></pre>
<p>The definitions are very mechanical and, again, strongly typed (apart from Functions&#39; names).</p>
<h3 id="ship-it-">Ship It!</h3>
<p>These are all the bits required to get our Durable Wish Fulfillment service up and running.
From this point, we can leverage all the existing tooling of Azure Functions:</p>
<ul>
<li>Visual Studio and Visual Studio Code for development and debugging</li>
<li><a href="https://github.com/Azure/azure-functions-core-tools">Azure Functions Core Tools</a> to run
the application locally and deploy it to Azure</li>
<li>The latest version of the Core Tools has dedicated commands to 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management">manage instances of Durable Functions</a></li>
</ul>
<p>There is a learning curve in the process of adopting the serverless architecture. However, a small
project like ours is a great way to do the learning. It sets Santa&#39;s IT department on the road
to success, and children will get better gifts more reliably!</p>
<h2 id="durablefunctions-fsharp">DurableFunctions.FSharp</h2>
<p>The above code was implemented with the library 
<a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">DurableFunctions.FSharp</a>. I created
this library as a thin F#-friendly wrapper around Durable Functions.</p>
<p>Frankly speaking, the whole purpose of this article is to introduce the library and make you curious
enough to give it a try. DurableFunctions.FSharp has several pieces in the toolbox:</p>
<ul>
<li><p><code>OrchestratorBuilder</code> and <code>orchestrator</code> computation expression which encapsulates proper 
usage of <code>Task</code>-based API of <code>DurableOrchestrationContext</code></p>
</li>
<li><p><code>Activity</code> generic type to define activities as first-class values</p>
</li>
<li><p><code>Activity</code> module with helper functions to call activities</p>
</li>
<li><p>Adapters for Azure Functions definition for <code>Async</code> and <code>Orchestrator</code></p>
</li>
<li><p>API of the original Durable Extensions is still available, so you can fall back to them if needed</p>
</li>
</ul>
<p>In my opinion, F# is a great language to develop serverless Functions. The simplicity of working with functions,
immutability by default, strong type system, focus on data pipelines are all useful in the world of
event-driven cloud applications.</p>
<p>Azure Durable Functions brings higher-level abstractions to compose workflows out of simple building
blocks. The goal of DurableFunctions.FSharp is to make such composition natural and enjoyable for F#
developers.</p>
<p><a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp#getting-started">Getting Started</a> 
is as easy as creating a new .NET Core project and referencing a NuGet package. </p>
<p>I&#39;d love to get as much feedback as possible! Leave comments below, create issues
on the <a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">GitHub repository</a>, or open a PR. 
This would be super awesome!</p>
<p>Happy coding, and Merry Christmas!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Many thanks to <a href="https://twitter.com/kashimizMSFT">Katy Shimizu</a>, <a href="https://twitter.com/DevonBurriss">Devon Burriss</a>,
<a href="https://twitter.com/iwasdavid">Dave Lowe</a>, <a href="https://twitter.com/cgillum">Chris Gillum</a>
for reviewing the draft of this article and their valuable contributions and suggestions.</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/f#/'>F#</a>, <a href='/tags/workflows/'>Workflows</a>, <a href='/tags/azure-durable-functions/'>Azure Durable Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Dec 7th, 2018</div>
    
    <h1><a href='/2018/12/making-sense-of-azure-durable-functions/'>Making Sense of Azure Durable Functions</a></h1>
    

    

    <div class="post-content">
        <p>Stateful Workflows on top of Stateless Serverless Cloud Functions&mdash;this is the essence
of the Azure Durable Functions library. That&#39;s a lot of fancy words in one sentence, and they
might be hard for the majority of readers to understand.</p>
<p>Please join me on the journey where I&#39;ll try to explain how those buzzwords fit
together. I will do this in 3 steps:</p>
<ul>
<li>Describe the context of modern cloud applications relying on serverless architecture;</li>
<li>Identify the limitations of basic approaches to composing applications out of the simple building blocks;</li>
<li>Explain the solutions that Durable Functions offer for those problems.</li>
</ul>
<h2 id="microservices">Microservices</h2>
<p>Traditionally, server-side applications were built in a style which is now referred to as
<strong>Monolith</strong>. If multiple people and teams were developing parts of the same application, they
mostly contributed to the same code base. If the code base were structured well, it would have
some distinct modules or components, and a single team would typically own each module:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/monolith.png" alt="Monolith"></p>
<center class="img-caption">Multiple components of a monolithic application</center>

<p>Usually, the modules would be packaged together at build time and then deployed as a single
unit, so a lot of communication between modules would stay inside the OS process.</p>
<p>Although the modules could stay loosely coupled over time, the coupling almost always occurred
on the level of the data store because all teams would use a single centralized database.</p>
<p>This model works great for small- to medium-size applications, but it turns out that teams
start getting in each other&#39;s way as the application grows since synchronization of contributions
takes more and more effort.</p>
<p>As a complex but viable alternative, the industry came up with a revised service-oriented
approach commonly called <strong>Microservices</strong>. The teams split the big application into &quot;vertical slices&quot; 
structured around the distinct business capabilities:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/microservices.png" alt="Microservices"></p>
<center class="img-caption">Multiple components of a microservice-based application</center>

<p>Each team then owns a whole vertical&mdash;from public communication contracts, or even UIs, down
to the data storage. Explicitly shared databases are strongly discouraged. Services talk to each
other via documented and versioned public contracts.</p>
<p>If the borders for the split were selected well&mdash;and that&#39;s the most tricky part&mdash;the
contracts stay stable over time, and thin enough to avoid too much chattiness. This gives
each team enough autonomy to innovate at their best pace and to make independent technical decisions.</p>
<p>One of the drawbacks of microservices is the change in deployment model. The services are now
deployed to separate servers connected via a network:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/distributed-system.png" alt="Distributed Systems"></p>
<center class="img-caption">Challenges of communication between distributed components</center>

<p>Networks are fundamentally unreliable: they work just fine most of the time, but when they 
fail, they fail in all kinds of unpredictable and least desirable manners. There are books
written on the topic of distributed systems architecture. TL;DR: it&#39;s hard.</p>
<p>A lot of the new adopters of microservices tend to ignore such complications. REST over HTTP(S) is the
dominant style of connecting microservices. Like any other synchronous communication
protocol, it makes the system brittle.</p>
<p>Consider what happens when one service becomes temporary unhealthy: maybe its database goes offline, or it&#39;s struggling to 
keep up with the request load, or a new version of the service is being deployed. All the requests to the problematic service start
failing&mdash;or worse&mdash;become very slow. The dependent service waits for the response, and
thus blocks all incoming requests of its own. The error propagates upstream very quickly causing cascading
failures all over the place:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/cascading-failures.png" alt="Cascading Failures"></p>
<center class="img-caption">Error in one component causes cascading failures</center>

<p>The application is down. Everybody screams and starts the blame war.</p>
<h2 id="event-driven-applications">Event-Driven Applications</h2>
<p>While cascading failures of HTTP communication can be mitigated with patterns like a circuit breaker
and graceful degradation, a better solution is to switch to the asynchronous style of communication
as the default. Some kind of persistent queueing service is used as an intermediary.</p>
<p>The style of application architecture which is based on sending events between services
is known as <strong>Event-Driven</strong>. When a service does something useful, it publishes an event&mdash;a record 
about the fact which happened to its business domain. Another service listens to the published events and 
executes its own duty in response to those facts:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/event-driven.png" alt="Event-Driven Application"></p>
<center class="img-caption">Communication in event-driven applications</center>

<p>The service that produces events might not know about the consumers. New event subscribers can
be introduced over time. This works better in theory than in practice, but the services tend to
get coupled less.</p>
<p>More importantly, if one service is down, other services don&#39;t catch fire immediately. The
upstream services keep publishing the events, which build up in the queue but can be stored safely
for hours or days. The downstream services might not be doing anything useful for this particular
flow, but it can stay healthy otherwise.</p>
<p>However, another potential issue comes hand-in-hand with loose coupling: low cohesion.
As Martin Fowler notices in his essay
<a href="https://martinfowler.com/articles/201701-event-driven.html">What do you mean by &quot;Event-Driven&quot;</a>:</p>
<blockquote>
<p>It&#39;s very easy to make nicely decoupled systems with event notification, without realizing 
that you&#39;re losing sight of the larger-scale flow.</p>
</blockquote>
<p>Given many components that publish and subscribe to a large number of event types, it&#39;s easy to stop
seeing the forest for the trees. Combinations of events usually constitute gradual workflows executed 
in time. A workflow is more than the sum of its parts, and understanding of the high-level flow is 
paramount to controlling the system behavior.</p>
<p>Hold this thought for a minute; we&#39;ll get back to it later. Now it&#39;s time to talk <em>cloud</em>.</p>
<h2 id="cloud">Cloud</h2>
<p>The birth of public cloud changed the way we architect applications. It made many things
much more straightforward: provisioning of new resources in minutes instead of months, scaling elastically
based on demand, and resiliency and disaster recovery at the global scale.</p>
<p>It made other things more complicated. Here is the picture of the global Azure network:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/azure-network.png" alt="Azure Network"></p>
<center class="img-caption">Azure locations with network connections</center>

<p>There are good reasons to deploy applications to more than one geographical location:
among others, to reduce network latency by staying close to the customer, and to achieve resilience through 
geographical redundancy. Public Cloud is the ultimate distributed system. As you remember,
distributed systems are hard.</p>
<p>There&#39;s more to that. Each cloud provider has dozens and dozens of managed services, which is
the curse and the blessing. Specialized services are great to provide off-the-shelf solutions 
to common complex problems. On the flip side, each service has distinct properties regarding 
consistency, resiliency and fault tolerance.</p>
<p>In my opinion, at this point developers have to embrace the public cloud and apply the distributed 
system design on top of it. If you agree, there is an excellent way to approach it.</p>
<h2 id="serverless">Serverless</h2>
<p>The slightly provocative term <strong>serverless</strong> is used to describe cloud services that do not
require provisioning of VMs, instances, workers, or any other fixed capacity to run
custom applications on top of them. Resources are allocated dynamically and transparently,
and the cost is based on their actual consumption, rather than on pre-purchased capacity.</p>
<p>Serverless is more about operational and economical properties of the system than about the
technology per se. Servers do exist, but they are someone else&#39;s concern. You don&#39;t manage
the uptime of serverless applications: the cloud provider does.</p>
<p>On top of that, you pay for what you use, similar to the consumption of other commodity resources
like electricity. Instead of buying a generator to power up your house, you just purchase energy
from the power company. You lose some control (e.g., no way to select the voltage), but this is fine
in most cases. The great benefit is no need to buy and maintain the hardware.</p>
<p>Serverless compute does the same: it supplies standard services on a pay-per-use basis.</p>
<p>If we talk more specifically about Function-as-a-Service offerings like Azure Functions, they
provide a standard model to run small pieces of code in the cloud.
You zip up the code or binaries and send it to Azure; Microsoft takes care of all the
hardware and software required to run it. The infrastructure automatically scales up or down based
on demand, and you pay per request, CPU time and memory that the application consumed. 
No usage&mdash;no bill.</p>
<p>However, there&#39;s always a &quot;but&quot;. FaaS services come with an opinionated development model that
applications have to follow:</p>
<ul>
<li><p><strong>Event-Driven</strong>: for each serverless function you have to define a specific trigger&mdash;the 
event type which causes it to run, be it an HTTP endpoint or a queue message;</p>
</li>
<li><p><strong>Short-Lived</strong>: functions can only run up to several minutes, and preferably for a few seconds 
or less;</p>
</li>
<li><p><strong>Stateless</strong>: as you don&#39;t control where and when function instances are provisioned or
deprovisioned, there is no way to store data within the process between requests reliably;
external storage has to be utilized.</p>
</li>
</ul>
<p>Frankly speaking, the majority of existing applications don&#39;t really fit into this model.
If you are lucky to work on a new application (or a new module of it), you are in better shape.</p>
<p>A lot of the serverless applications may be designed to look somewhat similar to this example
from <a href="https://www.serverless360.com/blog/building-reactive-solution-with-azure-event-grid">the Serverless360 blog</a>:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/serviceful-example.png" alt="Serviceful Serverless Application"></p>
<center class="img-caption">Sample application utilizing "serviceful" serverless architecture</center>

<p>There are 9 managed Azure services working together in this app. Most of them have a unique purpose, but
the services are all glued together with Azure Functions. An image is uploaded to Blob Storage, an
Azure Function calls Vision API to recognize the license plate and send the result to Event Grid, another 
Azure Function puts that event to Cosmos DB, and so on.</p>
<p>This style of cloud applications is sometimes referred to as <strong>Serviceful</strong> to emphasize the heavy usage
of managed services &quot;glued&quot; together by serverless functions.</p>
<p>Creating a comparable application without any managed services would be a much harder task,
even more so, if the application has to run at scale. Moreover, there&#39;s no way to keep the pay-as-you-go 
pricing model in the self-service world.</p>
<p>The application pictured above is still pretty straightforward. The processes
in enterprise applications are often much more sophisticated.</p>
<p>Remember the quote from Martin Fowler about losing sight of the large-scale flow. That was
true for microservices, but it&#39;s even more true for the &quot;nanoservices&quot; of cloud functions.</p>
<p>I want to dive deeper and give you several examples of related problems.</p>
<h2 id="challenges-of-serverless-composition">Challenges of Serverless Composition</h2>
<p>For the rest of the article, I&#39;ll define an imaginary business application for booking trips to software
conferences. In order to go to a conference, I need to buy tickets to the conference itself,
purchase the flights, and book a room at a hotel.</p>
<p>In this scenario, it makes sense to create three Azure Functions, each one responsible for one step
of the booking process. As we prefer message passing, each Function emits an event which
the next function can listen for:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/conference-booking.png" alt="Conference Booking Application"></p>
<center class="img-caption">Conference booking application</center>

<p>This approach works, however, problems do exist.</p>
<h3 id="flexible-sequencing">Flexible Sequencing</h3>
<p>As we need to execute the whole booking process in sequence, the Azure Functions are wired
one after another by configuring the output of one function to match with the event source of
the downstream function.</p>
<p>In the picture above, the functions&#39; sequence is hard-defined. If we were to swap the order of booking 
the flights and reserving the hotel, that would require a code change&mdash;at least of the 
input/output wiring definitions, but probably also the functions&#39; parameter types.</p>
<p>In this case, are the functions <em>really</em> decoupled?</p>
<h3 id="error-handling">Error Handling</h3>
<p>What happens if the Book Flight function becomes unhealthy, perhaps due to the
outage of the third-party flight-booking service? Well, that&#39;s why we use asynchronous messaging:
after the function execution fails, the message returns to the queue and is picked
up again by another execution.</p>
<p>However, such retries happen almost immediately for most event sources. This might not
be what we want: an exponential back-off policy could be a smarter idea. At this point,
the retry logic becomes <strong>stateful</strong>: the next attempt should &quot;know&quot; the history of previous attempts
to make a decision about retry timing.</p>
<p>There are more advanced error-handling patterns too. If executions failures are not
intermittent, we may decide to cancel the whole process and run compensating actions
against the already completed steps.</p>
<p>An example of this is a fallback action: if the flight is not possible (e.g., no routes for this
origin-destination combination), the flow could choose to book a train instead:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/fallback-on-error.png" alt="Fallback On Error"></p>
<center class="img-caption">Fallback after 3 consecutive failures</center>

<p>This scenario is not trivial to implement with stateless functions. We could wait until a
message goes to the dead-letter queue and then route it from there, but this is brittle and
not expressive enough.</p>
<h3 id="parallel-actions">Parallel Actions</h3>
<p>Sometimes the business process doesn&#39;t have to be sequential. In our reservation scenario,
there might be no difference whether we book a flight before a hotel or vice versa. It could
be desirable to run those actions in parallel.</p>
<p>Parallel execution of actions is easy with the pub-sub capabilities of an event bus: both functions
should subscribe to the same event and act on it independently.</p>
<p>The problem comes when we need to reconcile the outcomes of parallel actions, e.g., calculate the
final price for expense reporting purposes:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/fanout-fanin.png" alt="Fan-out / Fan-in"></p>
<center class="img-caption">Fan-out / fan-in pattern</center>

<p>There is no way to implement the Report Expenses block as a single Azure Function: functions
can&#39;t be triggered by two events, let alone correlate two <em>related</em> events.</p>
<p>The solution would probably include two functions, one per event, and the shared storage
between them to pass information about the first completed booking to the one who
completes last. All this wiring has to be implemented in custom code. The complexity grows
if more than two functions need to run in parallel.</p>
<p>Also, don&#39;t forget the edge cases. What if one of the function fails? How do you make sure there is
no race condition when writing and reading to/from the shared storage?</p>
<h3 id="missing-orchestrator">Missing Orchestrator</h3>
<p>All these examples give us a hint that we need an additional tool to organize low-level
single-purpose independent functions into high-level workflows.</p>
<p>Such a tool can be called an <strong>Orchestrator</strong> because its sole mission is to delegate work
to stateless actions while maintaining the big picture and history of the flow.</p>
<p>Azure Durable Functions aims to provide such a tool.</p>
<h2 id="introducing-azure-durable-functions">Introducing Azure Durable Functions</h2>
<h3 id="azure-functions">Azure Functions</h3>
<p>Azure Functions is the serverless compute service from Microsoft. Functions are event-driven:
each function defines a <strong>trigger</strong>&mdash;the exact definition of the event source, for instance,
the name of a storage queue.</p>
<p>Azure Functions can be programmed in <a href="https://docs.microsoft.com/en-us/azure/azure-functions/supported-languages">several languages</a>. 
A basic Function with a
<a href="https://docs.microsoft.com/azure/azure-functions/functions-bindings-storage-queue">Storage Queue trigger</a>
implemented in C# would look like this: </p>
<pre><code class="language-csharp">[FunctionName(&quot;MyFirstFunction&quot;)]
public static void QueueTrigger(
    [QueueTrigger(&quot;myqueue-items&quot;)] string myQueueItem, 
    ILogger log)
{
    log.LogInformation($&quot;C# function processed: {myQueueItem}&quot;);
}</code></pre>
<p>The <code>FunctionName</code> attribute exposes the C# static method as an Azure Function named <code>MyFirstFunction</code>. 
The <code>QueueTrigger</code> attribute defines the name of the storage queue to listen to. The function body
logs the information about the incoming message.</p>
<h3 id="durable-functions">Durable Functions</h3>
<p><a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Durable Functions</a>
is a library that brings workflow orchestration abstractions to
Azure Functions. It introduces a number of idioms and tools to define stateful,
potentially long-running operations, and manages a lot of mechanics of reliable communication 
and state management behind the scenes.</p>
<p>The library records the history of all actions in Azure Storage services, enabling durability 
and resilience to failures.</p>
<p>Durable Functions is 
<a href="https://github.com/Azure/azure-functions-durable-extension">open source</a>, 
Microsoft accepts external contributions, and the community is quite active.</p>
<p>Currently, you can write Durable Functions in 3 programming languages: C#, F#, and
Javascript (Node.js). All my examples are going to be in C#. For Javascript,
check <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/quickstart-js-vscode">this quickstart</a>
and <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/javascript">these samples</a>.
For F# see <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/fsharp">the samples</a>, 
<a href="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/">my walkthrough</a> and stay
tuned for another article soon.</p>
<p>Workflow building functionality is achieved by the introduction of two additional types
of triggers: Activity Functions and Orchestrator Functions.</p>
<h3 id="activity-functions">Activity Functions</h3>
<p>Activity Functions are simple stateless single-purpose building blocks
that do just one task and have no awareness of the bigger workflow. 
A new trigger type,
<code>ActivityTrigger</code>, was introduced to expose functions as workflow steps, as
I explain below.</p>
<p>Here is a simple Activity Function implemented in C#:</p>
<pre><code class="language-csharp">[FunctionName(&quot;BookConference&quot;)]
public static ConfTicket BookConference([ActivityTrigger] string conference)
{
    var ticket = BookingService.Book(conference);
    return new ConfTicket { Code = ticket };
}</code></pre>
<p>It has a common <code>FunctionName</code> attribute to expose the C# static method as an
Azure Function named <code>BookConference</code>. The name is important because it is used to
invoke the activity from orchestrators.</p>
<p>The <code>ActivityTrigger</code> attribute defines the trigger type and points to the input
parameter <code>conference</code> which the activity expects to get for each invocation.</p>
<p>The function can return a result of any serializable type; my sample function
returns a simple property bag called <code>ConfTicket</code>.</p>
<p>Activity Functions can do pretty much anything: call other services, load and
save data from/to databases, and use any .NET libraries.</p>
<h3 id="orchestrator-functions">Orchestrator Functions</h3>
<p>The Orchestrator Function is a unique concept introduced by Durable Functions. Its sole
purpose is to manage the flow of execution and data among several activity functions.</p>
<p>Its most basic form chains multiple independent activities into a single
sequential workflow.</p>
<p>Let&#39;s start with an example which books a conference ticket, a flight itinerary, and
a hotel room one-by-one:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/sequential-workflow.png" alt="Sequential Workflow"></p>
<center class="img-caption">3 steps of a workflow executed in sequence</center>

<p>The implementation of this workflow is defined by another C# Azure Function, this time with
<code>OrchestrationTrigger</code>:</p>
<pre><code class="language-csharp">[FunctionName(&quot;SequentialWorkflow&quot;)]
public static async Task Sequential([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var conference = await context.CallActivityAsync&lt;ConfTicket&gt;(&quot;BookConference&quot;, &quot;ServerlessDays&quot;);
    var flight = await context.CallActivityAsync&lt;FlightTickets&gt;(&quot;BookFlight&quot;, conference.Dates);
    await context.CallActivityAsync(&quot;BookHotel&quot;, flight.Dates);
}</code></pre>
<p>Again, attributes are used to describe the function for the Azure runtime.</p>
<p>The only input parameter has type <code>DurableOrchestrationContext</code>. This context is the tool that
enables the orchestration operations.</p>
<p>In particular, the <code>CallActivityAsync</code> method is used three times to invoke three activities one after the other.
The method body looks very typical for any C# code working with a <code>Task</code>-based API. However,
the behavior is entirely different. Let&#39;s have a look at the implementation details.</p>
<h2 id="behind-the-scenes">Behind the Scenes</h2>
<p>Let&#39;s walk through the lifecycle of one execution of the sequential workflow above.</p>
<p>When the orchestrator starts running, the first <code>CallActivityAsync</code> invocation is made to book the
conference ticket. What actually happens here is that a queue message is sent from the orchestrator
to the activity function. </p>
<p>The corresponding activity function gets triggered by the queue message. It does its job (books the 
ticket) and returns the result. The activity function serializes the result and sends it as a queue 
message back to the orchestrator:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/durable-messaging.png" alt="Durable Functions: Message Passing"></p>
<center class="img-caption">Messaging between the orchestrator and the activity</center>

<p>When the message arrives, the orchestrator gets triggered again and can proceed to the second
activity. The cycle repeats&mdash;a message gets sent to Book Flight activity, it gets triggered, does its
job, and sends a message back to the orchestrator.
The same message flow happens for the third call.</p>
<h3 id="stop-resume-behavior">Stop-resume behavior</h3>
<p>As discussed earlier, message passing is intended to decouple the sender and receiver in time.
For every message in the scenario above, no immediate response is expected. </p>
<p>On the C# level, when the <code>await</code> operator is executed, the code doesn&#39;t block the execution of the whole
orchestrator. Instead, it just quits: the orchestrator stops being active and its current step completes.</p>
<p>Whenever a return message arrives from an activity, the orchestrator code restarts. It always starts
with the first line. Yes, this means that the same line is executed multiple times: up to the number of
messages to the orchestrator.</p>
<p>However, the orchestrator stores the history of its past executions in Azure Storage, so the effect of the second pass
of the first line is different: instead of sending a message to the activity it already knows
the result of that activity, so <code>await</code> returns this result back and assigns it to the <code>conference</code> variable.</p>
<p>Because of these &quot;replays&quot;, the orchestrator&#39;s implementation has to be deterministic: don&#39;t use
<code>DateTime.Now</code>, random numbers or multi-thread operations; more details
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints">here</a>.</p>
<h3 id="event-sourcing">Event Sourcing</h3>
<p>Azure Functions are stateless, while workflows require a state to keep track of their progress. Every time a new
action towards the workflow&#39;s execution happens, the framework automatically records an event in table storage.</p>
<p>Whenever an orchestrator restarts the execution because a new message arrives from its activity,
it loads the complete history of this particular execution from storage. Durable Context uses
this history to make decisions whether to call the activity or return the previously stored result.</p>
<p>The pattern of storing the complete history of state changes as an append-only event store is
known as Event Sourcing. Event store provides several benefits:</p>
<ul>
<li><strong>Durability</strong>&mdash;if a host running an orchestration fails, the history is retained in
persistent storage and is loaded by the new host where the orchestration restarts;</li>
<li><strong>Scalability</strong>&mdash;append-only writes are fast and easy to spread over multiple storage servers;</li>
<li><strong>Observability</strong>&mdash;no history is ever lost, so it&#39;s straightforward to inspect and
analyze even after the workflow is complete.</li>
</ul>
<p>Here is an illustration of the notable events that get recorded during our sequential workflow:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/event-sourcing.png" alt="Durable Functions: Event Sourcing"></p>
<center class="img-caption">Log of events in the course of orchestrator progression</center>

<h3 id="billing">Billing</h3>
<p>Azure Functions on the serverless consumption-based plan are billed per execution + per duration of
execution.</p>
<p>The stop-replay behavior of durable orchestrators causes the single workflow &quot;instance&quot; to execute
the same orchestrator function multiple times. This also means paying for several short
executions.</p>
<p>However, the total bill usually ends up being much lower compared to the potential cost of blocking
synchronous calls to activities. The price of 5 executions of 100 ms each is significantly lower
than the cost of 1 execution of 30 seconds.</p>
<p>By the way, the first million executions per month are 
<a href="https://azure.microsoft.com/en-us/pricing/details/functions/">at no charge</a>,
so many scenarios incur no cost at all from Azure Functions service.</p>
<p>Another cost component to keep in mind is Azure Storage. Queues and Tables that are used behind the
scenes are charged to the end customer. In my experience, this charge remains close to zero for
low- to medium-load applications.</p>
<p>Beware of unintentional eternal loops or indefinite recursive fan-outs in your orchestrators. Those
can get expensive if you leave them out of control.</p>
<h2 id="error-handling-and-retries">Error-handling and retries</h2>
<p>What happens when an error occurs somewhere in the middle of the workflow? For instance, a third-party
flight booking service might not be able to process the request:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/error-handling.png" alt="Error Handling"></p>
<center class="img-caption">One activity is unhealthy</center>

<p>This situation is expected by Durable Functions. Instead of silently failing, the activity function
sends a message containing the information about the error back to the orchestrator.</p>
<p>The orchestrator deserializes the error details and, at the time of replay, throws a .NET exception
from the corresponding call. The developer is free to put a <code>try .. catch</code> block around the call
and handle the exception:</p>
<pre><code class="language-csharp">[FunctionName(&quot;SequentialWorkflow&quot;)]
public static async Task Sequential([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var conf = await context.CallActivityAsync&lt;ConfTicket&gt;(&quot;BookConference&quot;, &quot;ServerlessDays&quot;);
    try
    {
        var itinerary = MakeItinerary(/* ... */);
        await context.CallActivityAsync(&quot;BookFlight&quot;, itinerary);
    }
    catch (FunctionFailedException)
    {
        var alternativeItinerary = MakeAnotherItinerary(/* ... */);
        await context.CallActivityAsync(&quot;BookFlight&quot;, alternativeItinerary);
    }
    await context.CallActivityAsync(&quot;BookHotel&quot;, flight.Dates);
}</code></pre>
<p>The code above falls back to a &quot;backup plan&quot; of booking another itinerary. Another typical pattern
would be to run a compensating activity to cancel the effects of any previous actions (un-book the
conference in our case) and leave the system in a clean state.</p>
<p>Quite often, the error might be transient, so it might make sense to retry the failed operation
after a pause. It&#39;s a such a common scenario that Durable Functions provides a dedicated API:</p>
<pre><code class="language-csharp">var options = new RetryOptions(
    firstRetryInterval: TimeSpan.FromMinutes(1),                    
    maxNumberOfAttempts: 5);
options.BackoffCoefficient = 2.0;

await context.CallActivityWithRetryAsync(&quot;BookFlight&quot;, options, itinerary);</code></pre>
<p>The above code instructs the library to</p>
<ul>
<li>Retry up to 5 times</li>
<li>Wait for 1 minute before the first retry</li>
<li>Increase delays before every subsequent retry by the factor of 2 (1 min, 2 min, 4 min, etc.)</li>
</ul>
<p>The significant point is that, once again, the orchestrator does not block while
awaiting retries. After a failed call, a message is scheduled for the moment in the future
to re-run the orchestrator and retry the call.</p>
<h2 id="sub-orchestrators">Sub-orchestrators</h2>
<p>Business processes may consist of numerous steps. To keep the code of orchestrators manageable,
Durable Functions allows nested orchestrators. A &quot;parent&quot; orchestrator can call out to child
orchestrators via the <code>context.CallSubOrchestratorAsync</code> method:</p>
<pre><code class="language-csharp">[FunctionName(&quot;CombinedOrchestrator&quot;)]
public static async Task CombinedOrchestrator([OrchestrationTrigger] DurableOrchestrationContext context)
{
    await context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysAmsterdam);
    await context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysHamburg);
}</code></pre>
<p>The code above books two conferences, one after the other.</p>
<h2 id="fan-out-fan-in">Fan-out / Fan-in</h2>
<p>What if we want to run multiple activities in parallel?</p>
<p>For instance, in the example above, we could wish to book two conferences, but the
booking order might not matter. Still, when both bookings are completed, we want to combine the results
to produce an expense report for the finance department:</p>
<p><img src="/2018/12/making-sense-of-azure-durable-functions/parallel-calls.png" alt="Parallel Calls"></p>
<center class="img-caption">Parallel calls followed by a final step</center>

<p>In this scenario, the <code>BookTrip</code> orchestrator accepts an input parameter with the name of the 
conference and returns the expense information. <code>ReportExpenses</code> needs to receive both
expenses combined.</p>
<p>This goal can be easily achieved by scheduling two tasks (i.e., sending two messages) without
awaiting them separately. We use the familiar <code>Task.WhenAll</code> method to await both and combine
the results:</p>
<pre><code class="language-csharp">[FunctionName(&quot;ParallelWorkflow&quot;)]
public static async Task Parallel([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var amsterdam = context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysAmsterdam);
    var hamburg   = context.CallSubOrchestratorAsync(&quot;BookTrip&quot;, serverlessDaysHamburg);

    var expenses = await Task.WhenAll(amsterdam, hamburg);

    await context.CallActivityAsync(&quot;ReportExpenses&quot;, expenses);
}</code></pre>
<p>Remember that awaiting the <code>WhenAll</code> method doesn&#39;t synchronously block the orchestrator. It quits
the first time and then restarts two times on reply messages received from activities.
The first restart quits again, and only the second restart makes it past the <code>await</code>.</p>
<p><code>Task.WhenAll</code> returns an array of results (one result per each input task), which is then
passed to the reporting activity.</p>
<p>Another example of parallelization could be a workflow sending e-mails to hundreds of
recipients. Such fan-out wouldn&#39;t be hard with normal queue-triggered functions: simply send
hundreds of messages. However, combining the results, if required for the next step
of the workflow, is quite challenging.</p>
<p>It&#39;s straightforward with a durable orchestrator:</p>
<pre><code class="language-csharp">var emailSendingTasks =
    recepients
    .Select(to =&gt; context.CallActivityAsync&lt;bool&gt;(&quot;SendEmail&quot;, to))
    .ToArray();

var results = await Task.WhenAll(emailSendingTasks);

if (results.All(r =&gt; r)) { /* ... */ }</code></pre>
<p>Making hundreds of roundtrips to activities and back could cause numerous replays
of the orchestrator. As an optimization, if multiple activity functions complete around the same 
time, the orchestrator may internally process several messages as a batch and restart 
the orchestrator function only once per batch.</p>
<h2 id="other-concepts">Other Concepts</h2>
<p>There are many more patterns enabled by Durable Functions. Here is a quick list to give you some perspective:</p>
<ul>
<li>Waiting for the <em>first</em> completed task in a collection (rather than <em>all</em> of them) using the <code>Task.WhenAny</code>
method. Useful for scenarios like timeouts or competing actions.</li>
<li>Pausing the workflow for a given period or until a deadline.</li>
<li>Waiting for external events, e.g., bringing human interaction into the workflow.</li>
<li>Running recurring workflows, when the flow repeats until a certain condition is met.</li>
</ul>
<p>Further explanation and code samples are in 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">the docs</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I firmly believe that serverless applications utilizing a broad range of managed cloud services
are highly beneficial to many companies, due to both rapid development process and
the properly aligned billing model.</p>
<p>Serverless tech is still young; more high-level architectural patterns need to emerge
to enable expressive and composable implementations of large business systems.</p>
<p>Azure Durable Functions suggests some of the possible answers. It combines the clarity and readability
of sequential RPC-style code with the power and resilience of event-driven architecture.</p>
<p><a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">The documentation</a>
for Durable Functions is excellent, with plenty of examples and how-to guides.
Learn it, try it for your real-life scenarios, and let me know your opinion&mdash;I&#39;m
excited about the serverless future!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Many thanks to <a href="https://twitter.com/kashimizMSFT">Katy Shimizu</a>, <a href="https://twitter.com/cgillum">Chris Gillum</a>,
<a href="https://twitter.com/efleming18">Eric Fleming</a>, <a href="https://twitter.com/KevinJonesD">KJ Jones</a>,
<a href="https://twitter.com/William_DotNet">William Liebenberg</a>, <a href="https://twitter.com/ATosato86">Andrea Tosato</a>
for reviewing the draft of this article and their valuable contributions and suggestions. The community 
around Azure Functions and Durable Functions is superb!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/workflows/'>Workflows</a>, <a href='/tags/azure-durable-functions/'>Azure Durable Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Nov 19th, 2018</div>
    
    <h1><a href='/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/'>From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a></h1>
    

    
        <div class="remark">Originally published at <a href='https://blog.binaris.com/from-0-to-1000-instances/'>Binaris Blog</a></div>
    

    <div class="post-content">
        <p>Whenever I see a &quot;Getting Started with Function-as-a-Service&quot; tutorial, it usually shows off 
a synchronous HTTP-triggered scenario. In my projects, though, I use a lot of asynchronous 
functions triggered by a queue or an event stream.</p>
<p>Quite often, the number of messages passing through a queue isn&#39;t uniform over time. I might 
drop batches of work now and then. My app may get piles of queue items arriving from upstream 
systems that were down or under maintenance for an extended period. The system might see some 
rush-hour peaks every day or only a few busy days per month.</p>
<p>This is where serverless tech shines: You pay per execution, and then the promise is that the 
provider takes care of scaling up or down for you. Today, I want to put this scalability under 
test.</p>
<p>The goal of this article is to explore queue-triggered serverless functions and hopefully distill 
some practical advice regarding asynchronous functions for real projects. I will be evaluating 
the problem:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different types of workloads</li>
<li>For different performance tiers</li>
</ul>
<p>Let&#39;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some crucial factors and parameters 
that influence the outcome. My interpretation might be wrong. The results might change over time. 
If you happen to know a way to improve my tests, please let me know, and I will re-run them and 
re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>In this article I analyze the execution results of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via SQS queues</li>
<li>Azure Function triggered via Storage queues</li>
<li>Google Cloud Function triggered via Cloud Pub/Sub</li>
</ul>
<p>All functions are implemented in Javascript and are running on GA runtime.</p>
<p>At the beginning of each test, I threw 100,000 messages into a queue that was previously idle. 
Enqueuing never took longer than one minute (I sent the messages from multiple clients in 
parallel).</p>
<p>I disabled any batch processing, so each message was consumed by a separate function invocation.</p>
<p>I then analyzed the logs (AWS CloudWatch, Azure Application Insights, and GCP Stackdriver 
Logging) to generate charts of execution distribution over time.</p>
<h2 id="how-scaling-actually-works">How Scaling Actually Works</h2>
<p>To understand the experiment better, let&#39;s look at a very simplistic but still useful model of 
how cloud providers scale serverless applications.</p>
<p>All providers handle the increased load by 
<a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">scaling out</a>, i.e., 
by creating multiple instances of the same application that execute the chunks of work in 
parallel.</p>
<p>In theory, a cloud provider could spin up an instance for each message in the queue as soon as 
the messages arrive. The backlog processing time would then stay very close to zero.</p>
<p>In practice, allocating instances is not cheap. The Cloud provider has to boot up the function 
runtime, hit a <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">cold start</a>, and waste 
expensive resources on a job that potentially will take just a few milliseconds.</p>
<p>So the providers are trying to find a sweet spot between handling the work as soon as possible 
and using resources efficiently. The outcomes differ, which is the point of my article.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda defines scale out with a notion of Concurrent Executions. Each instance of your AWS 
Lambda is handling a single execution at any given time. In our case, it&#39;s processing a single 
SQS message.</p>
<p>It&#39;s helpful to think of a function instance as a container working on a single task. If execution 
pauses or waits for an external I/O operation, the instance is on hold.</p>
<p>The model of concurrent executions is universal to all trigger types supported by Lambdas. An 
instance doesn&#39;t work with event sources directly; it just receives an event to work on.</p>
<p>There is a central element in the system, let&#39;s call it &quot;Orchestrator&quot;. The Orchestrator is the 
component talking to an SQS queue and getting the messages from it. It&#39;s then the job of the 
Orchestrator and related infrastructure to provision the required number of instances for working 
on concurrent executions:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-queue-scaling.png" alt="Model of AWS Lambda Scale-Out"></p>
<center class="img-caption">Model of AWS Lambda Scale-Out</center>

<p>As to scaling behavior, here is what the official 
<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/scaling.html">AWS docs</a> say:</p>
<blockquote>
<p>AWS Lambda automatically scales up ... until the number of concurrent function executions 
reaches 1000 ... Amazon Simple Queue Service supports an initial burst of 5 concurrent function 
invocations and increases concurrency by 60 concurrent invocations per minute.</p>
</blockquote>
<h3 id="gcp">GCP</h3>
<p>The model of Google Cloud Functions is very similar to what AWS does. It runs a single 
simultaneous execution per instance and routes the messages centrally.</p>
<p>I wasn&#39;t able to find any scaling specifics except the definition of 
<a href="https://cloud.google.com/functions/quotas">Function Quotas</a>.</p>
<h3 id="azure">Azure</h3>
<p>Experiments with Azure Functions were run on 
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
&mdash;the dynamically scaled and billed-per-execution runtime. The concurrency model of Azure Functions 
is different from the counterparts of AWS/GCP.</p>
<p>Function App instance is closer to a VM than a single-task container. It runs multiple concurrent 
executions in parallel. Equally importantly, it pulls messages from the queue on its own instead of 
getting them pushed from a central Orchestrator.</p>
<p>There is still a central coordinator called Scale Controller, but its role is a bit more subtle. It 
connects to the same data source (the queue) and needs to determine how many instances to provision 
based on the metrics from that queue:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-scaling.png" alt="Model of Azure Function Scale-Out"></p>
<center class="img-caption">Model of Azure Function Scale-Out</center>

<p>This model has pros and cons. If one execution is idle, waiting for some I/O operation such as an 
HTTP request to finish, the instance might become busy processing other messages, thus being more 
efficient. Running multiple executions is useful in terms of shared resource utilization, e.g., 
keeping database connection pools and reusing HTTP connections.</p>
<p>On the flip side, the Scale Controller now needs to be smarter: to know not only the queue backlog 
but also how instances are doing and at what pace they are processing the messages. It&#39;s probably 
achievable based on queue telemetry though.</p>
<p>Let&#39;s start applying this knowledge in practical experiments.</p>
<h2 id="pause-the-world-workload">Pause-the-World Workload</h2>
<p>My first serverless function is aimed to simulate I/O-bound workloads without using external 
dependencies to keep the experiment clean. Therefore, the implementation is extremely 
straightforward: pause for 500 ms and return.</p>
<p>It could be loading data from a scalable third-party API. It could be running a database query. 
Instead, it just runs <code>setTimeout</code>.</p>
<p>I sent 100k messages to queues of all three cloud providers and observed the result.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda allows multiple instance sizes to be provisioned. Since the workload is neither CPU- 
nor memory-intensive, I was using the smallest memory allocation of 128 MB.</p>
<p>Here comes the first chart of many, so let&#39;s learn to read it. The horizontal axis shows time in 
minutes since all the messages were sent to the queue.</p>
<p>The line going from top-left to bottom-right shows the decreasing queue backlog. Accordingly, the 
left vertical axis denotes the number of items still-to-be-handled.</p>
<p>The bars show the number of concurrent executions crunching the messages at a given time. Every 
execution logs the instance ID so that I could derive the instance count from the logs. The right 
vertical axis shows the instance number.</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-iobound-scaling.png" alt="AWS Lambda processing 100k SQS messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">AWS Lambda processing 100k SQS messages with "Pause" handler</center>

<p>It took AWS Lambda 5.5 minutes to process the whole batch of 100k messages. For comparison, the 
same batch processed sequentially would take about 14 hours.</p>
<p>Notice how linear the growth of instance count is. If I apply the official scaling formula:</p>
<pre><code>Instance Count = 5 + Minutes * 60 = 5 + 5.5 * 60  = 335</code></pre><p>We get a very close result! Promises kept.</p>
<h3 id="gcp">GCP</h3>
<p>Same function, same chart, same instance size of 128 MB of RAM&mdash;but this time for Google Cloud Functions:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-iobound-scaling.png" alt="Google Cloud Function processing 100k Pub/Sub messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">Google Cloud Function processing 100k Pub/Sub messages with "Pause" handler</center>

<p>Coincidentally, the total amount of instances, in the end, was very close to AWS. The scaling
pattern looks entirely different though: Within the very first minute, there was a burst of 
scaling close to 300 instances, and then the growth got very modest.</p>
<p>Thanks to this initial jump, GCP managed to finish processing almost one minute earlier than AWS.</p>
<h3 id="azure">Azure</h3>
<p>Azure Function doesn&#39;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>The shape of the chart for Azure Functions is very similar, but the instance number growth is 
significantly different:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-iobound-scaling.png" alt="Azure Function processing 100k queue messages with &quot;Pause&quot; handler"></p>
<center class="img-caption">Azure Function processing 100k queue messages with "Pause" handler</center>

<p>The total processing time was a bit faster than AWS and somewhat slower than GCP. Azure Function 
instances process several messages in parallel, so it takes much less of them to do the same amount 
of work.</p>
<p>Instance number growth seems far more linear than bursty.</p>
<h3 id="what-we-learned">What we learned</h3>
<p>Based on this simple test, it&#39;s hard to say if one cloud provider handles scale-out better than 
the others.</p>
<p>It looks like all serverless platforms under stress are making decisions at the resolution of 5-15 
seconds, so the backlog processing delays are likely to be measured in minutes. It sounds quite far 
from the theoretical &quot;close to zero&quot; target but is most likely good enough for the majority of 
applications.</p>
<h2 id="crunching-numbers">Crunching Numbers</h2>
<p>That was an easy job though. Let&#39;s give cloud providers a hard time by executing CPU-heavy workloads 
and see if they survive!</p>
<p>This time, each message handler calculates a <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hash with a cost of 10. One such calculation takes about 200 ms on my laptop.</p>
<h3 id="aws">AWS</h3>
<p>Once again, I sent 100k messages to an SQS queue and recorded the processing speed and instance count.</p>
<p>Since the workload is CPU-bound, and AWS allocates CPU shares proportionally to the allocated 
memory, the instance size might have a significant influence on the result.</p>
<p>I started with the smallest memory allocation of 128 MB:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-cpubound-scaling.png" alt="AWS Lambda (128 MB) processing 100k SQS messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">AWS Lambda (128 MB) processing 100k SQS messages with "Bcrypt" handler</center>

<p>This time it took almost 10 minutes to complete the experiment.</p>
<p>The scaling shape is pretty much the same as last time, still correctly described by the formula 
<code>60 * Minutes + 5</code>. However, because AWS allocates a small fraction of a full CPU to each 128 MB 
execution, one message takes around 1,700 ms to complete. Thus, the total work increased 
approximately by the factor of 3 (47 hours if done sequentially).</p>
<p>At the peak, 612 concurrent executions were running, nearly double the amount in our initial 
experiment. So, the total processing time increased only by the factor of 2&mdash;up to 10 minutes.</p>
<p>Let&#39;s see if larger Lambda instances would improve the outcome. Here is the chart for 512 MB of 
allocated memory:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//aws-lambda-sqs-cpubound-scaling-512.png" alt="AWS Lambda (512 MB) processing 100k SQS messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">AWS Lambda (512 MB) processing 100k SQS messages with "Bcrypt" handler</center>

<p>And yes it does. The average execution duration is down to 400 ms: 4 times less, as expected. 
The scaling shape still holds, so the entire batch was done in less than four minutes.</p>
<h3 id="gcp">GCP</h3>
<p>I executed the same experiment on Google Cloud Functions. I started with 128 MB, and it looks 
impressive:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling.png" alt="Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (128 MB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>The average execution duration is very close to Amazon&#39;s: 1,600 ms. However, GCP scaled more 
aggressively&mdash;to a staggering 1,169 parallel executions! Scaling also has a different shape: 
It&#39;s not linear but grows in steep jumps. As a result, it took less than six minutes on the 
lowest CPU profile&mdash;very close to AWS&#39;s time on a 4x more powerful CPU.</p>
<p>What will GCP achieve on a faster CPU? Let&#39;s provision 512 MB. It must absolutely crush the 
test. Umm, wait, look at that:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling-512.png" alt="Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (512 MB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>It actually... got slower. Yes, the average execution time is 4x lower: 400 ms, but the scaling 
got much less aggressive too, which canceled the speedup.</p>
<p>I confirmed it with the largest instance size of 2,048 MB:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//gcp-cloud-function-pubsub-cpubound-scaling-2048.png" alt="Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Google Cloud Function (2 GB) processing 100k Pub/Sub messages with "Bcrypt" handler</center>

<p>CPU is fast: 160 ms average execution time, but the total time to process 100k messages went up 
to eight minutes. Beyond the initial spike at the first minute, it failed to scale up any further 
and stayed at about 110 concurrent executions.</p>
<p>It seems that GCP is not that keen to scale out larger instances. It&#39;s probably easier to find 
many small instances available on the pool rather than a similar number of giant instances.</p>
<h3 id="azure">Azure</h3>
<p>A single invocation takes about 400 ms to complete on Azure Function. Here is the burndown chart:</p>
<p><img src="/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing//azure-function-queue-cpubound-scaling.png" alt="Azure Function processing 100k queue messages with &quot;Bcrypt&quot; handler"></p>
<center class="img-caption">Azure Function processing 100k queue messages with "Bcrypt" handler</center>

<p>Azure spent 21 minutes to process the whole backlog. The scaling was linear, similarly to AWS, 
but with a much slower pace regarding instance size growth, about <code>2.5 * Minutes</code>.</p>
<p>As a reminder, each instance could process multiple queue messages in parallel, but each such 
execution would be competing for the same CPU resource, which doesn&#39;t help for the purely 
CPU-bound workload.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<p>Time for some conclusions and pieces of advice to apply in real serverless applications.</p>
<h3 id="serverless-is-great-for-async-data-processing">Serverless is great for async data processing</h3>
<p>If you are already using cloud services, such as managed queues and topics, serverless functions 
are the easiest way to consume them. </p>
<p>Moreover, the scalability is there too. When was the last time you ran 1,200 copies of your 
application?</p>
<h3 id="serverless-is-not-infinitely-scalable">Serverless is not infinitely scalable</h3>
<p>There are limits. Your functions won&#39;t scale perfectly to accommodate your spike&mdash;a 
provider-specific algorithm will determine the scaling pattern.</p>
<p>If you have large spikes in queue workloads, which is quite likely for medium- to high-load 
scenarios, you can and should expect delays up to several minutes before the backlog is fully 
digested.</p>
<p>All cloud providers have quotas and limits that define an upper boundary of scalability.</p>
<h3 id="cloud-providers-have-different-implementations">Cloud providers have different implementations</h3>
<p><strong>AWS Lambda</strong> seems to have a very consistent and well-documented linear scale growth for 
SQS-triggered Lambda functions. It will happily scale to 1,000 instances, or whatever other 
limit you hit first.</p>
<p><strong>Google Cloud Functions</strong> has the most aggressive scale-out strategy for the smallest instance 
sizes. It can be a cost-efficient and scalable way to run your queue-based workloads. Larger 
instances seem to scale in a more limited way, so a further investigation is required if you 
need those.</p>
<p><strong>Azure Functions</strong> share instances for multiple concurrent executions, which works better 
for I/O-bound workloads than for CPU-bound ones. Depending on the exact scenario that you 
have, it might help to play with instance-level settings.</p>
<h3 id="don-t-forget-batching">Don&#39;t forget batching</h3>
<p>For the tests, I was handling queue messages in the 1-by-1 fashion. In practice, it helps 
if you can batch several messages together and execute a single action for all of them in one go.</p>
<p>If the destination for your data supports batched operations, the throughput will usually 
increase immensely. 
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2017/09/19/processing-100000-events-per-second-on-azure-functions/">Processing 100,000 Events Per Second on Azure Functions</a>
is an excellent case to prove the point.</p>
<h3 id="you-might-get-too-much-scale">You might get too much scale</h3>
<p>A month ago, Troy Hunt published a great post 
<a href="https://www.troyhunt.com/breaking-azure-functions-with-too-many-connections/">Breaking Azure Functions with Too Many Connections</a>. 
His scenario looks very familiar: He uses queue-triggered Azure Functions to notify 
subscribers about data breaches. One day, he dropped 126 million items into the queue, and Azure 
scaled out, which overloaded Mozilla&#39;s servers and caused them to go all-timeouts.</p>
<p>Another consideration is that non-serverless dependencies limit the scalability of your serverless 
application. If you call a legacy HTTP endpoint, a SQL database, or a third-party web service&mdash;be
sure to test how they react when your serverless function scales out to hundreds of concurrent 
executions.</p>
<p>Stay tuned for more serverless performance goodness!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/scalability/'>Scalability</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/gcp/'>GCP</a>, <a href='/tags/google-cloud-functions/'>Google Cloud Functions</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Oct 10th, 2018</div>
    
    <h1><a href='/2018/10/azure-functions-v2-released-how-performant-is-it/'>Azure Functions V2 Is Released, How Performant Is It?</a></h1>
    

    

    <div class="post-content">
        <p>Azure Functions major version 2.0 was released into GA a few days back during Microsoft Ignite. The runtime is now
based on .NET Core and thus is cross-platform and more interoperable. It has a nice extensibility story too.</p>
<p>In theory, .NET Core runtime is more lean and performant.
But last time <a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">I checked back in April</a>,
the preview version of Azure Functions V2 had some serious issues with cold start durations.</p>
<p>I decided to give the new and shiny version another try and ran several benchmarks. All tests were conducted on 
Consumption plan.</p>
<p><strong>TL;DR</strong>: it&#39;s not perfect just yet.</p>
<h2 id="cold-starts">Cold Starts</h2>
<p>Cold starts happen when a new instance handles its first request, see my other posts:
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">one</a>,
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">two</a>,
<a href="https://mikhail.io/2018/08/serverless-cold-start-war/">three</a>.</p>
<h3 id="hello-world">Hello World</h3>
<p>The following chart gives a comparison of V1 vs V2 cold starts for the two most popular runtimes:
.NET and Javascript. The dark bar shows the most probable range of values, while the light ones
are possible but less frequent:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-dotnet-js.png" alt="Cold Starts V1 vs V2: .NET and Javascript"></p>
<p>Apparently, V2 is slower to start for both runtimes. V2 on .NET is slower by 10% on average and seems 
to have higher variation. V2 on Javascript is massively slower: 2 times on average, and the slowest startup
time goes above 10 seconds.</p>
<h3 id="dependencies-on-board">Dependencies On Board</h3>
<p>The values for the previous chart were calculated for Hello-World type of functions with no extra dependencies.</p>
<p>The chart below shows two more Javascript functions, this time with a decent number of dependencies:</p>
<ul>
<li>Referencing 3 NPM packages - 5MB zipped</li>
<li>Referencing 38 NPM packages - 35 MB zipped</li>
</ul>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-js-dependencies.png" alt="Cold Starts V1 vs V2: Javascript with NPM dependencies"></p>
<p>V2 clearly loses on both samples, but V2-V1 difference seems to be consistently within 2.5-3
seconds for any amount of dependencies.</p>
<p>All the functions were deployed with the Run-from-Package method which promises faster startup times.</p>
<h3 id="java">Java</h3>
<p>Functions V2 come with a preview of a new runtime: Java / JVM. It utilizes the same extensibility model 
as Javascript, and thus it seems to be a first-class citizen now.</p>
<p>Cold starts are not first-class though: </p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-java.png" alt="Cold Starts Java"></p>
<p>If you are a Java developer, be prepared for 20-25 seconds of initial startup time. That will probably 
be resolved when the Java runtime becomes generally available:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">That matches some of our internal data. We are looking into it.</p>&mdash; Paul Batum (@paulbatum) <a href="https://twitter.com/paulbatum/status/1048391445386735616?ref_src=twsrc%5Etfw">October 6, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="queue-processor">Queue Processor</h2>
<p>Cold starts are most problematic for synchronous triggers like HTTP requests. They are less relevant
for queue-based workloads, where scale out is of higher importance.</p>
<p>Last year I ran some tests around the ability of Functions to keep up with variable queue load:
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">one</a>,
<a href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/">two</a>.</p>
<p>Today I ran two simple tests to compare the scalability of V1 vs. V2 runtimes.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>In my first tests, a lightweight Javascript Function processed messages from an Azure Storage Queue. For
each message, it just pauses for 500 msec and then completes. This is supposed to simulate I/O-bound 
Functions.</p>
<p>I&#39;ve sent 100,000 messages to the queue and measured how fast they went away. Batch size (degree of parallelism
on each instance) was set to 16.</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/queue-scaling-io-based.png" alt="Processing Queue Messages with Lightweight I/O Workload"></p>
<p>Two lines show the queue backlogs of two runtimes, while the bars indicate the number of instances working
in parallel at a given minute.</p>
<p>We see that V2 was a bit faster to complete, probably due to more instances provisioned to it at any moment.
The difference is not big though and might be statistically insignificant.</p>
<h3 id="cpu-at-work">CPU at Work</h3>
<p>Functions in my second experiment are CPU-bound. Each message invokes calculation of a 10-stage Bcrypt
hash. On a very quiet moment, 1 such function call takes about 300-400 ms to complete, consuming 100% CPU 
load on a single core.</p>
<p>Both Functions are precompiled .NET and both are using <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.NET</a>.</p>
<p>Batch size (degree of parallelism on each instance) was set to 2 to avoid too much fighting for the same CPU. Yet, 
the average call duration is about 1.5 seconds (3x slower than possible).</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/queue-scaling-cpu-bound.png" alt="Processing Queue Messages with CPU-bound Workload"></p>
<p>The first thing to notice: it&#39;s the same number of messages with comparable &quot;sequential&quot; execution time, but 
the total time to complete the job increased 3-fold. That&#39;s because the workload is much more demanding to
the resources of application instances, and they struggle to parallelize work more aggressively.</p>
<p>V1 and V2 are again close to each other. One more time, V2 got more instances allocated to it most of the time.
And yet, it seemed to be <em>consistently</em> slower and lost about 2.5 minutes on 25 minutes interval (~10%).</p>
<h2 id="http-scalability">HTTP Scalability</h2>
<p>I ran two similar Functions &mdash; I/O-bound &quot;Pause&quot; (~100 ms) and CPU-bound Bcrypt (9 stages, ~150ms) &mdash; under a stress test.
But this time they were triggered by HTTP requests. Then I compared the results for V1 and V2.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>The grey bars on the following charts represent the rate of requests sent and processed within a given minute.</p>
<p>The lines are percentiles of response time: green lines for V2 and orange lines for V1.</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-io-based.png" alt="Processing HTTP Requests with Lightweight I/O Workload"></p>
<p>Yes, you saw it right, my Azure Functions were processing 100,000 messages per minute at peak. That&#39;s a lot of
messages.</p>
<p>Apart from the initial spike at minutes 2 and 3, both versions performed pretty close to each other.</p>
<p>50th percentile is flat close to the theoretic minimum of 100 ms, while the 95th percentile fluctuates a bit, but still
mostly stays quite low. </p>
<p>Note that the response time is measured from the client perspective, not by looking at the statistics provided by Azure.</p>
<h3 id="cpu-fanout">CPU Fanout</h3>
<p>How did CPU-heavy workload perform?</p>
<p>To skip ahead, I must say that the response time increased much more significantly, so my sample clients were
not able to generate request rates of 100k per minute. They &quot;only&quot; did about 48k per minute at peak, which still
seems massive to me.</p>
<p>I&#39;ve run the same test twice: one for Bcrypt implemented in .NET, and one for Javascript.</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-dotnet.png" alt="Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>V2 had a real struggle during the first minute, where response time got terribly slow up to 9 seconds.</p>
<p>Looking at the bold-green 50th percentile, we can see that it&#39;s consistently higher than the orange one throughout
the load growth period of the first 10 minutes. V2 seemed to have a harder time to adjust.</p>
<p>This might be explainable by slower growth of instance count:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-dotnet-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>This difference could be totally random, so let&#39;s look at a similar test with Javascript worker. Here is the percentile chart:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-js.png" alt="Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>The original slowness of the first 3 minutes is still there, but after that time V2 and V1 are on-par.</p>
<p>On-par doesn&#39;t sound that great though if you look at the significant edge in the number of allocated instances, in
favor of V2 this time:</p>
<p><img src="/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-js-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>Massive 147 instances were crunching Bcrypt hashes in Javascript V2, and that made it a bit faster to respond than V1.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As always, be reluctant to make definite conclusions based on simplistic benchmarks. But I see some trends which might
be true as of today:</p>
<ul>
<li>Performance of .NET Functions is comparable across two versions of Functions runtimes;</li>
<li>V1 still has a clear edge in the cold start time of Javascript Functions;</li>
<li>V2 is the only option for Java developers, but be prepared to very slow cold starts;</li>
<li>Scale-out characteristics seem to be independent of the runtime version, although there are blurry signs of
V2 being a bit slower to ramp up or slightly more resource hungry.</li>
</ul>
<p>I hope this helps in your serverless journey!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/cold-start/'>Cold Start</a>
    </div>
    
</article>

    <article class="post">
    <div class="post-date">Aug 30th, 2018</div>
    
    <h1><a href='/2018/08/serverless-cold-start-war/'>Serverless: Cold Start War</a></h1>
    

    

    <div class="post-content">
        <p>Serverless cloud services are hot. Except when they are not :)</p>
<p>AWS Lambda, Azure Functions, Google Cloud Functions are all similar in their attempt
to enable rapid development of cloud-native serverless applications.</p>
<p>Auto-provisioning and auto-scalability are the killer features of those Function-as-a-Service
cloud offerings. No management required, cloud providers will deliver infrastructure for the user
based on the actual incoming load.</p>
<p>One drawback of such dynamic provisioning is a phenomenon called &quot;cold start&quot;. Basically,
applications that haven&#39;t been used for a while take longer to startup and to handle the
first request.</p>
<p>Cloud providers keep a bunch of generic unspecialized workers in stock. Whenever a serverless
application needs to scale up, be it from 0 to 1 instances, or from N to N+1 likewise, the runtime
will pick one of the spare workers and will configure it to serve the named application:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart.png" alt="Cold Start"></p>
<p>This procedure takes time, so the latency of the application event handling increases. To avoid
doing this for every event, the specialized worker will be kept intact for some period of time.
When another event comes in, this worker will stand available to process it as soon as possible.
This is a &quot;warm start&quot;:</p>
<p><img src="/2018/08/serverless-cold-start-war//warmstart.png" alt="Warm Start"></p>
<p>The problem of cold start latency was described multiple times, here are the notable links:</p>
<ul>
<li><a href="https://blogs.msdn.microsoft.com/appserviceteam/2018/02/07/understanding-serverless-cold-start/">Understanding Serverless Cold Start</a></li>
<li><a href="https://hackernoon.com/cold-starts-in-aws-lambda-f9e3432adbf0">Everything you need to know about cold starts in AWS Lambda</a></li>
<li><a href="https://serverless.com/blog/keep-your-lambdas-warm/">Keeping Functions Warm</a></li>
<li><a href="https://theburningmonk.com/2018/01/im-afraid-youre-thinking-about-aws-lambda-cold-starts-all-wrong/">I&#39;m afraid you&#39;re thinking about AWS Lambda cold starts all wrong</a></li>
</ul>
<p>The goal of my article today is to explore how cold starts compare:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different languages and runtimes</li>
<li>For smaller vs larger applications (including dependencies)</li>
<li>How often cold starts happen</li>
<li>What can be done to optimize the cold starts</li>
</ul>
<p>Let&#39;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some important factors and parameters that
influence the outcome. My interpretation might be wrong. The results might change over time. If you happen 
to know a way to improve my tests, please let me know and I will re-run them and re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>All tests were run against HTTP Functions because that&#39;s where cold start matters the most. </p>
<p>All the functions were returning a simple JSON reporting their current instance ID, language etc.
Some functions were also loading extra dependencies, see below.</p>
<p>I did not rely on execution time reported by a cloud provider. Instead, I measured end-to-end duration from
the client perspective. This means that durations of HTTP gateway (e.g. API Gateway in case of AWS) are included
into the total duration. However, all calls were made from within the same region, so network latency should 
have minimal impact:</p>
<p><img src="/2018/08/serverless-cold-start-war//test-setup.png" alt="Test Setup"></p>
<p>Important note: I ran all my tests on GA (generally available) versions of services/languages, so e.g.
Azure tests were done with version 1 of Functions runtime (.NET Framework), and GCP tests were only made for
Javascript runtime.</p>
<h2 id="when-does-cold-start-happen-">When Does Cold Start Happen?</h2>
<p>Obviously, cold start happens when the very first request comes in. After that request is processed,
the instance is kept alive in case subsequent requests arrive. But for how long?</p>
<p>The answer differs between cloud providers.</p>
<p>To help you read the charts in this section, I&#39;ve marked cold starts with blue color dots, and warm starts
with orange color dots.</p>
<h3 id="azure">Azure</h3>
<p>Here is the chart for Azure. It shows the values of normalized request durations across
different languages and runtime versions (Y-axis) depending on the time since the previous
request in minutes (X-axis):</p>
<p><img src="/2018/08/serverless-cold-start-war//azure-coldstart-threshold.png" alt="Azure Cold Start Threshold"></p>
<p>Clearly, an idle instance lives for 20 minutes and then gets recycled. All requests after 20 minutes
threshold hit another cold start.</p>
<h3 id="aws">AWS</h3>
<p>AWS is more tricky. Here is the same kind of chart, relative durations vs time since the last request, 
measured for AWS Lambda:</p>
<p><img src="/2018/08/serverless-cold-start-war//aws-coldstart-threshold.png" alt="AWS Cold Start vs Warm Start"></p>
<p>There&#39;s no clear threshold here... For this sample, no cold starts happened within 28 minutes after the previous 
invocation. Afterward, the frequency of cold starts slowly rises. But even after 1 hour of inactivity, there&#39;s still a
good chance that your instance is alive and ready to take requests.</p>
<p>This doesn&#39;t match the official information that AWS Lambdas stay alive for just 5 minutes after the last
invocation. I reached out to Chris Munns, and he confirmed:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">
So what you are seeing is very much possible as the team plays with certain knobs/levers for execution environment lifecycle. 
let me know if you have concerns about it, but it should be just fine</p>&mdash; chrismunns (@chrismunns) 
<a href="https://twitter.com/chrismunns/status/1021452964630851585?ref_src=twsrc%5Etfw">July 23, 2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>A couple learning points here:</p>
<ul>
<li>AWS is working on improving cold start experience (and probably Azure/GCP do too)</li>
<li>My results might not be reliably reproducible in your application since it&#39;s affected by recent adjustments</li>
</ul>
<h3 id="gcp">GCP</h3>
<p>Google Cloud Functions left me completely puzzled. Here is the same chart for GCP cold starts (again,
orange dots are warm and blue ones are cold):</p>
<p><img src="/2018/08/serverless-cold-start-war//gcp-coldstart-threshold.png" alt="GCP Cold Start vs Warm Start"></p>
<p>This looks totally random to me. A cold start can happen in 3 minutes after the previous request, or an instance
can be kept alive for the whole hour. The probability of a cold start doesn&#39;t seem to depend on the interval,
at least just by looking at this chart.</p>
<p>Any ideas about what&#39;s going on are welcome!</p>
<h3 id="parallel-requests">Parallel requests</h3>
<p>Cold starts happen not only when the first instance of an application is provisioned. The same issue will happen whenever
all the provisioned instances are busy handling incoming events, and yet another event comes in (at scale out).</p>
<p>As far as I&#39;m aware, this behavior is common to all 3 providers, so I haven&#39;t prepared any comparison charts
for N+1 cold starts. Yet, be aware of them!</p>
<h2 id="reading-candle-charts">Reading Candle Charts</h2>
<p>In the following sections, you will see charts that represent statistical distribution of cold start time as
measured during my experiments. I repeated experiments multiple times and then grouped the metric values, e.g.
by the cloud provider or by language.</p>
<p>Each group will be represented by a &quot;candle&quot; on the chart. This is how you should read each candle:</p>
<p><img src="/2018/08/serverless-cold-start-war//sample-coldstart-chart.png" alt="How to Read Cold Start Charts"></p>
<h2 id="memory-allocation">Memory Allocation</h2>
<p>AWS Lambda and Google Cloud Functions have a setting to define the memory size that gets allocated to a single
instance of a function. A user can select a value from 128MB to 2GB and above at creation time.</p>
<p>More importantly, the virtual CPU cycles get allocated proportionally to this provisioned memory size. This means
that an instance of 512 MB will have twice as much CPU speed as an instance of 256MB.</p>
<p>Does this affect the cold start time?</p>
<p>I&#39;ve run a series of tests to compare cold start latency across the board of memory/CPU sizes. The results are
somewhat mixed.</p>
<p>AWS Lambda Javascript doesn&#39;t seem to have significant differences. This probably means that not so much CPU load
is required to start a Node.js &quot;Hello World&quot; application:</p>
<p><img src="/2018/08/serverless-cold-start-war//aws-coldstart-js-by-memory.png" alt="AWS Javascript Cold Start by Memory"></p>
<p>AWS Lambda .NET Core runtime does depend on memory size though. Cold start time drops dramatically with every increase
in allocated memory and CPU:</p>
<p><img src="/2018/08/serverless-cold-start-war//aws-coldstart-csharp-by-memory.png" alt="AWS C# Cold Start by Memory"></p>
<p>GCP Cloud Functions expose a similar effect even for Javascript runtime:</p>
<p><img src="/2018/08/serverless-cold-start-war//gcp-coldstart-js-by-memory.png" alt="GCP Javascript Cold Start by Memory"></p>
<p>In contrast to Amazon and Google, Microsoft doesn&#39;t ask to select a memory limit. Azure will charge Functions based 
on the actual memory usage. More importantly, it will always dedicate a full vCore for a given Function execution.</p>
<p>It&#39;s not exactly apples-to-apples, but I chose to fix the memory allocations of AWS Lambda and GCF to 1024 MB.
This feels the closest to Azure&#39;s vCore capacity, although I haven&#39;t tried a formal CPU performance comparison.</p>
<p>Given that, let&#39;s see how the 3 cloud providers compare in cold start time.</p>
<h2 id="javascript-baseline">Javascript Baseline</h2>
<p>Node.js is the only runtime supported in production by Google Cloud Functions right now. Javascript is also
probably by far the most popular language for serverless applications across the board.</p>
<p>Thus, it makes sense to compare the 3 cloud providers on how they perform in Javascript. The
base test measures the cold starts of &quot;Hello World&quot; type of functions. Functions have no 
dependencies, so deployment package is really small.</p>
<p>Here are the numbers for cold starts:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart-js-baseline.png" alt="Cold Start for Basic Javascript Functions"></p>
<p>AWS is clearly doing the best job here. GCP takes the second place, and Azure is the slowest. The rivals are
sort of close though, seemingly playing in the same league so the exact disposition might change over time.</p>
<h2 id="how-do-languages-compare-">How Do Languages Compare?</h2>
<p>I&#39;ve written Hello World HTTP function in all supported languages of the cloud platforms: </p>
<ul>
<li>AWS: Javascript, Python, Java, Go and C# (.NET Core)</li>
<li>Azure: Javascript and C# (precompiled .NET assembly)</li>
<li>GCP: Javascript</li>
</ul>
<p>Azure kind of supports much more languages, including Python and Java, but they are still considered
experimental / preview, so the cold starts are not fully optimized. See 
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">my previous article</a> for exact numbers.</p>
<p>Same applies to Python on GCP.</p>
<p>The following chart shows some intuition about the cold start duration per language. The languages
are ordered based on mean response time, from lowest to highest:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart-per-language.png" alt="Cold Start per Language per Cloud and Language"></p>
<p>AWS provides the richest selection of runtimes, and 4 out of 5 are faster than the other two cloud providers.
C# / .NET seems to be the least optimized (Amazon, why is that?).</p>
<h2 id="does-size-matter-">Does Size Matter?</h2>
<p>OK, enough of Hello World. A real-life function might be more heavy, mainly because it would
depend on other third-party libraries.</p>
<p>To simulate such scenario, I&#39;ve measured cold starts for functions with extra dependencies:</p>
<ul>
<li>Javascript referencing 3 NPM packages - 5MB zipped</li>
<li>Javascript referencing 38 NPM packages - 35 MB zipped</li>
<li>C# function referencing 5 NuGet packages - 2 MB zipped</li>
<li>Java function referencing 5 Maven packages - 15 MB zipped</li>
</ul>
<p>Here are the results:</p>
<p><img src="/2018/08/serverless-cold-start-war//coldstart-dependencies.png" alt="Cold Start Dependencies"></p>
<p>As expected, the dependencies slow the loading down. You should keep your Functions lean,
otherwise, you will pay in seconds for every cold start.</p>
<p>However, the increase in cold start seems quite low, especially for precompiled languages.</p>
<p>A very cool feature of GCP Cloud Functions is that you don&#39;t have to include NPM packages into
the deployment archive. You just add <code>package.json</code> file and the runtime will restore them for you.
This makes the deployment artifact ridiculously small, but doesn&#39;t seem to slow down the cold
starts either. Obviously, Google pre-restores the packages in advance, before the actual request 
comes in.</p>
<h2 id="avoiding-cold-starts">Avoiding Cold Starts</h2>
<p>The overall impression is that cold start delays aren&#39;t that high, so most applications can tolerate
them just fine.</p>
<p>If that&#39;s not the case, some tricks can be implemented to keep function instances warm.
The approach is universal for all 3 providers: once in X minutes, make an artificial call to
the function to prevent it from expiring.</p>
<p>Implementation details will differ since the expiration policies are different, as we explored
above.</p>
<p>For applications with higher load profile, you might want to fire several parallel &quot;warming&quot;
requests in order to make sure that enough instances are kept in warm stock.</p>
<p>For further reading, have a look at my 
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">Cold Starts Beyond First Request in Azure Functions</a>
and <a href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/">AWS Lambda Warmer as Pulumi Component</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Here are some lessons learned from all the experiments above:</p>
<ul>
<li>Be prepared for 1-3 seconds cold starts even for the smallest Functions</li>
<li>Different languages and runtimes have roughly comparable cold start time within the same platform</li>
<li>Minimize the number of dependencies, only bring what&#39;s needed</li>
<li>AWS keeps cold starts below 1 second most of the time, which is pretty amazing</li>
<li>All cloud providers are aware of the problem and are actively optimizing the cold start experience</li>
<li>It&#39;s likely that in middle term these optimizations will make cold starts a non-issue for the
vast majority of applications</li>
</ul>
<p>Do you see anything weird or unexpected in my results? Do you need me to dig deeper into other aspects?
Please leave a comment below or ping me on <a href="https://twitter.com/MikhailShilkov">twitter</a>, and let&#39;s 
sort it all out.</p>
<p>Stay tuned for more serverless perf goodness!</p>

    </div>

    

    
    <div class="post-tags">
        Posted In: <a href='/tags/azure/'>Azure</a>, <a href='/tags/azure-functions/'>Azure Functions</a>, <a href='/tags/serverless/'>Serverless</a>, <a href='/tags/performance/'>Performance</a>, <a href='/tags/cold-start/'>Cold Start</a>, <a href='/tags/aws/'>AWS</a>, <a href='/tags/aws-lambda/'>AWS Lambda</a>, <a href='/tags/gcp/'>GCP</a>, <a href='/tags/google-cloud-functions/'>Google Cloud Functions</a>
    </div>
    
</article>


<div class="page-nav">
    
    
    <a class="page-nav-older" href="/2/index.html">Next page &gt;&gt;</span></a>
    
</div>

<div id="me">
    <p itemscope itemtype="http://data-vocabulary.org/Person">
        <img src="/images/Headshot-Square.jpg" alt="Mikhail Shilkov" itemprop="photo" />
        I'm <b><span itemprop="name">Mikhail Shilkov</span></b>, a <span itemprop="title">software developer and architect</span>,
        a Microsoft Azure MVP, Russian expat living in the Netherlands. I am passionate about cloud technologies, 
        functional programming and the intersection of the two.
    </p>
    <p>
        <a href="https://www.linkedin.com/in/mikhailshilkov/">LinkedIn</a> &#8226;
        <a href="https://twitter.com/mikhailshilkov">@mikhailshilkov</a> &#8226;
        <a href="https://github.com/mikhailshilkov">GitHub</a> &#8226;
        <a href="https://stackoverflow.com/users/1171619/mikhail">Stack Overflow</a>
    </p>
</div>
</div>
<div class="container">
    <div class="navbar navbar-footer">
        <p class="navbar-center navbar-text">Content copyright &copy; 2018 Mikhail Shilkov</p>
    </div>
</div>



<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.pack.js"></script>
<script src="/vendor/prism.js"></script>
<script src="/site.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59218480-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>