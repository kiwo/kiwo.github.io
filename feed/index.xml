<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Mikhail Shilkov</title>
    <link href="https://mikhail.io/feed/" rel="self"/>
    <link href="https://mikhail.io"/>
    <updated>2017-12-13T15:10:49.734Z</updated>
    <id>https://mikhail.io/</id>
    <author>
        <name>Mikhail Shilkov</name>
        <email></email>
    </author>

    
    <entry>
        <title>Azure Functions Get More Scalable and Elastic</title>
        <link href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/"/>
        <updated>2017-12-13T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-12-13,/2017/12/azure-functions-get-more-scalable-and-elastic/</id>
        <content type="html"><![CDATA[<p>Back in August this year, I&#39;ve posted 
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">Azure Functions: Are They Really Infinitely Scalable and Elastic?</a>
with two experiments about Azure Function App auto scaling. I ran a simple
CPU-bound function based on Bcrypt hashing, and measured how well Azure
was running my Function under load.</p>
<p>The results were rather pessimistic. Functions were scaling up to many 
instances, but there were significant delays in doing so, so the processing
slowed down up to 40 minutes.</p>
<p>Azure Functions team notified me that they rolled out an updated version of
the service, which should significantly improve my results.</p>
<p>So I ran the exact same tests again, and got the new results. I will show
these results below.</p>
<p><em>TL;DR. Scaling responsiveness improved significantly. The max delay reduced
from 40 to 6 minutes. There are some improvements still to be desired: 
sub-minute latency is not yet reachable for similar scenarios.</em></p>
<h2 id="setup">Setup</h2>
<p>See the Function code and the description of the two experiments in
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">my previous article</a>.</p>
<h2 id="experiment-1-steady-load">Experiment 1: Steady Load</h2>
<p>In &quot;Steady Load&quot; scenario 100,000 messages were sent to the queue at
constant pace, evenly spread over 2 hours.</p>
<p>Here are the <strong>old</strong> metrics of Queue Backlog and Instance Count over time:</p>
<p><img src="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic//FunctionAppScaling.png" alt="Function App Scaling (Old)"></p>
<p><em>Old charts are shown in gray background</em></p>
<p>You can see a huge delay of almost one hour before the function caught up 
to speed of incoming messages and half-an-hour more before the backlog
got cleared.</p>
<p>The <strong>new</strong> results on the same chart after the runtime update:</p>
<p><img src="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic//FunctionAppScalingNew.png" alt="Function App Scaling (New)"></p>
<p>This looks much better. The maximum backlog is 7 times lower; there&#39;s almost
no initial delay before the auto scaling kicks in; and overall instance 
allocation is much more stable.</p>
<hr>
<p>One more chart is from the same experiment, but it shows slightly different 
metrics. The <strong>old</strong> results of Delay (Age) in seconds and Processing Rate 
in messages per minute:</p>
<p><img src="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic//FunctionAppDelay.png" alt="Function App Delay"></p>
<p>The <strong>new</strong> chart after the runtime update:</p>
<p><img src="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic//FunctionAppDelayNew.png" alt="Function App Delay"></p>
<p>Again, much less delay overall, and processing rate more-or-less stabilizes 
after the first 15 minutes.</p>
<h2 id="experiment-2-spiky-load">Experiment 2: Spiky Load</h2>
<p>The second experiment spanned over 5 hours. The messages were sent mostly
at low-ish fixed rate, except for 5 periods of sudden spikes. The
green line on the charts below shows these spikes very well.</p>
<p>At the first run 4 months ago, Functions runtime had troubles keeping up
to speed even between those bursts of messages.</p>
<p>Here is the chart of the <strong>old</strong> spiky load processing:</p>
<p><img src="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic//SpikyLoadProcessing.png" alt="Spicky Load Processing (Old)"></p>
<p>You can see that the backlog after each spike goes down really slow. The
blue line of processing rate doesn&#39;t match the green line almost nowhere,
which reveals the struggle to adapt.</p>
<p>The <strong>new</strong> results of the same chart after the runtime update are quite
different:</p>
<p><img src="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic//SpikyLoadProcessingNew.png" alt="Spicky Load Processing (New)"></p>
<p>Notice how the backlog is empty and the blue processing rate matches exactly
the incoming rate during all time except after traffic bursts. The queue goes 
up during each spike, but the processing rate immediately accelerates too,
and the crisis is gone within 15 minutes.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Azure Functions team is clearly working on improvements. While the results in
August were puzzling or even embarrassing, the December benchmark makes much
more sense.</p>
<p>Looks like Azure Functions are now suitable for CPU-intensive data processing
scenarios with flexible load, targeting the maximum delay at about several
minutes.</p>
<p>Obviously, the results are not perfect just yet. Here&#39;s what still can be
done better:</p>
<ul>
<li><p><strong>Scale faster initially</strong>. In the first experiment, the biggest delay
appeared right after the start, when the backlog was growing linearly for 
10 minutes. &quot;0 to 100&quot; might not be a very realistic scenario, but probably
that&#39;s how many folks will test Functions against their workloads.</p>
</li>
<li><p><strong>Do not scale down that fast after backlog goes to 0</strong>. Every time the
queue backlog goes to 0, the runtime kills the biggest part of instances
almost immediately. During my runs, this caused the queue to grow again without
a good reason from user&#39;s perspective.</p>
</li>
<li><p><strong>Do not allow the backlog to grow without message spikes</strong>. Related to
the previous item, but slightly different focus. When the load is stable,
I would expect the runtime to keep my queue as close to empty as possible.
I guess Azure tries to minimize the resources that it consumes behind
the scenes, but this should be balanced in favor of user experience.</p>
</li>
<li><p><strong>Make scaling algorithms more open</strong>. It&#39;s a black box right now. I
would love to see some documentation, if not code, to be published about
what exactly to expect from Consumption Plan auto scaling.</p>
</li>
</ul>
<p>I&#39;ll be running more scaling experiments with other types of workloads in the
nearest future, so... more benchmarks are coming.</p>
<p>Happy scaling!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Precompiled Azure Functions in F#</title>
        <link href="https://mikhail.io/2017/12/precompiled-azure-functions-in-fsharp/"/>
        <updated>2017-12-03T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-12-03,/2017/12/precompiled-azure-functions-in-fsharp/</id>
        <content type="html"><![CDATA[<p><em>This post is giving a start to 
<a href="https://sergeytihon.com/2017/10/22/f-advent-calendar-in-english-2017/">F# Advent Calendar in English 2017</a>. 
Please follow the calendar for all the great posts to come.</em></p>
<p>Azure Functions is a &quot;serverless&quot; cloud offering from Microsoft. It
allows you to run your custom code as response to events in the cloud. 
Functions are very easy to
start with; and you only pay per execution - with free allowance sufficient
for any proof-of-concept, hobby project or even low-usage production loads.
And when you need more, Azure will scale your project up automatically.</p>
<p>F# is one of the officially supported languages for Azure Functions.
Originally, F# support started with F# Script files (authored directly
in Azure portal or copied from local editor), so you can find many articles
online to get started, e.g.
<a href="http://brandewinder.com/2017/02/11/fsharp-azure-function-from-the-ground-up-part-1/">Creating an Azure Function in F# from the ground up</a> and
<a href="http://brandewinder.com/2017/03/06/fsharp-azure-function-from-the-ground-up-part-2/">Part 2</a>
by Mathias Brandewinder.</p>
<p>However, I find script-based model a bit limited. In today&#39;s article I
will focus on creating Azure Functions as precompiled .NET libraries.
Along the way, I&#39;ll use cross-platform tools like .NET Core and VS Code,
and I&#39;ll show how to integrate Functions with some popular tools
like Suave and Paket.</p>
<h2 id="create-a-project">Create a Project</h2>
<p>You can follow this walkthrough on Windows or Mac, just make sure that
you have <code>.NET Core 2</code> and <code>Node.js 8.x</code> with <code>npm</code> installed. My editor of
choice is Visual Studio Code with Ionide plugin.</p>
<p>I&#39;ll show you how to create a new F# Function App from scratch. If you want to
jump to runnable project, you can get it from 
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/6-precompiled-timer">my github</a>.</p>
<p>We start with creating a new F# library project for .NET Standard 2. Run
in your command line:</p>
<pre class="highlight"><code class="hljs sh">dotnet new classlib --language F<span class="hljs-comment"># --name HelloFunctions</span>
</code></pre>
<p>This command creates a folder with two files: <code>HelloFunctions.fsproj</code> project
file and <code>Library.fs</code> source code file.</p>
<p>Now, add a reference to Azure Functions NuGet package:</p>
<pre class="highlight"><code class="hljs sh">dotnet add package Microsoft.NET.Sdk.Functions
</code></pre>
<h2 id="define-a-function">Define a Function</h2>
<p>Open <code>Library.fs</code> code file and change it to the following code:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-keyword">namespace</span> HelloFunctions

<span class="hljs-keyword">open</span> System
<span class="hljs-keyword">open</span> Microsoft.Azure.WebJobs
<span class="hljs-keyword">open</span> Microsoft.Azure.WebJobs.Host

<span class="hljs-keyword">module</span> Say =
  <span class="hljs-keyword">let</span> <span class="hljs-keyword">private</span> daysUntil (d: DateTime) =
    (d - DateTime.Now).TotalDays |&gt; int

  <span class="hljs-keyword">let</span> hello (timer: TimerInfo, log: TraceWriter) =
    <span class="hljs-keyword">let</span> christmas = <span class="hljs-keyword">new</span> DateTime(<span class="hljs-number">2017</span>, <span class="hljs-number">12</span>, <span class="hljs-number">25</span>)

    daysUntil christmas
    |&gt; sprintf <span class="hljs-string">"%d days until Christmas"</span>
    |&gt; log.Info
</code></pre>
<p>We defined a function <code>hello</code> which should be triggered by Functions runtime
based on time intervals. Every time the function is called, we log how many
days we still need to wait before Christmas 2017.</p>
<p>To convert this simple F# function to an Azure Function, create a folder called
<code>Hello</code> (or choose any other name) next to the project file and add 
<code>function.json</code> file in there:</p>
<pre class="highlight"><code class="hljs json">{
  <span class="hljs-attr">"bindings"</span>: [
    {
      <span class="hljs-attr">"name"</span>: <span class="hljs-string">"timer"</span>,
      <span class="hljs-attr">"type"</span>: <span class="hljs-string">"timerTrigger"</span>,
      <span class="hljs-attr">"schedule"</span>: <span class="hljs-string">"0 * * * * *"</span>
    }
  ],
  <span class="hljs-attr">"scriptFile"</span>: <span class="hljs-string">"../bin/HelloFunctions.dll"</span>,
  <span class="hljs-attr">"entryPoint"</span>: <span class="hljs-string">"HelloFunctions.Say.hello"</span>
}
</code></pre>
<p>We defined that:</p>
<ul>
<li>Our function is triggered by timer</li>
<li>It runs every minute at 0 seconds</li>
<li>The entry point is our <code>hello</code> function in the compiled assembly</li>
</ul>
<h2 id="prepare-local-runtime">Prepare Local Runtime</h2>
<p>There are a couple more configuration files needed to be able to
run the Function App locally. <code>host.json</code> defines hosting parameters; empty
file will do for now:</p>
<pre class="highlight"><code class="hljs json">{
}
</code></pre>
<p>Most triggers need to connect to a Storage Account. For examples, timer
trigger uses it to hold leases to define which running instance will 
actually execute the action every minute. Copy a connection string to your 
Storage Account (local Storage emulator is fine too) and put it into 
<code>local.settings.json</code> file:</p>
<pre class="highlight"><code class="hljs json">{
  <span class="hljs-attr">"IsEncrypted"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-attr">"Values"</span>: {
    <span class="hljs-attr">"AzureWebJobsStorage"</span>: <span class="hljs-string">"...your connection string..."</span>
  }
}
</code></pre>
<p>Note that this file is only used for local development and is not published
to Azure by default.</p>
<p>Finally, we need to modify <code>fsproj</code> file to make the build tool copy those
files into <code>bin</code> folder. Add the following section in there:</p>
<pre class="highlight"><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">ItemGroup</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">Content</span> <span class="hljs-attr">Include</span>=<span class="hljs-string">"Hello\function.json"</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">CopyToOutputDirectory</span>&gt;</span>PreserveNewest<span class="hljs-tag">&lt;/<span class="hljs-name">CopyToOutputDirectory</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">Content</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">Content</span> <span class="hljs-attr">Include</span>=<span class="hljs-string">"host.json"</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">CopyToOutputDirectory</span>&gt;</span>PreserveNewest<span class="hljs-tag">&lt;/<span class="hljs-name">CopyToOutputDirectory</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">Content</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">Content</span> <span class="hljs-attr">Include</span>=<span class="hljs-string">"local.settings.json"</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">CopyToOutputDirectory</span>&gt;</span>PreserveNewest<span class="hljs-tag">&lt;/<span class="hljs-name">CopyToOutputDirectory</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">Content</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">ItemGroup</span>&gt;</span>
</code></pre>
<h2 id="run-app-locally">Run App Locally</h2>
<p>The first step is to build and publish our Function App with <code>dotnet</code>
commands:</p>
<pre class="highlight"><code class="hljs sh">dotnet build
dotnet publish
</code></pre>
<p>The first line produces the dll file and the second line copies it
and all of its dependencies to <code>publish</code> folder.</p>
<p>The nice thing about Azure Functions is that you can easily run them
locally on a development machine. Execute the following command to 
install the runtime and all the required libraries:</p>
<pre class="highlight"><code class="hljs sh">npm install -g azure-functions-core-tools@core
</code></pre>
<p>This will add a <code>func</code> CLI to your system which is the tool to
use for all Function related operations.</p>
<p>Navigate to <code>bin\Debug\netstandard2.0\publish</code> folder and run <code>func start</code> 
from there. You should see that your app is now running, and your timer
function is scheduled for execution:</p>
<p><img src="https://mikhail.io/2017/12/precompiled-azure-functions-in-fsharp/./funcstart.png" alt="Function App Start"></p>
<p>Once the next minute comes, the timer will trigger and you will see
messages in the log:</p>
<p><img src="https://mikhail.io/2017/12/precompiled-azure-functions-in-fsharp/./funcran.png" alt="Timer Trigger Working"></p>
<h2 id="integrate-into-vs-code-">Integrate into VS Code </h2>
<p>You are free to use full Visual Studio or any editor to develop Function
Apps in F#. I&#39;ve been mostly using VS Code for this purpose, and I believe
it&#39;s quite popular among F# community.</p>
<p>If you use VS Code, be sure to setup the tasks that you can use from within
the editor. I usually have at least 3 tasks: &quot;build&quot; (<code>dotnet build</code>),
&quot;publish&quot; (<code>dotnet publish</code>) and &quot;run&quot; (<code>func start --script-root bin\\debug\\netstandard2.0\\publish</code>),
with shortcuts configured to all of them.</p>
<p>You can find an example of <code>tasks.json</code> file
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/6-precompiled-timer/.vscode/tasks.json">here</a>.</p>
<p>Also, check out <a href="https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions">Azure Functions Extension</a>.</p>
<h2 id="deploy-to-azure">Deploy to Azure</h2>
<p>You can deploy the exact same application binaries to Azure. Start by 
creating an empty Function App in the portal, or via Azure CLI (<code>func</code> CLI
does not support that).</p>
<p>Then run the following command to deploy your precompiled function to
this app:</p>
<pre class="highlight"><code class="hljs sh">func azure functionapp publish &lt;FunctionAppName&gt;
</code></pre>
<p>At the first run, it will verify your Azure credentials.</p>
<p>In real-life production scenarios your workflow is probably going to be
similar to this:</p>
<ul>
<li>Change Function App code</li>
<li>Run it locally to test the change</li>
<li>Push the code changes to the source control repository</li>
<li>Have your CI/CD pipeline build it, run the tests and then push
the binaries to Azure Functions environment</li>
</ul>
<h2 id="http-trigger">HTTP Trigger</h2>
<p>Timer-triggered functions are useful, but that&#39;s just one limited use case.
Several other event types can trigger Azure Functions, and for all of them
you can create precompiled functions and run them locally.</p>
<p>The most ubiquotous trigger for any serverless app is probably HTTP. So,
for the rest of the article I will focus on several approaches to 
implement HTTP functions. Nonetheless, the same techique can be applied to
other triggers too.</p>
<p>F# code for the simplest HTTP Function can look like this:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-keyword">namespace</span> PrecompiledApp

<span class="hljs-keyword">open</span> Microsoft.AspNetCore.Mvc
<span class="hljs-keyword">open</span> Microsoft.AspNetCore.Http
<span class="hljs-keyword">open</span> Microsoft.Azure.WebJobs.Host

<span class="hljs-keyword">module</span> PrecompiledHttp =

  <span class="hljs-keyword">let</span> run(req: HttpRequest, log: TraceWriter) =
    log.Info(<span class="hljs-string">"F# HTTP trigger function processed a request."</span>)
    ContentResult(Content = <span class="hljs-string">"HO HO HO Merry Christmas"</span>, ContentType = <span class="hljs-string">"text/html"</span>)
</code></pre>
<p>You can find a full example of HTTP Function App 
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/5-precompiled">here</a>.</p>
<p>This code is using ASP.NET Core classes for request and response. It&#39;s still
just an F# function, so we need to bind it to a trigger in <code>function.json</code>:</p>
<pre class="highlight"><code class="hljs json">{
  <span class="hljs-attr">"bindings"</span>: [
    {
      <span class="hljs-attr">"type"</span>: <span class="hljs-string">"httpTrigger"</span>,
      <span class="hljs-attr">"methods"</span>: [<span class="hljs-string">"get"</span>],
      <span class="hljs-attr">"authLevel"</span>: <span class="hljs-string">"anonymous"</span>,
      <span class="hljs-attr">"name"</span>: <span class="hljs-string">"req"</span>,
      <span class="hljs-attr">"route"</span>: <span class="hljs-string">"hellosanta"</span>
    }
  ],
  <span class="hljs-attr">"scriptFile"</span>: <span class="hljs-string">"../bin/PrecompiledApp.dll"</span>,
  <span class="hljs-attr">"entryPoint"</span>: <span class="hljs-string">"PrecompiledApp.PrecompiledHttp.run"</span>
}
</code></pre>
<p>If you run the app, the function will be hosted at localhost</p>
<p><img src="https://mikhail.io/2017/12/precompiled-azure-functions-in-fsharp/./httpstart.png" alt="HTTP Trigger Working"></p>
<p>And a request to <code>http://localhost:7071/api/hellosanta</code> will get responded
with our &quot;HO HO HO&quot; message.</p>
<p>This function is of &quot;Hello World&quot; level, but the fact that it&#39;s inside a
normal F# library gives you lots of power.</p>
<p>Let&#39;s see at some examples of how to use it.</p>
<h2 id="suave-function">Suave Function</h2>
<p>What can we do to enhance developer experience? We can use our 
favourite F# libraries.</p>
<p><a href="http://suave.io/">Suave</a> is one of the most popular libraries to 
implement Web API&#39;s with. And we can use it in Azure Functions too!</p>
<p>Let&#39;s first make a small twist to HTTP trigger definition in <code>function.json</code>:</p>
<pre class="highlight"><code class="hljs undefined">"bindings": [
  {
    "type": "httpTrigger",
    "methods": ["get"],
    "authLevel": "anonymous",
    "name": "req",
    "route": "{*anything}"
  }
],
</code></pre>
<p>Binding now defines a wildcard route to redirect all requests 
to this function. That&#39;s because we want Suave to take care of routing
for us.</p>
<p>The definition of such routing will look familiar to all Suave users:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-keyword">module</span> App =
  <span class="hljs-keyword">open</span> Suave
  <span class="hljs-keyword">open</span> Suave.Successful
  <span class="hljs-keyword">open</span> Suave.Operators
  <span class="hljs-keyword">open</span> Suave.Filters

  <span class="hljs-keyword">let</span> app = 
    GET &gt;=&gt; choose
      [ path <span class="hljs-string">"/api/what"</span> &gt;=&gt; OK <span class="hljs-string">"Every time we love, every time we give, it's Christmas."</span>
        path <span class="hljs-string">"/api/when"</span> &gt;=&gt; OK <span class="hljs-string">"Christmas isn't a season. It's a feeling."</span>
        path <span class="hljs-string">"/api/how"</span> &gt;=&gt; OK <span class="hljs-string">"For it is in giving that we receive."</span> ]
</code></pre>
<p>Azure Function is just a one-liner wiring Suave app into the pipeline:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-keyword">module</span> Http =
  <span class="hljs-keyword">open</span> Suave.Azure.Functions.Context

  <span class="hljs-keyword">let</span> run req =
    req |&gt; runWebPart App.app  |&gt; Async.StartAsTask
</code></pre>
<p>The heavy lifting is done by <code>runWebPart</code> function, which is a utility
function defined in the same application. You can see the full code
of this wiring in <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/7-suave">my repo</a>.</p>
<p>Run the application and request the URL <code>http://localhost:7071/api/what</code> 
to see the function in action.</p>
<p>This example is very simple, but you can do lots of powerful stuff with Suave!
Most probably, you shouldn&#39;t go over the root and try to fit whole
mulpti-resource REST API into a single Azure Function. But it might still
make sense to keep related HTTP calls together, and Suave can help to keep
it cleaner.</p>
<h2 id="managing-dependencies-with-paket">Managing Dependencies with Paket</h2>
<p>Once your Function App becomes bigger and you start using multiple F#
projects, it makes sense to switch to <a href="https://fsprojects.github.io/Paket/">Paket</a>
package manager.</p>
<p>It is totally possible to use Paket with Azure Functions. There isn&#39;t much
specific to Azure Functions, really. Here is an example of <code>paket.dependecies</code>
file</p>
<pre class="highlight"><code class="hljs stylus">source https:<span class="hljs-comment">//www.nuget.org/api/v2</span>

framework: &gt;= netstandard2.<span class="hljs-number">0</span>
nuget FSharp<span class="hljs-selector-class">.Core</span>
nuget Microsoft<span class="hljs-selector-class">.NET</span><span class="hljs-selector-class">.Sdk</span><span class="hljs-selector-class">.Functions</span>
nuget Microsoft<span class="hljs-selector-class">.AspNetCore</span><span class="hljs-selector-class">.Mvc</span><span class="hljs-selector-class">.Core</span>
</code></pre><p>that I used in <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/8-paket">example</a> 
which demonstrates Paket + Functions combination.</p>
<h2 id="attribute-based-functions">Attribute-Based Functions</h2>
<p>Up until now, we were writing <code>function.json</code> files manually for each 
function. This is not very tedious, but it is error prone. Microsoft offers an alternative 
programming model where these files are auto-generated by Functions SDK.</p>
<p>This programming model is based on attributes, which are similar to WebJobs 
SDK attributes. With this approach, there&#39;s no <code>function.json</code> file in 
the project. Instead, the function declaration is decorated with attributes:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-meta">[&lt;FunctionName("AttributeBased")&gt;]</span>
<span class="hljs-keyword">let</span> run(<span class="hljs-meta">[&lt;HttpTrigger&gt;]</span> req: HttpRequest, log: TraceWriter)
</code></pre>
<p>The same development flow still works. Once you run <code>dotnet build</code>, a new 
<code>function.json</code> file will be generated and placed into <code>bin</code> folder. Functions 
runtime will be able to use it to run the function as usual.</p>
<p>Note that the generated file looks a bit different from the manual 
equivalent:</p>
<ol>
<li><p>It manifests itself with</p>
<pre class="highlight"><code class="hljs undefined"> "generatedBy": "Microsoft.NET.Sdk.Functions.Generator-1.0.6",
 "configurationSource": "attributes",
</code></pre>
</li>
<li><p>In case you use input and output bindings, you won&#39;t be able to see them
in the generated file. Only trigger will be visible in <code>json</code>. Don&#39;t worry,
input and output bindings will still work.</p>
</li>
</ol>
<p>You can find an example of HTTP function with attributes 
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/9-attributes">here</a>.</p>
<p>There are pro&#39;s and con&#39;s in this model. Obviously, not having to write
JSON files manually is beneficial. Some people find the binding attributes
really ugly though, especially when you have 3 or 4 bindings and each has 
multiple parameters. </p>
<p>My preference is to use attributes, but don&#39;t mix attribute decoration
with real code. I.e. keep the Function&#39;s body to a simple 1-liner, and
delegate the call to a properly defined F# function with the actual
domain logic.</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>Lots of F# users value the language for how quickly one can be productive
with it: based on concise syntax, powerful libraries and tools like FSI.</p>
<p>In my opinion, Azure Functions fit nicely into the picture. It takes just
several minutes before you can run your first Function App on developer
machine, and then seamlessly transfer it into the cloud.</p>
<p>I&#39;ve prepared a github repository where you can find more
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples">Examples of Azure Functions implemented in F#</a>.</p>
<p>Merry Serverless Functional Christmas!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Azure F#unctions Talk at FSharping Meetup in Prague</title>
        <link href="https://mikhail.io/2017/11/azure-functions-fsharp-talk/"/>
        <updated>2017-11-10T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-11-10,/2017/11/azure-functions-fsharp-talk/</id>
        <content type="html"><![CDATA[<p>On November 8th 2017 I gave a talk about developing Azure Functions
in F# at
<a href="https://www.meetup.com/FSharping/events/244137693/">FSharping</a>
meetup in Prague. </p>
<p>I really enjoyed giving this talk: the audience was
great and asked awesome questions. One more prove that F# community is
so welcoming and energizing!</p>
<p>All the demos of that session can be found in my
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples">github repository</a>.</p>
<p>The slides were only a small portion of my talk, but you can see them
below anyways.</p>
<p>Link to full-screen HTML slides: 
<a href="https://mikhail.io/talks/fsharping-azure-functions/">Azure F#unctions</a></p>
<p>Slides on SlideShare:</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/oQIZywbdCRXdQA" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""> 
</iframe> 

<p>Thanks for attending my talk! Feel free to post any feedback in the comments.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Azure Function Triggered by Azure Event Grid</title>
        <link href="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid/"/>
        <updated>2017-10-05T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-10-05,/2017/10/azure-function-triggered-by-azure-event-grid/</id>
        <content type="html"><![CDATA[<p><em>Update: I missed the elephant in the room. There actually exists a specialized
trigger for Event Grid binding. In the portal, just select <code>Experimental</code>
in <code>Scenario</code> drop down while creating the function. In precompiled 
functions, reference <code>Microsoft.Azure.WebJobs.Extensions.EventGrid</code> NuGet
package.</em></p>
<p><em>The rest of the article describes my original approach to trigger an
Azure Function from <a href="https://azure.microsoft.com/en-us/services/event-grid/">Azure Event Grid</a> 
with generic Web Hook trigger.</em></p>
<p>Here are the steps to follow:</p>
<h2 id="create-a-function-with-webhook-trigger">Create a Function with Webhook Trigger</h2>
<p>I&#39;m not aware of a specialized trigger type for Event Grid, so
I decided to use Generic Webhook trigger (which is essentially an
HTTP trigger).</p>
<p>I used the Azure Portal to generate a function, so here is the 
<code>function.json</code> that I got:</p>
<pre class="highlight"><code class="hljs json">{
  <span class="hljs-attr">"bindings"</span>: [
    {
      <span class="hljs-attr">"type"</span>: <span class="hljs-string">"httpTrigger"</span>,
      <span class="hljs-attr">"direction"</span>: <span class="hljs-string">"in"</span>,
      <span class="hljs-attr">"webHookType"</span>: <span class="hljs-string">"genericJson"</span>,
      <span class="hljs-attr">"name"</span>: <span class="hljs-string">"req"</span>
    },
    {
      <span class="hljs-attr">"type"</span>: <span class="hljs-string">"http"</span>,
      <span class="hljs-attr">"direction"</span>: <span class="hljs-string">"out"</span>,
      <span class="hljs-attr">"name"</span>: <span class="hljs-string">"res"</span>
    }
  ],
  <span class="hljs-attr">"disabled"</span>: <span class="hljs-literal">false</span>
}
</code></pre>
<p>For precompiled functions, just decorate it with <code>HttpTriggerAttribute</code> with
POST method:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> Task&lt;HttpResponseMessage&gt; <span class="hljs-title">Run</span>(<span class="hljs-params">
    [HttpTrigger(AuthorizationLevel.Function, <span class="hljs-string">"post"</span></span>)] HttpRequestMessage req)
</span></code></pre>
<h2 id="parse-the-payload">Parse the Payload</h2>
<p>Events from Event Grid will arrive in a specific predefined JSON format.
Here is an example of events to expect:</p>
<pre class="highlight"><code class="hljs json">[{
  <span class="hljs-attr">"id"</span>: <span class="hljs-string">"0001"</span>,
  <span class="hljs-attr">"eventType"</span>: <span class="hljs-string">"MyHelloWorld"</span>,
  <span class="hljs-attr">"subject"</span>: <span class="hljs-string">"Hello World!"</span>,
  <span class="hljs-attr">"eventTime"</span>: <span class="hljs-string">"2017-10-05T08:53:07"</span>,
  <span class="hljs-attr">"data"</span>: {
    <span class="hljs-attr">"hello"</span>: <span class="hljs-string">"world"</span>
  },
  <span class="hljs-attr">"topic"</span>: <span class="hljs-string">"/SUBSCRIPTIONS/GUID/RESOURCEGROUPS/NAME/PROVIDERS/MICROSOFT.EVENTGRID/TOPICS/MY-EVENTGRID-TOPIC1"</span>
}]
</code></pre>
<p>To be able to parse those data more easily, I defined a C# class to deserialize
JSON to:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">GridEvent</span>
{
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Id { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> EventType { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Subject { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> DateTime EventTime { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> Dictionary&lt;<span class="hljs-keyword">string</span>, <span class="hljs-keyword">string</span>&gt; Data { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Topic { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
}
</code></pre>
<p>Now, the function can read the events (note, that they are sent in arrays)
from the body of POST request:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">async</span> Task&lt;HttpResponseMessage&gt; <span class="hljs-title">Run</span>(<span class="hljs-params">HttpRequestMessage req, TraceWriter log</span>)
</span>{
    <span class="hljs-keyword">string</span> jsonContent = <span class="hljs-keyword">await</span> req.Content.ReadAsStringAsync();
    <span class="hljs-keyword">var</span> events = JsonConvert.DeserializeObject&lt;GridEvent[]&gt;(jsonContent);

    <span class="hljs-comment">// do something with events</span>

    <span class="hljs-keyword">return</span> req.CreateResponse(HttpStatusCode.OK);
}
</code></pre>
<h2 id="validate-the-endpoint">Validate the Endpoint</h2>
<p>To prevent you from sending events to endpoints that you don&#39;t own, Event
Grid requires each subsriber to validate itself. For this purpose, Event
Grid will send events of the special type <code>SubscriptionValidation</code>. </p>
<p>The validation request will contain a code, which we need to echo back in
200-OK HTTP response. </p>
<p>Here is a small piece of code to do just that:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">if</span> (req.Headers.GetValues(<span class="hljs-string">"Aeg-Event-Type"</span>).FirstOrDefault() == <span class="hljs-string">"SubscriptionValidation"</span>)
{
    <span class="hljs-keyword">var</span> code = events[<span class="hljs-number">0</span>].Data[<span class="hljs-string">"validationCode"</span>];
    <span class="hljs-keyword">return</span> req.CreateResponse(HttpStatusCode.OK,
        <span class="hljs-keyword">new</span> { validationResponse = code });
}
</code></pre>
<p>The function is ready!</p>
<h2 id="create-a-custom-event-grid-topic">Create a Custom Event Grid Topic</h2>
<p>To test it out, go to the portal and create a custom Event Grid topic.
Then click on Add Event Subscription button, give it a name and copy paste
the function URL (including key) to Subscriber Endpoint field:</p>
<p><img src="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid//function-url.png" alt="Azure Function URL"></p>
<p><img src="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid//event-subscription.png" alt="Event Grid Subscription"></p>
<p>Creating a subscription will immediately trigger a validation request to
your function, so you should see one invocation in the logs.</p>
<h2 id="send-custom-events">Send Custom Events</h2>
<p>Now, go to your favorite HTTP client (curl, Postman, etc) and send a sample
event to check how the whole setup works:</p>
<pre class="highlight"><code class="hljs http"><span class="hljs-keyword">POST</span> <span class="hljs-string">/api/events</span> HTTP/1.1
<span class="hljs-attribute">Host</span>: &lt;your-eventgrid-topic&gt;.westus2-1.eventgrid.azure.net
<span class="hljs-attribute">aeg-sas-key</span>: &lt;key&gt;
<span class="hljs-attribute">Content-Type</span>: application/json

<span class="json">[{
  <span class="hljs-attr">"id"</span>: <span class="hljs-string">"001"</span>,
  <span class="hljs-attr">"eventType"</span>: <span class="hljs-string">"MyHelloWorld"</span>,
  <span class="hljs-attr">"subject"</span>: <span class="hljs-string">"Hello World!"</span>,
  <span class="hljs-attr">"eventTime"</span>: <span class="hljs-string">"2017-10-05T08:53:07"</span>,
  <span class="hljs-attr">"data"</span>: {
    <span class="hljs-attr">"hello"</span>: <span class="hljs-string">"world"</span>
  }
}]
</span></code></pre>
<p>Obviously, adjust the endpoint and key based on the data from the portal.</p>
<p>You should get a 200-OK back and then see your event in Azure Function 
invocation logs.</p>
<p>Have fun!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Wanted: Effectively-Once Processing in Azure</title>
        <link href="https://mikhail.io/2017/09/wanted-effectively-once-processing-in-azure/"/>
        <updated>2017-09-25T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-09-25,/2017/09/wanted-effectively-once-processing-in-azure/</id>
        <content type="html"><![CDATA[<p><em>This experimental post is a question. The question
is too broad for StackOverflow, so I&#39;m posting it here. Please engage in the
comments section, or forward the link to subject experts.</em></p>
<p>TL;DR: Are there any known patterns / tools / frameworks to provide 
scalable, stateful, effectively-once, end-to-end processing of messages, 
to be hosted in Azure, preferably on PaaS-level of service?</p>
<h2 id="motivational-example">Motivational Example</h2>
<p>Let&#39;s say we are making a TODO app. There is a constant flow of requests
to create a TODO in the system. Each request contains just two fields:
a title and a project ID which TODO should belong to. Here is the definition:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">TodoRequest</span> </span>= {
  ProjectId: int
  Title: string
}
</code></pre>
<p>Now, we want to process the request and assign each TODO an identifier,
which should be an auto-incremented integer. Numeration is unique per project,
so each TODO must have its own combination of <code>ProjectId</code> and <code>Id</code>:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">Todo</span> </span>= {
  ProjectId: int
  Id: int
  Title: string
}
</code></pre>
<p>Now, instead of relying on some database sequences, I want to describe this
transformation as a function. The function has the type <code>(TodoRequest, int) -&gt;
(Todo, int)</code>, i.e. it transforms a tuple of a request and current per-project
state (last generated ID) to a tuple of a TODO and post-processing state:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-keyword">let</span> create (request: TodoRequest, state: int) =
  <span class="hljs-keyword">let</span> nextId = state + <span class="hljs-number">1</span>
  <span class="hljs-keyword">let</span> todo = {
    ProjectId = request.ProjectId
    Id = nextId
    Title = request.Title
  }
  todo, nextId
</code></pre>
<p>This is an extremely simple function, and I can use it to great success to
process local, non-durable data.</p>
<p>But if I need to make a reliable distributed application out of it, I need
to take care of lots of things:</p>
<ol>
<li><p>No request should be lost. I need to persist all the requests into 
a durable storage in case of processor crash. </p>
</li>
<li><p>Similarly, I need to persist TODO&#39;s too. Presumably, some downstream 
logic will use the persisted data later on in TODO&#39;s lifecycle.</p>
</li>
<li><p>The state (the counter) must be durable too. In case of crash of processing
function, I want to be able to restart processing after recovery. </p>
</li>
<li><p>Processing of the requests should be sequential per project ID. Otherwise
I might get a clash of ID&#39;s in case two requests belonging to the same 
project are processed concurrently.</p>
</li>
<li><p>I still want requests to different projects to be processed in parallel,
to make sure the system scales up with the growth of project count.</p>
</li>
<li><p>There must be no holes or duplicates in TODO numbering per project, even
in face of system failures. In worst case, I agree to tolerate a duplicated
entry in the output log, but it must be exactly the same entry (i.e. two 
entries with same project id, id and title).</p>
</li>
<li><p>The system should tolerate a permanent failure of any single hardware
dependency and automatically fail-over within reasonable time.</p>
</li>
</ol>
<p>It&#39;s not feasible to meet all of those requirements without relying on some
battle-tested distributed services or frameworks.</p>
<p>Which options do I know of?</p>
<h2 id="transactions">Transactions</h2>
<p>Traditionally, this kind of requirements were solved by using transactions
in something like SQL Server. If I store requests, TODO&#39;s and current ID per
project in the same relational database, I can make each processing step a
single atomic transaction. </p>
<p>This addresses all the concerns, as long as we can stay inside the single 
database. That&#39;s probably a viable option for the TODO app, but less of so
if I convert my toy example to some real applications like IoT data 
processing.</p>
<p>Can we do the same for distributed systems at scale?</p>
<h2 id="azure-event-hubs">Azure Event Hubs</h2>
<p>Since I touched IoT space, the logical choice would be to store our entries
in Azure Event Hubs. That works for many criteria, but I don&#39;t see any available
approach to make such processing consistent in the face of failures.</p>
<p>When processing is done, we need to store 3 pieces: generated TODO event,
current processing offset and current ID. Event goes to another event hub,
processing offset is stored in Blob Storage and ID can be saved to something
like Table Storage. </p>
<p>But there&#39;s no way to store those 3 pieces atomically. Whichever order we 
choose, we are bound to get anomalies in some specific failure modes.</p>
<h2 id="azure-functions">Azure Functions</h2>
<p>Azure Functions don&#39;t solve those problems. But I want to mention this
Function-as-a-Service offering because they provide an ideal programming
model for my use case.</p>
<p>I need to take just one step from my domain function to Azure Function: 
to define bindings for e.g. Event Hubs and Table Storage.</p>
<p>However, reliability guarantees will stay poor. I won&#39;t get neither sequential
processing per Event Hub partition key, nor atomic state commit.</p>
<h2 id="azure-service-fabric">Azure Service Fabric</h2>
<p>Service Fabric sounds like a good candidate service for reliable processing. 
Unfortunately, I don&#39;t have much experience with it to judge.</p>
<p>Please leave a comment if you do.</p>
<h2 id="jvm-world">JVM World</h2>
<p>There are products in JVM world which claim to solve my problem perfectly.</p>
<p>Apache Kafka was the inspiration for Event Hubs log-based messaging. The recent
Kafka release provides effectively-once processing semantics as long as
data stay inside Kafka. Kafka does that with atomic publishing to multiple
topics, and state storage based on compacted topics.</p>
<p>Apache Flink has similar guarantees for its stream processing APIs.</p>
<p>Great, but how do I get such awesomeness in .NET code, and without installing 
expensive ZooKeeper-managed clusters?</p>
<h2 id="call-for-feedback">Call for Feedback</h2>
<p>Do you know a solution, product or service?</p>
<p>Have you developed effectively-once processing on .NET / Azure stack?</p>
<p>Are you in touch with somebody who works on such framework?</p>
<p>Please leave a comment, or ping me on Twitter.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Azure Functions: Are They Really Infinitely Scalable and Elastic?</title>
        <link href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/"/>
        <updated>2017-08-31T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-08-31,/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/</id>
        <content type="html"><![CDATA[<p><em>Updated results are available at 
<a href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/">Azure Functions Get More Scalable and Elastic</a>.</em></p>
<p>Automatic elastic scaling is a built-in feature of Serverless computing
paradigm. One doesn&#39;t have to provision servers anymore, they just need to
write code that will be provisioned on as many servers as needed based on the
actual load. That&#39;s the theory.</p>
<p>In particular, Azure Functions can be hosted on the Consumption plan:</p>
<blockquote>
<p>The Consumption plan automatically allocates compute power when your 
code is running, scales out as necessary to handle load, and then scales 
down when code is not running.</p>
</blockquote>
<p>In this post I will run a simple stress test to get a feel of how such
automatic allocation works in practice and what kind of characteristics 
we can rely on.</p>
<h2 id="setup">Setup</h2>
<p>Here are the parameters that I chose for my test of today:</p>
<ul>
<li>Azure Function written in C# and hosted on Consumption plan</li>
<li>Triggered by Azure Storage Queue binding</li>
<li>Workload is strictly CPU-bound, no I/O is executed</li>
</ul>
<p>Specifically, each queue item represents one password that I need to hash.
Each function call performs 12-round <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hashing. Bcrypt is a slow algorithm recommended for
password hashing, because it makes potential hash collision attacks really 
hard and costly.</p>
<p>My function is based on <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.Net</a>
implementation, and it&#39;s extremely simple:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Run</span>(<span class="hljs-params">[QueueTrigger(<span class="hljs-string">"bcrypt-password"</span></span>)] <span class="hljs-keyword">string</span> password)
</span>{
    BCrypt.Net.BCrypt.HashPassword(password, <span class="hljs-number">12</span>);
}
</code></pre>
<p>It turns out that a single execution of this function takes approximately
1 second on an instance of Consumption plan, and consumes 100% CPU during
that second.</p>
<p>Now, the challenge is simple. I send 100,000 passwords
to the queue and see how long it will take to hash them, and also how the
autoscaling will behave. I will run it two times, with different pace of
sending messages to the queue.</p>
<p>That sounds like a perfect job for a Function App on Consumption plan:</p>
<ul>
<li>Needs to scale based on load</li>
<li>CPU intensive - easy to see how busy each server is</li>
<li>Queue-based - easy to see the incoming vs outgoing rate</li>
</ul>
<p>Let&#39;s see how it went.</p>
<h2 id="experiment-1-steady-load">Experiment 1: Steady Load</h2>
<p>In my first run, I was sending messages at constant rate. 100,000 messages
were sent within 2 hours, without spikes or drops in the pace.</p>
<p>Sounds like an easy job for autoscaling facilities. But here is the actual 
chart of data processing:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//FunctionAppScaling.png" alt="Function App Scaling"></p>
<p>The horizontal axis is time in minutes since the first message came in.</p>
<p>The orange line shows the queue backlog - the amount of messages sitting in
the queue at a given moment.</p>
<p>The blue area represents the amount of instances (virtual servers) allocated
to the function by Azure runtime (see the numbers at the right side).</p>
<p>We can divide the whole process into 3 logical segments, approximately 
40 minutes each:</p>
<p><strong>Laging behind</strong>. Runtime starts with 0 instances, and immediately switches
to 1 when the first message comes in. However it&#39;s reluctant to add any more
servers for the next 20 (!) minutes. The scaling heuristic is probably based
on the past history for this queue/function, and it wasn&#39;t busy at all during
the hours before.</p>
<p>After 20 minutes, the runtime starts adding more instances: it goes up to 2, 
then jumps to 4, then reaches 5 at minute 40. The CPU is constantly at 
100% and the queue backlog grows linearly.</p>
<p><strong>Rapid scale up</strong>. After minute 40, it looks like the runtime realizes 
that it needs more power. Much more power! The growth speeds up real quick
and by minute 54 the backlog stops growing, even though the messages are still
coming in. But there are now 21 instances working, which is enough to
finally match and beat the rate of incoming messages.</p>
<p>The runtime doesn&#39;t stop growing though. CPU&#39;s are still at 100%, and the backlog
is still very high, so the scaling goes up and up. The amount of instances
reaches astonishing 55, at which point all the backlog is processed and
there are no messages in the queue.</p>
<p><strong>Searching for balance</strong>. When queue is almost empty and CPU drops below
100% for the first time, the runtime decides to scale down. It does that quickly
and aggressively, switching from 55 to 21 instances in just 2 minutes.</p>
<p>From there it keeps slowly reducing the number of instances until the backlog 
starts growing again. The runtime allows the backlog to grow a bit, but
then figures out a balanced number of servers (17) to keep the backlog flat 
at around 2,000 messages. </p>
<p>It stays at 17 until the producer stops sending new messages. The backlog 
goes to 0, and the amount of instances gradually drops to 0 within 10 minutes.</p>
<p>The second chart from the same experiment looks very similar, but it shows
different metrics:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//FunctionAppDelay.png" alt="Function App Delay"></p>
<p>The gray line is the delay in minutes since the currently processed message
got enqueued (message &quot;age&quot;, in-queue latency). The blue line is the 
total processing rate, measured in messages per minute.</p>
<p>Due to perfect scalability and stability of my function, both charts are almost
exactly the same. I&#39;ve put it here so that you could see that the slowest
message spent more than 40 minutes sitting inside the queue.</p>
<h2 id="experiment-2-spiky-load">Experiment 2: Spiky Load</h2>
<p>With the second run, I tried to emulate a spiky load profile. I was sending
my 100,000 messages throughout 6 hours at lower pace than during the first
run. But sometimes the producer switched to fast mode and sent a bigger bunch
of messages in just several minutes. Here is the actual chart of incoming
message rate:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//SpikyLoad.png" alt="Spiky Load"></p>
<p>It&#39;s easy to imagine some service which has a usage pattern like that, when
spikes of the events happen from time to time, or in rush hours.</p>
<p>This is how the Function App managed to process the messages:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//SpikyLoadProcessing.png" alt="Spiky Load Processing Result"></p>
<p>The green line still shows the amount of incoming messages per minute. The 
blue line denotes how many messages were actually processed at that minute.
And the orange bars are queue backlogs - the amount of messages pending.</p>
<p>Here are several observations:</p>
<ul>
<li><p>Obviously, processing latency is way too far from real time. There is
constantly quite a significant backlog in the queue, and processing delay
reaches 20 minutes at peak.</p>
</li>
<li><p>It took the runtime 2 hours to clean the backlog for the first time. Even
without any spikes during the first hour, the autoscaling algorithm needs
time to get up to speed.</p>
</li>
<li><p>Function App runtime is able to scale up quite fast (look at the reaction
on the fourth spike), but it&#39;s not really willing to do that most of the time.</p>
</li>
<li><p>The growth of the backlog after minute 280 is purely caused by wrong
decision of runtime. While the load is completely steady, the runtime
decided to shut down most workers after 20 minutes of empty backlog, and could
not recover for the next hour.</p>
</li>
</ul>
<h2 id="conclusions">Conclusions</h2>
<p>I tried to get a feeling about the ability of Azure Functions to scale
on demand, adapting to the workload. The function under test was purely CPU-bound,
and for that I can give two main conclusions:</p>
<ul>
<li><p>Function Apps are able to scale to high amount of instances running at the
same time, and to eventually process large parallel jobs (at least up to 55
instances).</p>
</li>
<li><p>Significant processing delays are to be expected for heavy loads. Function
App runtime has quite some inertia, and the resulting processing latency can
easily go up to tens of minutes.</p>
</li>
</ul>
<p>If you know how these results can be improved, or why they are less than 
optimal, please leave a comment or contact me directly.</p>
<p>I look forward to conducting more tests in the future!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Authoring a Custom Binding for Azure Functions</title>
        <link href="https://mikhail.io/2017/07/authoring-custom-binding-azure-functions/"/>
        <updated>2017-07-26T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-26,/2017/07/authoring-custom-binding-azure-functions/</id>
        <content type="html"><![CDATA[<p>In my <a href="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/">previous post</a>
I described how I used Durable Functions extensions
in Azure Function App. Durable Functions are using several binding types
that are not part of the standard suite: <code>OrchestrationClient</code>,
<code>OrchestrationTrigger</code>, <code>ActivityTrigger</code>. These custom bindings 
<a href="https://azure.github.io/azure-functions-durable-extension/articles/installation.html">are installed</a>
by copying the corresponding assemblies to a special Extensions folder.</p>
<p>Although Bring-Your-Own-Binding (BYOB) feature hasn&#39;t been released yet, I
decided to follow the path of Durable Functions and create my own 
custom binding.</p>
<h2 id="configuration-binding">Configuration Binding</h2>
<p>I&#39;ve picked a really simple use case for my first experiments with custom
bindings: reading configuration values.</p>
<p>Azure Functions store their configuration values in App Settings (local
runtime uses <code>local.settings.json</code> file for that).</p>
<p>That means, when you need a configuration value inside your C# code,
you normally do</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">string</span> setting = ConfigurationManager.AppSettings[<span class="hljs-string">"MySetting"</span>];
</code></pre>
<p>Alternatively, <code>Environment.GetEnvironmentVariable()</code> method can be used.</p>
<p>When I <a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">needed to collect</a> 
service bus subscription metrics, I wrote this kind of bulky code:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> resourceToScale = ConfigurationManager.AppSettings[<span class="hljs-string">"ResourceToScale"</span>];

<span class="hljs-keyword">var</span> connectionString = ConfigurationManager.AppSettings[<span class="hljs-string">"ServiceBusConnection"</span>];
<span class="hljs-keyword">var</span> topic = ConfigurationManager.AppSettings[<span class="hljs-string">"Topic"</span>];
<span class="hljs-keyword">var</span> subscription = ConfigurationManager.AppSettings[<span class="hljs-string">"Subscription"</span>];
</code></pre>
<p>The code is no rocket science, but it&#39;s tedious to write, so instead I came
up with this idea to define Functions:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">MyFunction</span>(<span class="hljs-params">
    [TimerTrigger(<span class="hljs-string">"0 */1 * * * *"</span></span>)] TimerInfo timer,
    [<span class="hljs-title">Configuration</span>(<span class="hljs-params">Key = <span class="hljs-string">"ResourceToScale"</span></span>)] <span class="hljs-keyword">string</span> resource,
    [Configuration] ServiceBusSubscriptionConfig config)
</span></code></pre>
<p>Note two usages of <code>Configuration</code> attribute. The first one defines the 
specific configuration key, and binds its value to a string parameter. The 
other one binds <em>multiple</em> configuration values to a POCO parameter. I defined
the config class as</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ServiceBusSubscriptionConfig</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">ServiceBusSubscriptionConfig</span>(<span class="hljs-params"><span class="hljs-keyword">string</span> serviceBusConnection, <span class="hljs-keyword">string</span> topic, <span class="hljs-keyword">string</span> subscription</span>)
    </span>{
        ServiceBusConnection = serviceBusConnection;
        Topic = topic;
        Subscription = subscription;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> ServiceBusConnection { <span class="hljs-keyword">get</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Topic { <span class="hljs-keyword">get</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Subscription { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>The immutable class is a bit verbose, but I still prefer it over get-set
container in this scenario.</p>
<p>The binding behavior is convention-based in this case: the binding engine
should load configuration values based on the names of class properties.</p>
<h2 id="motivation">Motivation</h2>
<p>So, why do I need such binding?</p>
<p>As I said, it&#39;s a simple use case to play with BYOB feature, and overall,
<strong>understand</strong> the internals of Function Apps a bit better.</p>
<p>But apart from that, I removed 4 lines of garbage from the function body
(at the cost of two extra parameters). <strong>Less noise</strong> means more readable code,
especially when I put this code on a webpage.</p>
<p>As a bonus, the <strong>testability</strong> of the function immediately increased. It&#39;s so
much easier for the test just to accept the configuration as input parameter,
instead of fine-tuning the configuration files inside test projects, or
hiding <code>ConfigurationManager</code> usage behind a mockable facade.</p>
<p>Such approach does seem to be the strength of Azure Functions code in
general. It&#39;s often possible to reduce imperative IO-related code to 
attribute-decorated function parameters.</p>
<h2 id="implementing-a-custom-binding">Implementing a Custom Binding</h2>
<p>The actual implementation process of a custom non-trigger binding is quite
simple:</p>
<p><strong>Create a class library</strong> with the word &quot;Extension&quot; in its name. Import
<code>Microsoft.Azure.WebJobs</code> and <code>Microsoft.Azure.WebJobs.Extensions</code> NuGet
packages (at the time of writing I used <code>2.1.0-beta1</code> version).</p>
<p><strong>Define</strong> a class for binding attribute:</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">AttributeUsage(AttributeTargets.Parameter)</span>]
[<span class="hljs-meta">Binding</span>]
<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ConfigurationAttribute</span> : <span class="hljs-title">Attribute</span>
{
    [<span class="hljs-meta">AutoResolve</span>]
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Key { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
}
</code></pre>
<p>The attribute is marked as <code>Binding</code> and the <code>Key</code> property is marked as
resolvable from <code>function.json</code>.</p>
<p><strong>Implement</strong> <code>IExtensionConfigProvider</code> which will tell the function runtime
how to use your binding correctly.</p>
<p>The interface has just one method to implement:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ConfigurationExtensionConfigProvider</span> : <span class="hljs-title">IExtensionConfigProvider</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Initialize</span>(<span class="hljs-params">ExtensionConfigContext context</span>)
    </span>{
        <span class="hljs-comment">// ... see below</span>
    }
}
</code></pre>
<p>The first step of the implementation is to define a rule for our new
<code>ConfigurationAttribute</code> and tell this rule how to get a string value out
of any attribute instance:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> rule = context.AddBindingRule&lt;ConfigurationAttribute&gt;();
rule.BindToInput&lt;<span class="hljs-keyword">string</span>&gt;(a =&gt; ConfigurationManager.AppSettings[a.Key]);
</code></pre>
<p>That&#39;s really all that needs to happen to bind <code>string</code> parameters.</p>
<p>To make our binding work with any POCO, we need a more elaborate construct:</p>
<pre class="highlight"><code class="hljs cs">rule.BindToInput&lt;Env&gt;(_ =&gt; <span class="hljs-keyword">new</span> Env());
<span class="hljs-keyword">var</span> cm = context.Config.GetService&lt;IConverterManager&gt;();
cm.AddConverter&lt;Env, OpenType, ConfigurationAttribute&gt;(<span class="hljs-keyword">typeof</span>(PocoConverter&lt;&gt;));
</code></pre>
<p>I instruct the rule to bind to my custom class <code>Env</code>, and then I say that
this class <code>Env</code> is convertable to any type (denoted by special <code>OpenType</code>
type argument) with a generic converter called <code>PocoConverter</code>.</p>
<p>The <code>Env</code> class is a bit dummy (it exists just because I need <em>some</em> class):</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">private</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Env</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> <span class="hljs-title">GetValue</span>(<span class="hljs-params"><span class="hljs-keyword">string</span> key</span>) </span>=&gt; ConfigurationManager.AppSettings[key];
}
</code></pre>
<p>And <code>PocoConverter</code> is a piece of reflection, that loops through property
names and reads configuration values out of them. Then it calls a constructor
which matches the property count:</p>
<pre class="highlight"><code class="hljs undefined">private class PocoConverter&lt;T&gt; : IConverter&lt;Env, T&gt;
{
    public T Convert(Env env)
    {
        var values = typeof(T)
            .GetProperties()
            .Select(p =&gt; p.Name)
            .Select(env.GetValue)
            .Cast&lt;object&gt;()
            .ToArray();

        var constructor = typeof(T).GetConstructor(values.Select(v =&gt; v.GetType()).ToArray());
        if (constructor == null)
        {
            throw new Exception("We tried to bind to your C# class, but it looks like there's no constructor which accepts all property values");
        }

        return (T)constructor.Invoke(values);
    }
}
</code></pre>
<p>This piece of code is not particularly robust, but it is good enough to
illustrate the concept.</p>
<p>And that&#39;s it, the binding it ready! You can find the complete example in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/tree/master/custom-binding-azure-functions">my github repo</a>.</p>
<h2 id="deploying-custom-bindings">Deploying Custom Bindings</h2>
<p>Since BYOB feature is in early preview, there is no tooling for automated
deployment, and we need to do everything manually. But the process is not
too sophisticated:</p>
<ol>
<li><p>Create a folder for custom bindings, e.g. <code>D:\BindingExtensions</code>.</p>
</li>
<li><p>Set <code>AzureWebJobs_ExtensionsPath</code> parameter in your app settings
to that folder&#39;s path. For local development add a line to <code>local.settings.json</code>:</p>
<pre class="highlight"><code class="hljs undefined"> "AzureWebJobs_ExtensionsPath": "D:\\BindingExtensions",
</code></pre>
</li>
<li><p>Create a subfolder for your extension, e.g. 
<code>D:\BindingExtensions\ConfigurationExtension</code>.</p>
</li>
<li><p>Copy the contents of <code>bin\Debug\</code> of your extension&#39;s class library
to that folder.</p>
</li>
<li><p>Reference your extension library from your Function App.</p>
</li>
</ol>
<p>You are good to go! Decorate your function parameters with the new attribute.</p>
<p>Run the function app locally to try it out. In the console output you should
be able to see something like</p>
<pre class="highlight"><code class="hljs oxygene">Loaded custom <span class="hljs-keyword">extension</span>: ConfigurationExtensionConfigProvider <span class="hljs-keyword">from</span> 
<span class="hljs-string">'D:\BindingExtensions\ConfigurationExtension\MyExtensions.dll'</span>
</code></pre><p>You will be able to debug your extension if needed.</p>
<h2 id="useful-links">Useful Links</h2>
<p>Use the following links to find out more about custom bindings, see more
examples and walkthroughs, and get fresh updates:</p>
<ul>
<li><a href="https://github.com/Azure/azure-webjobs-sdk/wiki/Extensibility">Extensibility in Azure WebJobs SDK</a></li>
<li><a href="https://github.com/Azure/WebJobsExtensionSamples/tree/master/SampleExtension">Sample Extension for Azure Functions</a>,
<a href="https://github.com/Azure/WebJobsExtensionSamples/blob/master/FunctionApp/ReaderFunction.cs">Sample Usage in Precompiled App</a> and
<a href="https://github.com/Azure/WebJobsExtensionSamples/tree/master/ScriptRuntimeSample/Reader">Sample Usage in Script Runtime</a></li>
<li><a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/src/WebJobs.Extensions.DurableTask">Custom Bindings of Durable Functions</a></li>
<li><a href="https://azure.github.io/azure-functions-durable-extension/articles/installation.html">Installation Guide for Durable Functions</a></li>
</ul>
<p>Have a good binding!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Custom Autoscaling with Durable Functions</title>
        <link href="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/"/>
        <updated>2017-07-24T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-24,/2017/07/custom-autoscaling-with-durable-functions/</id>
        <content type="html"><![CDATA[<p>In my previous post 
<a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">Custom Autoscaling of Azure App Service with a Function App</a>
I&#39;ve created a Function App which watches a Service Bus Subscription
backlog and adjusts the scale of App Service based on the observed load.</p>
<p>It works fine but there are two minor issues that I would like to address
in this article:</p>
<ul>
<li><p><strong>Scaling Logic</strong> function from that workflow needs to preserve state
between calls. I used Table Storage bindings for that, which proved to
be a bit verbose and low level: I needed to manage conversion to entity and 
JSON serialization myself;</p>
</li>
<li><p>There is no feedback from <strong>Scaler</strong> function (which executes the change)
back to <strong>Scaling Logic</strong> function. Thus, if scaling operation is slow or
fails, there is no easy way to notify the logic about that.</p>
</li>
</ul>
<p>Let&#39;s see how these issues can be solved with Azure Durable Functions.</p>
<h2 id="meet-durable-functions">Meet Durable Functions</h2>
<p>Microsoft has recently announced the preview of 
<a href="https://azure.github.io/azure-functions-durable-extension/">Durable Functions</a>:</p>
<blockquote>
<p>Durable Functions is an Azure Functions extension for building long-running, 
stateful function orchestrations in code using C# in a serverless environment.</p>
</blockquote>
<p>The library is built on top of <a href="https://github.com/Azure/durabletask">Durable Task Framework</a>
and introduces several patterns for Function coordination and stateful
processing. Please go read the <a href="https://azure.github.io/azure-functions-durable-extension/">documentation</a>,
it&#39;s great and has some very useful examples.</p>
<p>I decided to give Durable Functions a try for my autoscaling workflow. Feel
free to refer to <a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">the first part</a>
to understand my goals and the previous implementation.</p>
<h2 id="architecture">Architecture</h2>
<p>The flow of metric collection, scaling logic and scaling action stays the
same. The state and cross-function communication aspects are now delegated
to Durable Functions, so the diagram becomes somewhat simpler:</p>
<p><img src="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions//AutoscalingArchitecture.png" alt="Autoscaling Architecture"></p>
<p>The blue sign on <strong>Scaling Logic</strong> function denotes its statefulness.</p>
<p>Let&#39;s walk through the functions implementation to see how the workflow
plays out.</p>
<p>This time I&#39;ll start with <strong>Scaler</strong> function and then flow from right to left
to make the explanation more clear.</p>
<h2 id="scaler">Scaler</h2>
<p><strong>Scaler</strong> function applies the scaling decisions to the Azure resource, App
Service Plan in my case. I&#39;ve extracted App Service related code to a helper, 
to keep the function minimal and clean. You can see the full code in 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/durable-functions/DurableScaling.cs">my github repo</a>.</p>
<p><strong>Scaler</strong> function is triggered by Durable Function&#39;s <code>ActivityTrigger</code>. That
basically means that it&#39;s ready to be called from other functions. Here is
the code:</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">FunctionName(nameof(Scaler))</span>]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> <span class="hljs-title">Scaler</span>(<span class="hljs-params">[ActivityTrigger] DurableActivityContext context</span>)
</span>{
    <span class="hljs-keyword">var</span> action = context.GetInput&lt;ScaleAction&gt;();

    <span class="hljs-keyword">var</span> newCapacity = ScalingHelper.ChangeAppServiceInstanceCount(
        action.ResourceName,
        action.Type == ScaleActionType.Down ? <span class="hljs-number">-1</span> : +<span class="hljs-number">1</span>);

    <span class="hljs-keyword">return</span> newCapacity;
}
</code></pre>
<p>In order to receive an input value, I utilize <code>context.GetInput()</code> method.
I believe that the team is working on support of custom classes 
(<code>ScaleAction</code> in my case) directly as function parameters.</p>
<p>The function then executes the scale change and returns back the new capacity
of App Service Plan. Note that this is new: we were not able to return
values in the previous implementation.</p>
<h2 id="scaling-logic">Scaling Logic</h2>
<p><strong>Scaling Logic</strong> is using <a href="https://azure.github.io/azure-functions-durable-extension/articles/samples/counter.html">Stateful Actor pattern</a>.
One instance of such actor is created for each scalable resource (I only use
1 now). Here is the implementation (again, simplified for readability):</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">FunctionName(nameof(ScalingLogic))</span>]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">async</span> Task&lt;ScalingState&gt; <span class="hljs-title">ScalingLogic</span>(<span class="hljs-params">
    [OrchestrationTrigger] DurableOrchestrationContext context, 
    TraceWriter log</span>)
</span>{
    <span class="hljs-keyword">var</span> state = context.GetInput&lt;ScalingState&gt;();

    <span class="hljs-keyword">var</span> metric = <span class="hljs-keyword">await</span> context.WaitForExternalEvent&lt;Metric&gt;(<span class="hljs-keyword">nameof</span>(Metric));

    UpdateHistory(state.History, metric.Value);
    ScaleAction action = CalculateScalingAction(state);

    <span class="hljs-keyword">if</span> (action != <span class="hljs-literal">null</span>)
    {
        <span class="hljs-keyword">var</span> result = <span class="hljs-keyword">await</span> context.CallFunctionAsync&lt;<span class="hljs-keyword">int</span>&gt;(<span class="hljs-keyword">nameof</span>(Scaler), action);
        log.Info(<span class="hljs-string">$"Scaling logic: Scaled to <span class="hljs-subst">{result}</span> instances."</span>);
        state.LastScalingActionTime = context.CurrentUtcDateTime;
    }

    context.ContinueAsNew(state);
    <span class="hljs-keyword">return</span> state;
}
</code></pre>
<p>Here is how it works:</p>
<ul>
<li><p>Function is bound to <code>OrchestrationTrigger</code>, yet another trigger type from
Durable Functions;</p>
</li>
<li><p>It loads durable state from the received <code>context</code>;</p>
</li>
<li><p>It then waits for an external event called Metric (to be sent by <strong>Collector</strong>
function, see the next section);</p>
</li>
<li><p>When an event is received, the function updates its state and calculates
if a scaling action is warranted;</p>
</li>
<li><p>If yes, it calls <strong>Scaler</strong> function and sends the scale action. It expects
an integer result, denoting the new amount of instances;</p>
</li>
<li><p>It then calls <code>ContinueAsNew</code> method to start a new iteration of the actor
loop, providing the updated state.</p>
</li>
</ul>
<p>One important note: the orchestrated function 
<a href="https://azure.github.io/azure-functions-durable-extension/articles/topics/checkpointing-and-replay.html">has to be deterministic</a>. 
That means, for example, that <code>DateTime.Now</code> is not allowed to be used. 
I use <code>context.CurrentUtcDateTime</code> instead for time-related calculations.</p>
<p>The implementation of this function solves both problems that I mentioned 
in the introduction. We do not manage state storage and serialization manually,
and we now have the ability to get feedback from <strong>Scaler</strong> function.</p>
<h2 id="metrics-collector">Metrics Collector</h2>
<p>I&#39;ve extracted Service Bus related code to a helper to keep the code sample
minimal and clean. You can see the full code in 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/durable-functions/DurableScaling.cs">my github repo</a>.</p>
<p>Here is the remaining implementation of <strong>Metric Collector</strong>:</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">FunctionName(nameof(MetricCollector))</span>]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">async</span> Task <span class="hljs-title">MetricCollector</span>(<span class="hljs-params">
    [TimerTrigger(<span class="hljs-string">"0 */1 * * * *"</span></span>)] TimerInfo myTimer,
    [OrchestrationClient] DurableOrchestrationClient client,
    TraceWriter log)
</span>{
    <span class="hljs-keyword">var</span> resource = Environment.GetEnvironmentVariable(<span class="hljs-string">"ResourceToScale"</span>);

    <span class="hljs-keyword">var</span> status = <span class="hljs-keyword">await</span> client.GetStatusAsync(resource);
    <span class="hljs-keyword">if</span> (status == <span class="hljs-literal">null</span>)
    {
        <span class="hljs-keyword">await</span> client.StartNewAsync(<span class="hljs-keyword">nameof</span>(ScalingLogic), resource, <span class="hljs-keyword">new</span> ScalingState());
    }
    <span class="hljs-keyword">else</span>
    {
        <span class="hljs-keyword">var</span> metric = ServiceBusHelper.GetSubscriptionMetric(resource);
        log.Info(<span class="hljs-string">$"Collector: Current metric value is <span class="hljs-subst">{metric.Value.Value}</span> at <span class="hljs-subst">{DateTime.Now}</span>"</span>);
        <span class="hljs-keyword">await</span> client.RaiseEventAsync(resource, <span class="hljs-keyword">nameof</span>(Metric), metric);
    }
}
</code></pre>
<p>It&#39;s still a timer-triggered &quot;normal&quot; (non-durable) function, but now it 
also has an additional binding to <code>OrchestrationClient</code>. This client is used 
to communicate metric data to the <strong>Scaling Logic</strong>.</p>
<p>With the current implementation, <strong>Metric Collector</strong> also has a second
responsibility: actor instance management. At every iteration, it queries
for the current status of corresponding actor. If that is <code>null</code>, Collector
creates a new instance with initial empty state.</p>
<p>To my liking, this aspect is a bit unfortunate, but it seems to be required
with the current implementation of Durable Functions framework. See 
<a href="https://github.com/Azure/azure-functions-durable-extension/issues/21">my related question on github</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>I adjusted the initial flow of autoscaling functions to use Durable Functions
library. It made the state management look more straightforward, and also
allowed direct communication between two functions in strongly-typed
request-response manner.</p>
<p>The resulting code is relatively clear and resembles the typical structure
of async-await code that C# developers are used to.</p>
<p>There are some downsides that I found about Durable Functions too:</p>
<ul>
<li><p>This is a very early preview, so there are some implementation issues.
A couple times I managed to put my functions into a state where they were stuck
and no calls could be made anymore. The only way I could get out of there is by
clearing some blobs in the Storage Account;</p>
</li>
<li><p>The actor instance management story feels raw. The function, which needs to
send events to actors, has to manage their lifecycle and instance IDs. I would
need to add some more checks to make the code production ready, e.g. to 
restart actors if they end up in faulty state;</p>
</li>
<li><p>There are some concurrency issues in function-to-function communication
to be resolved;</p>
</li>
<li><p>Some discipline is required to keep Durable functions side-effect free
and deterministic. The multiple executions caused by awaits and replays are
counter-intuitive (at least for novice devs), and thus error-prone.</p>
</li>
</ul>
<p>Having said that, I believe Durable Functions can be a very useful abstraction
to simplify some of the more advanced scenarios and workflows. I look
forward to further iterations of the library, and I will keep trying it out
for more scenarios.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Custom Autoscaling of Azure App Service with a Function App</title>
        <link href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/"/>
        <updated>2017-07-17T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-17,/2017/07/custom-auto-scaling-in-azure/</id>
        <content type="html"><![CDATA[<p>The power of cloud computing comes from its elasticity and ability to adapt to changing
load. Most Azure services can be scaled up or down manually: by human interaction in the
portal, or by running a command or a script.</p>
<p>Some services in Azure also support Autoscaling, i.e. they may change the resource 
allocation dynamically, based on predefined rules and current operational metrics.</p>
<p>Azure App Service is one example of such service: it supports 
<a href="https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/insights-how-to-scale#scaling-based-on-a-pre-set-metric">Scaling based on a pre-set metric</a>.
This is a powerful option that enables website or webjobs to react on varying load,
e.g. based on CPU utilization.</p>
<p>At the same time, the flexibility of the built-in autoscaling is somewhat
limited:</p>
<ul>
<li><p>Only a handful of metrics is supported: for instance, Service Bus Queues 
are supported as metric source, while Service Bus Subscriptions are not;</p>
</li>
<li><p>It&#39;s not possible to combine several metrics in one rule: e.g. scale down only if
several queues are empty at the same time, not just one of them;</p>
</li>
<li><p>Thresholds are the same for any number of instances: I can&#39;t define
a scale down rule threshold to be 60% for 8 instances but 30% for 2 instances;</p>
</li>
<li><p>The minimum time of reaction is limited to 5 minutes.</p>
</li>
</ul>
<p>Other services, like SQL Database and Cosmos DB, don&#39;t have the built-in autoscaling
functionality at all.</p>
<p>This post starts the series of articles about custom implementation 
of autoscaling. The implementation will be based on Azure Functions as building 
blocks of scaling workflows.</p>
<h2 id="goal">Goal</h2>
<p>To keep the task very specific for now, I want the following from my first 
custom autoscaling implementation:</p>
<ul>
<li><p>Be able to scale the amount of instances up and down in a given App Service 
Plan;</p>
</li>
<li><p>Do so based on the given Service Bus Subscription backlog (amount of messages 
pending to be processed);</p>
</li>
<li><p>Scale up, if the average backlog during any 10 minutes is above a threshold;</p>
</li>
<li><p>Scale down, if the maximum backlog during any 10 minutes is below another 
(lower) threshold;</p>
</li>
<li><p>After scaling up or down, take a cooldown period of 10 minutes;</p>
</li>
<li><p>Have a log of scaling decisions and numbers behind;</p>
</li>
<li><p>Scaling rules should be extensible to allow more complex calculation later 
on.</p>
</li>
</ul>
<h2 id="architecture">Architecture</h2>
<p>I decided that the scaling rules should be written in a general-purpose programming language
(C# for this post), instead of just picking from a limited list of configurations.</p>
<p>I chose Azure Functions as the mechanism to host and run this logic in Azure cloud. </p>
<p>Here is a diagram of Functions that I ended up creating:</p>
<p><img src="https://mikhail.io/2017/07/custom-auto-scaling-in-azure//AutoscalingArchitecture.png" alt="Autoscaling Architecture"></p>
<p>The components of my autoscaling app are:</p>
<ul>
<li><p><strong>Metric Collector</strong> function is based on Timer trigger: it fires every minute and collects
the subscription backlog metric from a given Service Bus Subscription;</p>
</li>
<li><p>Collector then sends this metric to the <strong>Metrics</strong> storage queue;</p>
</li>
<li><p><strong>Scaling Logic</strong> function pulls the metric from the queue. It maintains the 
metric values for 10 minutes, calculates average/maximum value, and if they hit 
thresholds - issues a command to scale App Service Plan up or down;</p>
</li>
<li><p>The command is sent to <strong>Actions</strong> storage queue;</p>
</li>
<li><p><strong>Scaler</strong> function receives the commands from the queue and executes 
the re-scaling action on App Service Plan using Azure Management SDK.</p>
</li>
</ul>
<p>The implementation of this workflow is discussed below. I am using Visual Studio 2017 Version 15.3 
Preview 4.0 to author pre-compiled Azure Functions with nice built-in tooling.</p>
<h2 id="metric-collector">Metric Collector</h2>
<p>First, let&#39;s define <code>MetricValue</code> class, which simply holds time and value:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">MetricValue</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">MetricValue</span>(<span class="hljs-params">DateTime time, <span class="hljs-keyword">int</span> <span class="hljs-keyword">value</span></span>)
    </span>{
        <span class="hljs-keyword">this</span>.Time = time;
        <span class="hljs-keyword">this</span>.Value = <span class="hljs-keyword">value</span>;
    }

    <span class="hljs-keyword">public</span> DateTime Time { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> Value { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>and <code>Metric</code> class which extends the value with resource name (e.g. App Service
Plan name) and measured parameter name:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Metric</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Metric</span>(<span class="hljs-params"><span class="hljs-keyword">string</span> resourceName, <span class="hljs-keyword">string</span> name, MetricValue <span class="hljs-keyword">value</span></span>)
    </span>{
        <span class="hljs-keyword">this</span>.ResourceName = resourceName;
        <span class="hljs-keyword">this</span>.Name = name;
        <span class="hljs-keyword">this</span>.Value = <span class="hljs-keyword">value</span>;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> ResourceName { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Name { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> MetricValue Value { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>The function definition has two associated bindings: timer trigger (runs every
minute) and return binding to the storage queue:</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">FunctionName(<span class="hljs-meta-string">"MetricCollector"</span>)</span>]
[<span class="hljs-meta">return: Queue(<span class="hljs-meta-string">"Metrics"</span>)</span>]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> Metric <span class="hljs-title">MetricCollector</span>(<span class="hljs-params">[TimerTrigger(<span class="hljs-string">"0 */1 * * * *"</span></span>)] TimerInfo myTimer, TraceWriter log)
</span>{
    <span class="hljs-keyword">var</span> connectionString = Environment.GetEnvironmentVariable(<span class="hljs-string">"ServiceBusConnection"</span>);
    <span class="hljs-keyword">var</span> topic = Environment.GetEnvironmentVariable(<span class="hljs-string">"Topic"</span>);
    <span class="hljs-keyword">var</span> subscription = Environment.GetEnvironmentVariable(<span class="hljs-string">"Subscription"</span>);

    <span class="hljs-keyword">var</span> nsmgr = NamespaceManager.CreateFromConnectionString(connectionString);
    <span class="hljs-keyword">var</span> subscriptionClient = nsmgr.GetSubscription(topic, subscription);
    <span class="hljs-keyword">var</span> backlog = subscriptionClient.MessageCountDetails.ActiveMessageCount;

    log.Info(<span class="hljs-string">$"Collector: Current metric value is <span class="hljs-subst">{backlog}</span>"</span>);

    <span class="hljs-keyword">var</span> resource = Environment.GetEnvironmentVariable(<span class="hljs-string">"ResourceToScale"</span>);
    <span class="hljs-keyword">var</span> <span class="hljs-keyword">value</span> = <span class="hljs-keyword">new</span> MetricValue(DateTime.Now, (<span class="hljs-keyword">int</span>)backlog);
    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Metric(resource, <span class="hljs-string">$"<span class="hljs-subst">{topic}</span>-<span class="hljs-subst">{subscription}</span>-backlog"</span>, <span class="hljs-keyword">value</span>);
}
</code></pre>
<p>The function executes the following steps:</p>
<ul>
<li>Reads configuration value for Service Bus parameters;</li>
<li>Connects to Service Bus and retrieves <code>ActiveMessageCount</code> for the given 
subscription;</li>
<li>Logs the value for tracing and debugging;</li>
<li>Returns the metric value mentioning which resource it&#39;s intended for.</li>
</ul>
<h2 id="scaling-logic">Scaling Logic</h2>
<p>The core of autoscaling implementation resides in <code>ScalingLogic</code> function. </p>
<p>The function defines 4 (oh my!) bindings:</p>
<ul>
<li>Queue trigger to react on messages from the collector;</li>
<li>Output queue binding to send commands with action to execute;</li>
<li>Combination of input and output bindings to the same row in Table Storage to 
keep the state in between function calls.</li>
</ul>
<p>The bindings are illustrated on the following picture:</p>
<p><img src="https://mikhail.io/2017/07/custom-auto-scaling-in-azure//ScalingLogicBindings.png" alt="Binding of Scaling Logic Function"></p>
<p>And here is the corresponding Function signature:</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">FunctionName(<span class="hljs-meta-string">"ScalingLogic"</span>)</span>]
[<span class="hljs-meta">return: Queue(<span class="hljs-meta-string">"Actions"</span>)</span>]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> ScaleAction <span class="hljs-title">ScalingLogic</span>(<span class="hljs-params">
    [QueueTrigger(<span class="hljs-string">"Metrics"</span></span>)] Metric metric, 
    [<span class="hljs-title">Table</span>(<span class="hljs-params"><span class="hljs-string">"Scaling"</span>, <span class="hljs-string">"{ResourceName}"</span>, <span class="hljs-string">"{Name}"</span></span>)] ScalingStateEntity stateEntity, 
    [<span class="hljs-title">Table</span>(<span class="hljs-params"><span class="hljs-string">"Scaling"</span>, <span class="hljs-string">"{ResourceName}"</span>, <span class="hljs-string">"{Name}"</span></span>)] <span class="hljs-keyword">out</span> ScalingStateEntity newStateEntity,
    TraceWriter log)
</span></code></pre>
<p>Table storage is partitioned per scalable resource, and state is stored per metric;
thus multiple resources and metrics are supported out of the box.</p>
<p>The function implementation is relatively complex, so I&#39;ll describe it in parts.</p>
<p><code>ScaleAction</code> is a simple message class:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">enum</span> ScaleActionType
{
    Up,
    Down
}

<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ScaleAction</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">ScaleAction</span>(<span class="hljs-params"><span class="hljs-keyword">string</span> resourceName, ScaleActionType type</span>)
    </span>{
        <span class="hljs-keyword">this</span>.ResourceName = resourceName;
        <span class="hljs-keyword">this</span>.Type = type;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> ResourceName { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> ScaleActionType Type { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>Table Storage only allows primitive types for its columns, like strings. 
So I had to create a separate Table Storage entity class:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ScalingStateEntity</span> : <span class="hljs-title">TableEntity</span>
{
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> SerializedState { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
}
</code></pre>
<p>which stores serialized state, from the state class itself:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ScalingState</span>
{
    <span class="hljs-keyword">public</span> List&lt;MetricValue&gt; History { <span class="hljs-keyword">get</span>; } = <span class="hljs-keyword">new</span> List&lt;MetricValue&gt;();

    <span class="hljs-keyword">public</span> DateTime LastScalingActionTime { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; } = DateTime.MinValue;
}
</code></pre>
<p>Now let&#39;s look at the function body. It consists of four blocks. </p>
<p>The first block retrieves the previous values of the metric and logs it too:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 1. Deserialize state</span>
<span class="hljs-keyword">var</span> state = stateEntity?.SerializedState != <span class="hljs-literal">null</span> 
    ? JsonConvert.DeserializeObject&lt;ScalingState&gt;(stateEntity.SerializedState) 
    : <span class="hljs-keyword">new</span> ScalingState();
<span class="hljs-keyword">var</span> history = state.History;
log.Info(<span class="hljs-string">$"Scaling logic: Received <span class="hljs-subst">{metric.Name}</span>, previous state is <span class="hljs-subst">{<span class="hljs-keyword">string</span>.Join(<span class="hljs-string">", "</span>, history)}</span>"</span>);
</code></pre>
<p>The second block adds the current metric value and removes all metrics which are
not in the target period of 10 minutes anymore:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 2. Add current metric value, remove old values</span>
history.Add(metric.Value);
history.RemoveAll(e =&gt; e.Time &lt; metric.Value.Time.Substract(period));
</code></pre>
<p>Now, the actual logic finally kicks in and produces the scaling action if average
or maximum value is above or below respective thresholds. For my implementation I also
chose to apply this rule after 5th data point. Cooldown period is also respected:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 3. Compare the aggregates to thresholds, produce scaling action if needed</span>
ScaleAction action = <span class="hljs-literal">null</span>;
<span class="hljs-keyword">if</span> (history.Count &gt;= <span class="hljs-number">5</span>
    &amp;&amp; DateTime.Now - state.LastScalingActionTime &gt; cooldownPeriod)
{
    <span class="hljs-keyword">var</span> average = (<span class="hljs-keyword">int</span>)history.Average(e =&gt; e.Value);
    <span class="hljs-keyword">var</span> maximum = (<span class="hljs-keyword">int</span>)history.Max(e =&gt; e.Value);
    <span class="hljs-keyword">if</span> (average &gt; thresholdUp)
    {
        log.Info(<span class="hljs-string">$"Scaling logic: Value <span class="hljs-subst">{average}</span> is too high, scaling <span class="hljs-subst">{metric.ResourceName}</span> up..."</span>);
        state.LastScalingActionTime = DateTime.Now;
        action = <span class="hljs-keyword">new</span> ScaleAction(metric.ResourceName, ScaleActionType.Up);
    }
    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (maximum &lt; thresholdDown)
    {
        log.Info(<span class="hljs-string">$"Scaling logic: Value <span class="hljs-subst">{maximum}</span> is low, scaling <span class="hljs-subst">{metric.ResourceName}</span> down..."</span>);
        state.LastScalingActionTime = DateTime.Now;
        action = <span class="hljs-keyword">new</span> ScaleAction(metric.ResourceName, ScaleActionType.Down);
    }
}
</code></pre>
<p>Finally, the state is serialized back to table entity and action is returned: </p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 4. Serialize the state back and return the action</span>
newStateEntity = stateEntity != <span class="hljs-literal">null</span> 
    ? stateEntity 
    : <span class="hljs-keyword">new</span> ScalingStateEntity { PartitionKey = metric.ResourceName, RowKey = metric.Name };
newStateEntity.SerializedState = JsonConvert.SerializeObject(state);
<span class="hljs-keyword">return</span> action;
</code></pre>
<p>Note, that if no scaling action is warranted, the function simply returns <code>null</code> and no message 
gets sent to the output queue.</p>
<h2 id="scaler">Scaler</h2>
<p>The last function of the workflow is called <code>Scaler</code>: it listens for scaling commands and executes them.
I am using Azure Management Fluent SDK to scale the App Service Plan capacity:</p>
<pre class="highlight"><code class="hljs cs">[<span class="hljs-meta">FunctionName(<span class="hljs-meta-string">"Scaler"</span>)</span>]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Scaler</span>(<span class="hljs-params">[QueueTrigger(<span class="hljs-string">"Actions"</span></span>)] ScaleAction action, TraceWriter log)
</span>{
    <span class="hljs-keyword">var</span> secrets = Environment.GetEnvironmentVariable(<span class="hljs-string">"ServicePrincipal"</span>).Split(<span class="hljs-string">','</span>);
    <span class="hljs-keyword">var</span> credentials = SdkContext.AzureCredentialsFactory
        .FromServicePrincipal(secrets[<span class="hljs-number">0</span>], secrets[<span class="hljs-number">1</span>], secrets[<span class="hljs-number">2</span>], AzureEnvironment.AzureGlobalCloud);
    <span class="hljs-keyword">var</span> azure = Azure.Configure()
        .Authenticate(credentials)
        .WithDefaultSubscription();

    <span class="hljs-keyword">var</span> plan = azure.AppServices
        .AppServicePlans
        .List()
        .First(p =&gt; p.Name.Contains(action.ResourceName));

    <span class="hljs-keyword">var</span> newCapacity = action.Type == ScaleActionType.Down ? plan.Capacity - <span class="hljs-number">1</span> : plan.Capacity + <span class="hljs-number">1</span>;
    log.Info(<span class="hljs-string">$"Scaler: Switching <span class="hljs-subst">{action.ResourceName}</span> from <span class="hljs-subst">{plan.Capacity}</span> <span class="hljs-subst">{action.Type}</span> to <span class="hljs-subst">{newCapacity}</span>"</span>);

    plan.Update()
        .WithCapacity(newCapacity)
        .Apply();
}
</code></pre>
<p>The functionality is pretty straightforward. Here are some links where you can read more about 
<a href="https://docs.microsoft.com/en-us/dotnet/azure/dotnet-sdk-azure-authenticate?view=azure-dotnet#a-namemgmt-authaazure-management-libraries-for-net-authentication">Authentication in Azure Management libraries</a> 
and <a href="https://github.com/Azure-Samples/app-service-dotnet-scale-web-apps">Managing Web App with Fluent SDK</a>.</p>
<h2 id="conclusion-and-further-steps">Conclusion and Further Steps</h2>
<p>This was quite a lot of code for a single blog post, but most of it was
fairly straightforward. You can find the full implemenation in 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/servicebussubscription-to-appserviceplan/MetricCollector.cs">my github</a>.</p>
<p>Overall, I&#39;ve established an application based on Azure Functions, which
watches the predefined metrics and scales the specified resource up and down
based on target metric values.</p>
<p>The current example works only for the combination of Service Bus Subscription
and App Service Plan, but it is clear how to extend it to more scenarios.</p>
<p>The flexibility of such autoscaling solution exceeds the built-in functionality
that is available in Azure Portal.</p>
<p>The most complex part of my Autoscaling application is the Scaling Logic
function. In the next article of the series, I will refactor it to use
<a href="https://azure.github.io/azure-functions-durable-extension/index.html">Durable Functions</a> - 
the upcoming Orchestration framework for Function Apps.</p>
<p>Stay tuned, and happy scaling!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Sending Large Batches to Azure Service Bus</title>
        <link href="https://mikhail.io/2017/07/sending-large-batches-to-azure-service-bus/"/>
        <updated>2017-07-04T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-04,/2017/07/sending-large-batches-to-azure-service-bus/</id>
        <content type="html"><![CDATA[<p>Azure Service Bus client supports sending messages in batches (<code>SendBatch</code>
and <code>SendBatchAsync</code> methods of <code>QueueClient</code> and <code>TopicClient</code>). However,
the size of a single batch must stay below 256k bytes, otherwise the whole
batch will get rejected.</p>
<p>How do we make sure that the batch-to-be-sent is going to fit? The rest 
of this article will try to answer this seemingly simple question.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Given a list of messages of arbitrary type <code>T</code>, we want to send them to Service
Bus in batches. The amount of batches should be close to minimal, but
obviously each one of them must satisfy the restriction of 256k max size.</p>
<p>So, we want to implement a method with the following signature:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages);
</code></pre>
<p>which would work for collections of any size.</p>
<p>To limit the scope, I will restrict the article to the following assumptions:</p>
<ul>
<li><p>Each individual message is less than 256k serialized. If that wasn&#39;t true,
we&#39;d have to put the body into external blob storage first, and then send
the reference. It&#39;s not directly related to the topic of discussion.</p>
</li>
<li><p>I&#39;ll use <code>public BrokeredMessage(object serializableObject)</code> constructor.
Custom serialization could be used, but again, it&#39;s not related to batching,
so I&#39;ll ignore it.</p>
</li>
<li><p>We won&#39;t care about transactions, i.e. if connectivity dies in the middle
of sending the big batch, we might end up with partially sent batch.</p>
</li>
</ul>
<h2 id="messages-of-known-size">Messages of Known Size</h2>
<p>Let&#39;s start with a simple use case: the size of each message is known to us. 
It&#39;s defined by hypothetical <code>Func&lt;T, long&gt; getSize</code> function. Here is a 
helpful extension method that will split an arbitrary collection based on
a metric function and maximum chunk size:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> List&lt;List&lt;T&gt;&gt; ChunkBy&lt;T&gt;(<span class="hljs-keyword">this</span> IEnumerable&lt;T&gt; source, Func&lt;T, <span class="hljs-keyword">long</span>&gt; metric, <span class="hljs-keyword">long</span> maxChunkSize)
{
    <span class="hljs-keyword">return</span> source
        .Aggregate(
            <span class="hljs-keyword">new</span>
            {
                Sum = <span class="hljs-number">0</span>L,
                Current = (List&lt;T&gt;)<span class="hljs-literal">null</span>,
                Result = <span class="hljs-keyword">new</span> List&lt;List&lt;T&gt;&gt;()
            },
            (agg, item) =&gt;
            {
                <span class="hljs-keyword">var</span> <span class="hljs-keyword">value</span> = metric(item);
                <span class="hljs-keyword">if</span> (agg.Current == <span class="hljs-literal">null</span> || agg.Sum + <span class="hljs-keyword">value</span> &gt; maxChunkSize)
                {
                    <span class="hljs-keyword">var</span> current = <span class="hljs-keyword">new</span> List&lt;T&gt; { item };
                    agg.Result.Add(current);
                    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> { Sum = <span class="hljs-keyword">value</span>, Current = current, agg.Result };
                }

                agg.Current.Add(item);
                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> { Sum = agg.Sum + <span class="hljs-keyword">value</span>, agg.Current, agg.Result };
            })
        .Result;
}
</code></pre>
<p>Now, the implementation of <code>SendBigBatchAsync</code> is simple:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task <span class="hljs-title">SendBigBatchAsync</span>(<span class="hljs-params">IEnumerable&lt;T&gt; messages, Func&lt;T, <span class="hljs-keyword">long</span>&gt; getSize</span>)
</span>{
    <span class="hljs-keyword">var</span> chunks = messages.ChunkBy(getSize, MaxServiceBusMessage);
    <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> chunk <span class="hljs-keyword">in</span> chunks)
    {
        <span class="hljs-keyword">var</span> brokeredMessages = chunk.Select(m =&gt; <span class="hljs-keyword">new</span> BrokeredMessage(m));
        <span class="hljs-keyword">await</span> client.SendBatchAsync(brokeredMessages);
    }
}

<span class="hljs-keyword">private</span> <span class="hljs-keyword">const</span> <span class="hljs-keyword">long</span> MaxServiceBusMessage = <span class="hljs-number">256000</span>;
<span class="hljs-keyword">private</span> <span class="hljs-keyword">readonly</span> QueueClient client;
</code></pre>
<p>Note that I do <code>await</code> for each chunk sequentially to preserve message ordering.
Another thing to notice is that we lost all-or-nothing guarantee: we might
be able to send the first chunk, and then get an exception from subsequent
parts. Some sort of retry mechanism is probably needed.</p>
<h2 id="brokeredmessage-size">BrokeredMessage.Size</h2>
<p>OK, how do we determine the size of each message? How do we implement 
<code>getSize</code> function? </p>
<p><code>BrokeredMessage</code> class exposes <code>Size</code> property, so it might be tempting to
rewrite our method the following way:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages)
{
    <span class="hljs-keyword">var</span> brokeredMessages = messages.Select(m =&gt; <span class="hljs-keyword">new</span> BrokeredMessage(m));
    <span class="hljs-keyword">var</span> chunks = brokeredMessages.ChunkBy(bm =&gt; bm.Size, MaxServiceBusMessage);
    <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> chunk <span class="hljs-keyword">in</span> chunks)
    {
        <span class="hljs-keyword">await</span> client.SendBatchAsync(chunk);
    }
}
</code></pre>
<p>Unfortunately, this won&#39;t work properly. A quote from documentation:</p>
<blockquote>
<p>The value of Size is only accurate after the BrokeredMessage 
instance is sent or received.</p>
</blockquote>
<p>My experiments show that <code>Size</code> of a draft message returns the size of 
the message body, ignoring headers. If the message bodies are large, and
each chunk has just a handful of them, the code might work ok-ish. </p>
<p>But it will significantly underestimate the size of large batches of messages
with small payload.</p>
<p>So, for the rest of this article I&#39;ll try to adjust the calculation for headers.</p>
<h2 id="fixed-header-size">Fixed Header Size</h2>
<p>It could be that the header size of each message is always the same.
Quite often people will set the same headers for all their messages,
or set no custom headers at all. </p>
<p>In this case, you might just measure this size once, and then put this
fixed value inside a configuration file.</p>
<p>Here is how you measure the headers of a <code>BrokeredMessage</code> message:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> sizeBefore = message.Size;
client.Send(message);
<span class="hljs-keyword">var</span> sizeAfter = message.Size;
<span class="hljs-keyword">var</span> headerSize = sizeAfter - sizeBefore;
</code></pre>
<p>Now you just need to adjust one line from the previous version of 
<code>SendBigBatchAsync</code> method</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> chunks = brokeredMessages.ChunkBy(bm =&gt; FixedHeaderSize + bm.Size, MaxServiceBusMessage);
</code></pre>
<p><code>FixedHeaderSize</code> might be simply hard-coded, or taken from configuration
per application.</p>
<h2 id="measuring-of-header-size-per-message">Measuring of Header Size per Message</h2>
<p>If the size of headers varies per message, you need a way to adjust batching
algorithm accordingly. </p>
<p>Unfortunately, I haven&#39;t found a straightforward way to accomplish that. It looks like
you&#39;d have to serialize the headers yourself, and then measure the size of
resulting binary. This is not a trivial operation to do correctly,
and also implies some performance penalty.</p>
<p>Sean Feldman <a href="https://weblogs.asp.net/sfeldman/asb-batching-brokered-messages">came up</a> 
with a way to <em>estimate</em> the size of headers. That might be a good way to go,
though the estimation tends to err on the safe side for messages with small
payload.</p>
<h2 id="heuristics-retry">Heuristics &amp; Retry</h2>
<p>The last possibility that I want to consider is actually allow yourself
violating the max size of the batch, but then handle the exception, retry
the send operation and adjust future calculations based on actual measured size
of the failed messages. The size is known after trying to <code>SendBatch</code>, even if
operation failed, so we can use this information.</p>
<p>Here is a sketch of how to do that in code:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// Sender is reused across requests</span>
<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">BatchSender</span>
{
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">readonly</span> QueueClient queueClient;
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">long</span> batchSizeLimit = <span class="hljs-number">262000</span>;
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">long</span> headerSizeEstimate = <span class="hljs-number">54</span>; <span class="hljs-comment">// start with the smallest header possible</span>

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">BatchSender</span>(<span class="hljs-params">QueueClient queueClient</span>)
    </span>{
        <span class="hljs-keyword">this</span>.queueClient = queueClient;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages)
    {
        <span class="hljs-keyword">var</span> packets = (<span class="hljs-keyword">from</span> m <span class="hljs-keyword">in</span> messages
                     <span class="hljs-keyword">let</span> bm = <span class="hljs-keyword">new</span> BrokeredMessage(m)
                     <span class="hljs-keyword">select</span> <span class="hljs-keyword">new</span> { Source = m, Brokered = bm, BodySize = bm.Size }).ToList();
        <span class="hljs-keyword">var</span> chunks = packets.ChunkBy(p =&gt; <span class="hljs-keyword">this</span>.headerSizeEstimate + p.Brokered.Size, <span class="hljs-keyword">this</span>.batchSizeLimit);
        <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> chunk <span class="hljs-keyword">in</span> chunks)
        {
            <span class="hljs-keyword">try</span>
            {
                <span class="hljs-keyword">await</span> <span class="hljs-keyword">this</span>.queueClient.SendBatchAsync(chunk.Select(p =&gt; p.Brokered));
            }
            <span class="hljs-keyword">catch</span> (MessageSizeExceededException)
            {
                <span class="hljs-keyword">var</span> maxHeader = packets.Max(p =&gt; p.Brokered.Size - p.BodySize);
                <span class="hljs-keyword">if</span> (maxHeader &gt; <span class="hljs-keyword">this</span>.headerSizeEstimate)
                {
                    <span class="hljs-comment">// If failed messages had bigger headers, remember this header size </span>
                    <span class="hljs-comment">// as max observed and use it in future calculations</span>
                    <span class="hljs-keyword">this</span>.headerSizeEstimate = maxHeader;
                }
                <span class="hljs-keyword">else</span>
                {
                    <span class="hljs-comment">// Reduce max batch size to 95% of current value</span>
                    <span class="hljs-keyword">this</span>.batchSizeLimit = (<span class="hljs-keyword">long</span>)(<span class="hljs-keyword">this</span>.batchSizeLimit * <span class="hljs-number">.95</span>);
                }

                <span class="hljs-comment">// Re-send the failed chunk</span>
                <span class="hljs-keyword">await</span> <span class="hljs-keyword">this</span>.SendBigBatchAsync(packets.Select(p =&gt; p.Source));
            }

        }
    }
}
</code></pre>
<p>The code example is quite involved, here is what actually happens:</p>
<ol>
<li><p>Create a brokered message for each message object, but also save the
corresponding source message. This is critical to be able to re-send items:
there&#39;s no way to send the same <code>BrokeredMessage</code> instance twice.</p>
</li>
<li><p>Also save the body size of the brokered message. We&#39;ll use it for retry
calculation.</p>
</li>
<li><p>Start with some guess of header size estimate. I start with 54 bytes, 
which seems to be the minimal header size possible.</p>
</li>
<li><p>Split the batch into chunks the same way we did before.</p>
</li>
<li><p>Try sending chunks one by one.</p>
</li>
<li><p>If send operation fails with <code>MessageSizeExceededException</code>, iterate
through failed items and find out the actual header size of the message.</p>
</li>
<li><p>If that actual size is bigger than our known estimate, increase the estimate
to the newly observed value. Retry sending the chunk (not the whole batch) with
this new setting.</p>
</li>
<li><p>If the header is small, but message size is still too big - reduce the 
allowed total size of the chunk. Retry again.</p>
</li>
</ol>
<p>The combination of checks of steps 7 and 8 should make the mechanism reliable
and self-adopting to message header payloads.</p>
<p>Since we reuse the sender between send operations, the size parameters will
also converge quite quickly and no more retries will be needed. Thus the 
performance overhead should be minimal.</p>
<h2 id="conclusion">Conclusion</h2>
<p>It seems like there is no &quot;one size fits all&quot; solution for this problem at 
the moment. The best implementation might depend on your messaging 
requirements.</p>
<p>But if you have the silver bullet solution, please leave a comment under
this post and answer <a href="https://stackoverflow.com/questions/44779707/split-batch-of-messages-to-be-sent-to-azure-service-bus">my StackOverflow question</a>!</p>
<p>Otherwise, let&#39;s hope that the new 
<a href="https://github.com/azure/azure-service-bus-dotnet">.NET Standard-compatible Service Bus client</a>
will solve this issue for us. Track <a href="https://github.com/Azure/azure-service-bus-dotnet/issues/109">this github issue</a>
for status updates.</p>
]]></content>
    </entry>
    
</feed>