<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Mikhail Shilkov</title>
    <link href="https://mikhail.io/feed/" rel="self"/>
    <link href="https://mikhail.io"/>
    <updated>2018-10-22T21:24:43.062Z</updated>
    <id>https://mikhail.io/</id>
    <author>
        <name>Mikhail Shilkov</name>
        <email></email>
    </author>

    
    <entry>
        <title>Azure Functions V2 Is Released, How Performant Is It?</title>
        <link href="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/"/>
        <updated>2018-10-10T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-10-10,/2018/10/azure-functions-v2-released-how-performant-is-it/</id>
        <content type="html"><![CDATA[<p>Azure Functions major version 2.0 was released into GA a few days back during Microsoft Ignite. The runtime is now
based on .NET Core and thus is cross-platform and more interoperable. It has a nice extensibility story too.</p>
<p>In theory, .NET Core runtime is more lean and performant.
But last time <a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">I checked back in April</a>,
the preview version of Azure Functions V2 had some serious issues with cold start durations.</p>
<p>I decided to give the new and shiny version another try and ran several benchmarks. All tests were conducted on 
Consumption plan.</p>
<p><strong>TL;DR</strong>: it&#39;s not perfect just yet.</p>
<h2 id="cold-starts">Cold Starts</h2>
<p>Cold starts happen when a new instance handles its first request, see my other posts:
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">one</a>,
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">two</a>,
<a href="https://mikhail.io/2018/08/serverless-cold-start-war/">three</a>.</p>
<h3 id="hello-world">Hello World</h3>
<p>The following chart gives a comparison of V1 vs V2 cold starts for the two most popular runtimes:
.NET and Javascript. The dark bar shows the most probable range of values, while the light ones
are possible but less frequent:</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-dotnet-js.png" alt="Cold Starts V1 vs V2: .NET and Javascript"></p>
<p>Apparently, V2 is slower to start for both runtimes. V2 on .NET is slower by 10% on average and seems 
to have higher variation. V2 on Javascript is massively slower: 2 times on average, and the slowest startup
time goes above 10 seconds.</p>
<h3 id="dependencies-on-board">Dependencies On Board</h3>
<p>The values for the previous chart were calculated for Hello-World type of functions with no extra dependencies.</p>
<p>The chart below shows two more Javascript functions, this time with a decent number of dependencies:</p>
<ul>
<li>Referencing 3 NPM packages - 5MB zipped</li>
<li>Referencing 38 NPM packages - 35 MB zipped</li>
</ul>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-js-dependencies.png" alt="Cold Starts V1 vs V2: Javascript with NPM dependencies"></p>
<p>V2 clearly loses on both samples, but V2-V1 difference seems to be consistently within 2.5-3
seconds for any amount of dependencies.</p>
<p>All the functions were deployed with the Run-from-Package method which promises faster startup times.</p>
<h3 id="java">Java</h3>
<p>Functions V2 come with a preview of a new runtime: Java / JVM. It utilizes the same extensibility model 
as Javascript, and thus it seems to be a first-class citizen now.</p>
<p>Cold starts are not first-class though: </p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/cold-starts-java.png" alt="Cold Starts Java"></p>
<p>If you are a Java developer, be prepared for 20-25 seconds of initial startup time. That will probably 
be resolved when the Java runtime becomes generally available:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">That matches some of our internal data. We are looking into it.</p>&mdash; Paul Batum (@paulbatum) <a href="https://twitter.com/paulbatum/status/1048391445386735616?ref_src=twsrc%5Etfw">October 6, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="queue-processor">Queue Processor</h2>
<p>Cold starts are most problematic for synchronous triggers like HTTP requests. They are less relevant
for queue-based workloads, where scale out is of higher importance.</p>
<p>Last year I ran some tests around the ability of Functions to keep up with variable queue load:
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">one</a>,
<a href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/">two</a>.</p>
<p>Today I ran two simple tests to compare the scalability of V1 vs. V2 runtimes.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>In my first tests, a lightweight Javascript Function processed messages from an Azure Storage Queue. For
each message, it just pauses for 500 msec and then completes. This is supposed to simulate I/O-bound 
Functions.</p>
<p>I&#39;ve sent 100,000 messages to the queue and measured how fast they went away. Batch size (degree of parallelism
on each instance) was set to 16.</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/queue-scaling-io-based.png" alt="Processing Queue Messages with Lightweight I/O Workload"></p>
<p>Two lines show the queue backlogs of two runtimes, while the bars indicate the number of instances working
in parallel at a given minute.</p>
<p>We see that V2 was a bit faster to complete, probably due to more instances provisioned to it at any moment.
The difference is not big though and might be statistically insignificant.</p>
<h3 id="cpu-at-work">CPU at Work</h3>
<p>Functions in my second experiment are CPU-bound. Each message invokes calculation of a 10-stage Bcrypt
hash. On a very quiet moment, 1 such function call takes about 300-400 ms to complete, consuming 100% CPU 
load on a single core.</p>
<p>Both Functions are precompiled .NET and both are using <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.NET</a>.</p>
<p>Batch size (degree of parallelism on each instance) was set to 2 to avoid too much fighting for the same CPU. Yet, 
the average call duration is about 1.5 seconds (3x slower than possible).</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/queue-scaling-cpu-bound.png" alt="Processing Queue Messages with CPU-bound Workload"></p>
<p>The first thing to notice: it&#39;s the same number of messages with comparable &quot;sequential&quot; execution time, but 
the total time to complete the job increased 3-fold. That&#39;s because the workload is much more demanding to
the resources of application instances, and they struggle to parallelize work more aggressively.</p>
<p>V1 and V2 are again close to each other. One more time, V2 got more instances allocated to it most of the time.
And yet, it seemed to be <em>consistently</em> slower and lost about 2.5 minutes on 25 minutes interval (~10%).</p>
<h2 id="http-scalability">HTTP Scalability</h2>
<p>I ran two similar Functions &mdash; I/O-bound &quot;Pause&quot; (~100 ms) and CPU-bound Bcrypt (9 stages, ~150ms) &mdash; under a stress test.
But this time they were triggered by HTTP requests. Then I compared the results for V1 and V2.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>The grey bars on the following charts represent the rate of requests sent and processed within a given minute.</p>
<p>The lines are percentiles of response time: green lines for V2 and orange lines for V1.</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-io-based.png" alt="Processing HTTP Requests with Lightweight I/O Workload"></p>
<p>Yes, you saw it right, my Azure Functions were processing 100,000 messages per minute at peak. That&#39;s a lot of
messages.</p>
<p>Apart from the initial spike at minutes 2 and 3, both versions performed pretty close to each other.</p>
<p>50th percentile is flat close to the theoretic minimum of 100 ms, while the 95th percentile fluctuates a bit, but still
mostly stays quite low. </p>
<p>Note that the response time is measured from the client perspective, not by looking at the statistics provided by Azure.</p>
<h3 id="cpu-fanout">CPU Fanout</h3>
<p>How did CPU-heavy workload perform?</p>
<p>To skip ahead, I must say that the response time increased much more significantly, so my sample clients were
not able to generate request rates of 100k per minute. They &quot;only&quot; did about 48k per minute at peak, which still
seems massive to me.</p>
<p>I&#39;ve run the same test twice: one for Bcrypt implemented in .NET, and one for Javascript.</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-dotnet.png" alt="Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>V2 had a real struggle during the first minute, where response time got terribly slow up to 9 seconds.</p>
<p>Looking at the bold-green 50th percentile, we can see that it&#39;s consistently higher than the orange one throughout
the load growth period of the first 10 minutes. V2 seemed to have a harder time to adjust.</p>
<p>This might be explainable by slower growth of instance count:</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-dotnet-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>This difference could be totally random, so let&#39;s look at a similar test with Javascript worker. Here is the percentile chart:</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-js.png" alt="Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>The original slowness of the first 3 minutes is still there, but after that time V2 and V1 are on-par.</p>
<p>On-par doesn&#39;t sound that great though if you look at the significant edge in the number of allocated instances, in
favor of V2 this time:</p>
<p><img src="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/http-scaling-cpu-bound-js-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>Massive 147 instances were crunching Bcrypt hashes in Javascript V2, and that made it a bit faster to respond than V1.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As always, be reluctant to make definite conclusions based on simplistic benchmarks. But I see some trends which might
be true as of today:</p>
<ul>
<li>Performance of .NET Functions is comparable across two versions of Functions runtimes;</li>
<li>V1 still has a clear edge in the cold start time of Javascript Functions;</li>
<li>V2 is the only option for Java developers, but be prepared to very slow cold starts;</li>
<li>Scale-out characteristics seem to be independent of the runtime version, although there are blurry signs of
V2 being a bit slower to ramp up or slightly more resource hungry.</li>
</ul>
<p>I hope this helps in your serverless journey!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Serverless: Cold Start War</title>
        <link href="https://mikhail.io/2018/08/serverless-cold-start-war/"/>
        <updated>2018-08-30T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-08-30,/2018/08/serverless-cold-start-war/</id>
        <content type="html"><![CDATA[<p>Serverless cloud services are hot. Except when they are not :)</p>
<p>AWS Lambda, Azure Functions, Google Cloud Functions are all similar in their attempt
to enable rapid development of cloud-native serverless applications.</p>
<p>Auto-provisioning and auto-scalability are the killer features of those Function-as-a-Service
cloud offerings. No management required, cloud providers will deliver infrastructure for the user
based on the actual incoming load.</p>
<p>One drawback of such dynamic provisioning is a phenomenon called &quot;cold start&quot;. Basically,
applications that haven&#39;t been used for a while take longer to startup and to handle the
first request.</p>
<p>Cloud providers keep a bunch of generic unspecialized workers in stock. Whenever a serverless
application needs to scale up, be it from 0 to 1 instances, or from N to N+1 likewise, the runtime
will pick one of the spare workers and will configure it to serve the named application:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//coldstart.png" alt="Cold Start"></p>
<p>This procedure takes time, so the latency of the application event handling increases. To avoid
doing this for every event, the specialized worker will be kept intact for some period of time.
When another event comes in, this worker will stand available to process it as soon as possible.
This is a &quot;warm start&quot;:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//warmstart.png" alt="Warm Start"></p>
<p>The problem of cold start latency was described multiple times, here are the notable links:</p>
<ul>
<li><a href="https://blogs.msdn.microsoft.com/appserviceteam/2018/02/07/understanding-serverless-cold-start/">Understanding Serverless Cold Start</a></li>
<li><a href="https://hackernoon.com/cold-starts-in-aws-lambda-f9e3432adbf0">Everything you need to know about cold starts in AWS Lambda</a></li>
<li><a href="https://serverless.com/blog/keep-your-lambdas-warm/">Keeping Functions Warm</a></li>
<li><a href="https://theburningmonk.com/2018/01/im-afraid-youre-thinking-about-aws-lambda-cold-starts-all-wrong/">I&#39;m afraid you&#39;re thinking about AWS Lambda cold starts all wrong</a></li>
</ul>
<p>The goal of my article today is to explore how cold starts compare:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different languages and runtimes</li>
<li>For smaller vs larger applications (including dependencies)</li>
<li>How often cold starts happen</li>
<li>What can be done to optimize the cold starts</li>
</ul>
<p>Let&#39;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some important factors and parameters that
influence the outcome. My interpretation might be wrong. The results might change over time. If you happen 
to know a way to improve my tests, please let me know and I will re-run them and re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>All tests were run against HTTP Functions because that&#39;s where cold start matters the most. </p>
<p>All the functions were returning a simple JSON reporting their current instance ID, language etc.
Some functions were also loading extra dependencies, see below.</p>
<p>I did not rely on execution time reported by a cloud provider. Instead, I measured end-to-end duration from
the client perspective. This means that durations of HTTP gateway (e.g. API Gateway in case of AWS) are included
into the total duration. However, all calls were made from within the same region, so network latency should 
have minimal impact:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//test-setup.png" alt="Test Setup"></p>
<p>Important note: I ran all my tests on GA (generally available) versions of services/languages, so e.g.
Azure tests were done with version 1 of Functions runtime (.NET Framework), and GCP tests were only made for
Javascript runtime.</p>
<h2 id="when-does-cold-start-happen-">When Does Cold Start Happen?</h2>
<p>Obviously, cold start happens when the very first request comes in. After that request is processed,
the instance is kept alive in case subsequent requests arrive. But for how long?</p>
<p>The answer differs between cloud providers.</p>
<p>To help you read the charts in this section, I&#39;ve marked cold starts with blue color dots, and warm starts
with orange color dots.</p>
<h3 id="azure">Azure</h3>
<p>Here is the chart for Azure. It shows the values of normalized request durations across
different languages and runtime versions (Y-axis) depending on the time since the previous
request in minutes (X-axis):</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//azure-coldstart-threshold.png" alt="Azure Cold Start Threshold"></p>
<p>Clearly, an idle instance lives for 20 minutes and then gets recycled. All requests after 20 minutes
threshold hit another cold start.</p>
<h3 id="aws">AWS</h3>
<p>AWS is more tricky. Here is the same kind of chart, relative durations vs time since the last request, 
measured for AWS Lambda:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//aws-coldstart-threshold.png" alt="AWS Cold Start vs Warm Start"></p>
<p>There&#39;s no clear threshold here... For this sample, no cold starts happened within 28 minutes after the previous 
invocation. Afterward, the frequency of cold starts slowly rises. But even after 1 hour of inactivity, there&#39;s still a
good chance that your instance is alive and ready to take requests.</p>
<p>This doesn&#39;t match the official information that AWS Lambdas stay alive for just 5 minutes after the last
invocation. I reached out to Chris Munns, and he confirmed:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">
So what you are seeing is very much possible as the team plays with certain knobs/levers for execution environment lifecycle. 
let me know if you have concerns about it, but it should be just fine</p>&mdash; chrismunns (@chrismunns) 
<a href="https://twitter.com/chrismunns/status/1021452964630851585?ref_src=twsrc%5Etfw">July 23, 2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>A couple learning points here:</p>
<ul>
<li>AWS is working on improving cold start experience (and probably Azure/GCP do too)</li>
<li>My results might not be reliably reproducible in your application since it&#39;s affected by recent adjustments</li>
</ul>
<h3 id="gcp">GCP</h3>
<p>Google Cloud Functions left me completely puzzled. Here is the same chart for GCP cold starts (again,
orange dots are warm and blue ones are cold):</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//gcp-coldstart-threshold.png" alt="GCP Cold Start vs Warm Start"></p>
<p>This looks totally random to me. A cold start can happen in 3 minutes after the previous request, or an instance
can be kept alive for the whole hour. The probability of a cold start doesn&#39;t seem to depend on the interval,
at least just by looking at this chart.</p>
<p>Any ideas about what&#39;s going on are welcome!</p>
<h3 id="parallel-requests">Parallel requests</h3>
<p>Cold starts happen not only when the first instance of an application is provisioned. The same issue will happen whenever
all the provisioned instances are busy handling incoming events, and yet another event comes in (at scale out).</p>
<p>As far as I&#39;m aware, this behavior is common to all 3 providers, so I haven&#39;t prepared any comparison charts
for N+1 cold starts. Yet, be aware of them!</p>
<h2 id="reading-candle-charts">Reading Candle Charts</h2>
<p>In the following sections, you will see charts that represent statistical distribution of cold start time as
measured during my experiments. I repeated experiments multiple times and then grouped the metric values, e.g.
by the cloud provider or by language.</p>
<p>Each group will be represented by a &quot;candle&quot; on the chart. This is how you should read each candle:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//sample-coldstart-chart.png" alt="How to Read Cold Start Charts"></p>
<h2 id="memory-allocation">Memory Allocation</h2>
<p>AWS Lambda and Google Cloud Functions have a setting to define the memory size that gets allocated to a single
instance of a function. A user can select a value from 128MB to 2GB and above at creation time.</p>
<p>More importantly, the virtual CPU cycles get allocated proportionally to this provisioned memory size. This means
that an instance of 512 MB will have twice as much CPU speed as an instance of 256MB.</p>
<p>Does this affect the cold start time?</p>
<p>I&#39;ve run a series of tests to compare cold start latency across the board of memory/CPU sizes. The results are
somewhat mixed.</p>
<p>AWS Lambda Javascript doesn&#39;t seem to have significant differences. This probably means that not so much CPU load
is required to start a Node.js &quot;Hello World&quot; application:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//aws-coldstart-js-by-memory.png" alt="AWS Javascript Cold Start by Memory"></p>
<p>AWS Lambda .NET Core runtime does depend on memory size though. Cold start time drops dramatically with every increase
in allocated memory and CPU:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//aws-coldstart-csharp-by-memory.png" alt="AWS C# Cold Start by Memory"></p>
<p>GCP Cloud Functions expose a similar effect even for Javascript runtime:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//gcp-coldstart-js-by-memory.png" alt="GCP Javascript Cold Start by Memory"></p>
<p>In contrast to Amazon and Google, Microsoft doesn&#39;t ask to select a memory limit. Azure will charge Functions based 
on the actual memory usage. More importantly, it will always dedicate a full vCore for a given Function execution.</p>
<p>It&#39;s not exactly apples-to-apples, but I chose to fix the memory allocations of AWS Lambda and GCF to 1024 MB.
This feels the closest to Azure&#39;s vCore capacity, although I haven&#39;t tried a formal CPU performance comparison.</p>
<p>Given that, let&#39;s see how the 3 cloud providers compare in cold start time.</p>
<h2 id="javascript-baseline">Javascript Baseline</h2>
<p>Node.js is the only runtime supported in production by Google Cloud Functions right now. Javascript is also
probably by far the most popular language for serverless applications across the board.</p>
<p>Thus, it makes sense to compare the 3 cloud providers on how they perform in Javascript. The
base test measures the cold starts of &quot;Hello World&quot; type of functions. Functions have no 
dependencies, so deployment package is really small.</p>
<p>Here are the numbers for cold starts:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//coldstart-js-baseline.png" alt="Cold Start for Basic Javascript Functions"></p>
<p>AWS is clearly doing the best job here. GCP takes the second place, and Azure is the slowest. The rivals are
sort of close though, seemingly playing in the same league so the exact disposition might change over time.</p>
<h2 id="how-do-languages-compare-">How Do Languages Compare?</h2>
<p>I&#39;ve written Hello World HTTP function in all supported languages of the cloud platforms: </p>
<ul>
<li>AWS: Javascript, Python, Java, Go and C# (.NET Core)</li>
<li>Azure: Javascript and C# (precompiled .NET assembly)</li>
<li>GCP: Javascript</li>
</ul>
<p>Azure kind of supports much more languages, including Python and Java, but they are still considered
experimental / preview, so the cold starts are not fully optimized. See 
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">my previous article</a> for exact numbers.</p>
<p>Same applies to Python on GCP.</p>
<p>The following chart shows some intuition about the cold start duration per language. The languages
are ordered based on mean response time, from lowest to highest:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//coldstart-per-language.png" alt="Cold Start per Language per Cloud and Language"></p>
<p>AWS provides the richest selection of runtimes, and 4 out of 5 are faster than the other two cloud providers.
C# / .NET seems to be the least optimized (Amazon, why is that?).</p>
<h2 id="does-size-matter-">Does Size Matter?</h2>
<p>OK, enough of Hello World. A real-life function might be more heavy, mainly because it would
depend on other third-party libraries.</p>
<p>To simulate such scenario, I&#39;ve measured cold starts for functions with extra dependencies:</p>
<ul>
<li>Javascript referencing 3 NPM packages - 5MB zipped</li>
<li>Javascript referencing 38 NPM packages - 35 MB zipped</li>
<li>C# function referencing 5 NuGet packages - 2 MB zipped</li>
<li>Java function referencing 5 Maven packages - 15 MB zipped</li>
</ul>
<p>Here are the results:</p>
<p><img src="https://mikhail.io/2018/08/serverless-cold-start-war//coldstart-dependencies.png" alt="Cold Start Dependencies"></p>
<p>As expected, the dependencies slow the loading down. You should keep your Functions lean,
otherwise, you will pay in seconds for every cold start.</p>
<p>However, the increase in cold start seems quite low, especially for precompiled languages.</p>
<p>A very cool feature of GCP Cloud Functions is that you don&#39;t have to include NPM packages into
the deployment archive. You just add <code>package.json</code> file and the runtime will restore them for you.
This makes the deployment artifact ridiculously small, but doesn&#39;t seem to slow down the cold
starts either. Obviously, Google pre-restores the packages in advance, before the actual request 
comes in.</p>
<h2 id="avoiding-cold-starts">Avoiding Cold Starts</h2>
<p>The overall impression is that cold start delays aren&#39;t that high, so most applications can tolerate
them just fine.</p>
<p>If that&#39;s not the case, some tricks can be implemented to keep function instances warm.
The approach is universal for all 3 providers: once in X minutes, make an artificial call to
the function to prevent it from expiring.</p>
<p>Implementation details will differ since the expiration policies are different, as we explored
above.</p>
<p>For applications with higher load profile, you might want to fire several parallel &quot;warming&quot;
requests in order to make sure that enough instances are kept in warm stock.</p>
<p>For further reading, have a look at my 
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">Cold Starts Beyond First Request in Azure Functions</a>
and <a href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/">AWS Lambda Warmer as Pulumi Component</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Here are some lessons learned from all the experiments above:</p>
<ul>
<li>Be prepared for 1-3 seconds cold starts even for the smallest Functions</li>
<li>Different languages and runtimes have roughly comparable cold start time within the same platform</li>
<li>Minimize the number of dependencies, only bring what&#39;s needed</li>
<li>AWS keeps cold starts below 1 second most of the time, which is pretty amazing</li>
<li>All cloud providers are aware of the problem and are actively optimizing the cold start experience</li>
<li>It&#39;s likely that in middle term these optimizations will make cold starts a non-issue for the
vast majority of applications</li>
</ul>
<p>Do you see anything weird or unexpected in my results? Do you need me to dig deeper into other aspects?
Please leave a comment below or ping me on <a href="https://twitter.com/MikhailShilkov">twitter</a>, and let&#39;s 
sort it all out.</p>
<p>Stay tuned for more serverless perf goodness!</p>
]]></content>
    </entry>
    
    <entry>
        <title>AWS Lambda Warmer as Pulumi Component</title>
        <link href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/"/>
        <updated>2018-08-02T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-08-02,/2018/08/aws-lambda-warmer-as-pulumi-component/</id>
        <content type="html"><![CDATA[<p>Out of curiosity, I&#39;m currently investigating cold starts of Function-as-a-Service platforms of major cloud providers. Basically,
if a function is not called for several minutes, the cloud instance behind it might be recycled, and then the next request will
take longer because a new instance will need to be provisioned.</p>
<p>Recently, Jeremy Daly <a href="https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/">posted</a> a nice
article about the proper way to keep AWS Lambda instances &quot;warm&quot; to (mostly) prevent cold starts with minimal overhead.
Chris Munns <a href="https://twitter.com/chrismunns/status/1017777028274294784">endorsed</a> the article, so we know it&#39;s the right way.</p>
<p>The amount of actions to be taken is quite significant:</p>
<ul>
<li>Define a CloudWatch event which would fire every 5 minutes</li>
<li>Bind this event as another trigger for your Lambda</li>
<li>Inside the Lambda, detect whether current invocation is triggered by our CloudWatch event</li>
<li>If so, short-circuit the execution and return immediately; otherwise, run the normal workload</li>
<li>(Bonus point) If you want to keep multiple instances alive, do some extra dancing with calling itself N times in parallel,
provided by an extra permission to do so.</li>
</ul>
<h2 id="pursuing-reusability">Pursuing Reusability</h2>
<p>To simplify this for his readers, Jeremy was so kind to</p>
<ul>
<li>Create an NPM package which you can install and then call from a function-to-be-warmed</li>
<li>Provide SAM and Serverless Framework templates to automate Cloud Watch integration</li>
</ul>
<p>Those are still two distinct steps: writing the code (JS + NPM) and provisioning the cloud resources (YAML + CLI). There are some
drawbacks to that:</p>
<ul>
<li>You need to change two parts, which don&#39;t look like each other</li>
<li>They have to work in sync, e.g. Cloud Watch event must provide the right payload for the handler</li>
<li>There&#39;s still some boilerplate for every new Lambda</li>
</ul>
<h2 id="pulumi-components">Pulumi Components</h2>
<p>Pulumi takes a different approach. You can blend the application code and infrastructure management code
into one cohesive cloud application.</p>
<p>Related resources can be combined together into reusable components, which hide repetitive stuff behind code abstractions.</p>
<p>One way to define an AWS Lambda with Typescript in Pulumi is the following:</p>
<pre><code class="language-typescript">const handler = (event: any, context: any, callback: (error: any, result: any) =&gt; void) =&gt; {
    const response = {
        statusCode: 200,
        body: &quot;Cheers, how are things?&quot;
      };

    callback(null, response);
};

const lambda = new aws.serverless.Function(&quot;my-function&quot;, { /* options */ }, handler);</code></pre>
<p>The processing code <code>handler</code> is just passed to infrastructure code as a parameter.</p>
<p>So, if I wanted to make reusable API for an &quot;always warm&quot; function, how would it look like?</p>
<p>From the client code perspective, I just want to be able to do the same thing:</p>
<pre><code class="language-typescript">const lambda = new mylibrary.WarmLambda(&quot;my-warm-function&quot;, { /* options */ }, handler);</code></pre>
<p>CloudWatch? Event subscription? Short-circuiting? They are implementation details!</p>
<h2 id="warm-lambda">Warm Lambda</h2>
<p>Here is how to implement such component. The declaration starts with a Typescript class:</p>
<pre><code class="language-typescript">export class WarmLambda extends pulumi.ComponentResource {
    public lambda: aws.lambda.Function;

    // Implementation goes here...
}</code></pre>
<p>We expose the raw Lambda Function object, so that it could be used for further bindings and retrieving outputs.</p>
<p>The constructor accepts the same parameters as <code>aws.serverless.Function</code> provided by Pulumi:</p>
<pre><code class="language-typescript">constructor(name: string,
        options: aws.serverless.FunctionOptions,
        handler: aws.serverless.Handler,
        opts?: pulumi.ResourceOptions) {

    // Subresources are created here...
}</code></pre>
<p>We start resource provisioning by creating the CloudWatch rule to be triggered every 5 minutes:</p>
<pre><code class="language-typescript">const eventRule = new aws.cloudwatch.EventRule(`${name}-warming-rule`, 
    { scheduleExpression: &quot;rate(5 minutes)&quot; },
    { parent: this, ...opts }
);</code></pre>
<p>Then goes the cool trick. We substitute the user-provided handler with our own &quot;outer&quot; handler. This handler closes
over <code>eventRule</code>, so it can use the rule to identify the warm-up event coming from CloudWatch. If such is identified,
the handler short-circuits to the callback. Otherwise, it passes the event over to the original handler:</p>
<pre><code class="language-typescript">const outerHandler = (event: any, context: aws.serverless.Context, callback: (error: any, result: any) =&gt; void) =&gt;
{
    if (event.resources &amp;&amp; event.resources[0] &amp;&amp; event.resources[0].includes(eventRule.name.get())) {
        console.log(&#39;Warming...&#39;);
        callback(null, &quot;warmed!&quot;);
    } else {
        console.log(&#39;Running the real handler...&#39;);
        handler(event, context, callback);
    }
};</code></pre>
<p>That&#39;s a great example of synergy enabled by doing both application code and application infrastructure in a
single program. I&#39;m free to mix and match objects from both worlds.</p>
<p>It&#39;s time to bind both <code>eventRule</code> and <code>outerHandler</code> to a new serverless function:</p>
<pre><code class="language-typescript">const func = new aws.serverless.Function(
    `${name}-warmed`, 
    options, 
    outerHandler, 
    { parent: this, ...opts });
this.lambda = func.lambda;            </code></pre>
<p>Finally, I create an event subscription from CloudWatch schedule to Lambda:</p>
<pre><code class="language-typescript">this.subscription = new serverless.cloudwatch.CloudwatchEventSubscription(
    `${name}-warming-subscription`, 
    eventRule,
    this.lambda,
    { },
    { parent: this, ...opts });</code></pre>
<p>And that&#39;s all we need for now! See the full code 
<a href="https://github.com/mikhailshilkov/pulumi-serverless-examples/blob/master/WarmedLambda-TypeScript/warmLambda.ts">here</a>.</p>
<p>Here is the output of <code>pulumi update</code> command for my sample &quot;warm&quot; lambda application:</p>
<pre><code>     Type                                                      Name                            Plan
 +   pulumi:pulumi:Stack                                       WarmLambda-WarmLambda-dev       create
 +    samples:WarmLambda                                       i-am-warm                       create
 +      aws-serverless:cloudwatch:CloudwatchEventSubscription  i-am-warm-warming-subscription  create
 +        aws:lambda:Permission                                i-am-warm-warming-subscription  create
 +        aws:cloudwatch:EventTarget                           i-am-warm-warming-subscription  create
 +      aws:cloudwatch:EventRule                               i-am-warm-warming-rule          create
 +      aws:serverless:Function                                i-am-warm-warmed                create
 +         aws:lambda:Function                                 i-am-warm-warmed                create</code></pre><p>7 Pulumi components and 4 AWS cloud resources are provisioned by one <code>new WarmLambda()</code> line.</p>
<h2 id="multi-instance-warming">Multi-Instance Warming</h2>
<p>Jeremy&#39;s library supports warming several instances of Lambda by issuing parallel self-calls.</p>
<p>Reproducing the same with Pulumi component should be fairly straightforward:</p>
<ul>
<li>Add an extra constructor option to accept the number of instances to keep warm</li>
<li>Add a permission to call Lambda from itself</li>
<li>Fire N calls when warming event is triggered</li>
<li>Short-circuit those calls in each instance</li>
</ul>
<p>Note that only the first item would be visible to the client code. That&#39;s the power of componentization
and code reuse.</p>
<p>I didn&#39;t need multi-instance warming, so I&#39;ll leave the implementation as exercise for the reader.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Obligatory note: most probably, you don&#39;t need to add warming to your AWS Lambdas.</p>
<p>But whatever advanced scenario you might have, it&#39;s likely that it is easier to express the scenario
in terms of general-purpose reusable component, rather than a set of guidelines or templates.</p>
<p>Happy hacking!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Getting Started with AWS Lambda in Pulumi</title>
        <link href="https://mikhail.io/2018/07/getting-started-with-aws-lambda-in-pulumi/"/>
        <updated>2018-07-12T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-07-12,/2018/07/getting-started-with-aws-lambda-in-pulumi/</id>
        <content type="html"><![CDATA[<p>For a small research project of mine, I needed to create HTTP triggered
AWS Lambda&#39;s in all supported programming languages.</p>
<p>I&#39;m not a power AWS user, so I get easily confused about the configuration
of things like IAM roles or API Gateway. Moreover, I wanted my environment to
be reproducible, so manual AWS Console wasn&#39;t a good option.</p>
<p>I decided it was a good job for Pulumi. They pay a lot of attention to
serverless and especially AWS Lambda, and I love the power of 
configuration as code.</p>
<p>I created a Pulumi program which provisions Lambda&#39;s running on Javascript,
.NET, Python, Java and Go. Pulumi program itself is written in Javascript.</p>
<p>I&#39;m describing the resulting code below in case folks need to do the same thing.
The code itself is on <a href="https://github.com/mikhailshilkov/pulumi-aws-serverless-examples">my github</a>.</p>
<h2 id="javascript">Javascript</h2>
<p>Probably, the vast majority of Pulumi + AWS Lambda users will be using
Javascript as programming language for their serverless functions.</p>
<p>No wonder that this scenario is the easiest to start with. There is a
high-level package <code>@pulumi/cloud-aws</code> which hides all the AWS machinery from
a developer. </p>
<p>The simplest function will consist of just several lines:</p>
<pre><code class="language-js">const cloud = require(&quot;@pulumi/cloud-aws&quot;);

const api = new cloud.API(&quot;aws-hellolambda-js&quot;);
api.get(&quot;/js&quot;, (req, res) =&gt; {
    res.status(200).json(&quot;Hi from Javascript lambda&quot;);
});

exports.endpointJs = api.publish().url;</code></pre>
<p>Configure your Pulumi stack, run <code>pulumi update</code> and a Lambda 
is up, running and accessible via HTTP.</p>
<h2 id="-net-core">.NET Core</h2>
<p>.NET is my default development environment and AWS Lambda supports .NET Core
as execution runtime.</p>
<p>Pulumi program is still Javascript, so it can&#39;t mix C# code in. Thus, the setup
looks like this:</p>
<ul>
<li>There is a .NET Core 2.0 application written in C# and utilizing
<code>Amazon.Lambda.*</code> NuGet packages</li>
<li>I build and publish this application with <code>dotnet</code> CLI</li>
<li>Pulumi then utilizes the published binaries to create deployment artifacts</li>
</ul>
<p>C# function looks like this:</p>
<pre><code class="language-csharp">public class Functions
{
    public async Task&lt;APIGatewayProxyResponse&gt; GetAsync(APIGatewayProxyRequest request, ILambdaContext context)
    {
        return new APIGatewayProxyResponse
        {
            StatusCode = (int)HttpStatusCode.OK,
            Body = &quot;\&quot;Hi from C# Lambda\&quot;&quot;,
            Headers = new Dictionary&lt;string, string&gt; { { &quot;Content-Type&quot;, &quot;application/json&quot; } }
        };
    }
}</code></pre>
<p>For non-Javascript lambdas I utilize <code>@pulumi/aws</code> package. It&#39;s of lower level
than <code>@pulumi/cloud-aws</code>, so I had to setup IAM first:</p>
<pre><code class="language-js">const aws = require(&quot;@pulumi/aws&quot;);

const policy = {
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
            &quot;Principal&quot;: {
                &quot;Service&quot;: &quot;lambda.amazonaws.com&quot;,
            },
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Sid&quot;: &quot;&quot;,
        },
    ],
};
const role = new aws.iam.Role(&quot;precompiled-lambda-role&quot;, {
    assumeRolePolicy: JSON.stringify(policy),
});</code></pre>
<p>And then I did a raw definition of AWS Lambda:</p>
<pre><code class="language-js">const pulumi = require(&quot;@pulumi/pulumi&quot;);

const csharpLambda = new aws.lambda.Function(&quot;aws-hellolambda-csharp&quot;, {
    runtime: aws.lambda.DotnetCore2d0Runtime,
    code: new pulumi.asset.AssetArchive({
        &quot;.&quot;: new pulumi.asset.FileArchive(&quot;./csharp/bin/Debug/netcoreapp2.0/publish&quot;),
    }),
    timeout: 5,
    handler: &quot;app::app.Functions::GetAsync&quot;,
    role: role.arn
});</code></pre>
<p>Note the path to <code>publish</code> folder, which should match the path created by
<code>dotnet publish</code>, and the handler name matching C# class/method.</p>
<p>Finally, I used <code>@pulumi/aws-serverless</code> to define API Gateway endpoint for
the lambda:</p>
<pre><code class="language-js">const serverless = require(&quot;@pulumi/aws-serverless&quot;);

const precompiledApi = new serverless.apigateway.API(&quot;aws-hellolambda-precompiledapi&quot;, {
    routes: [
        { method: &quot;GET&quot;, path: &quot;/csharp&quot;, handler: csharpLambda },
    ],
});</code></pre>
<p>That&#39;s definitely more ceremony compared to Javascript version. But hey, it&#39;s
code, so if you find yourself repeating the same code, go ahead and make a
higher order component out of it, incapsulating the repetitive logic.</p>
<h2 id="python">Python</h2>
<p>Pulumi supports Python as scripting language, but I&#39;m sticking to Javascript
for uniform experience.</p>
<p>In this case, the flow is similar to .NET but simpler: no compilation step
is required. Just define a <code>handler.py</code>:</p>
<pre><code class="language-python">def handler(event, context): 
    return {
        &#39;statusCode&#39;: 200,
        &#39;headers&#39;: {&#39;Content-Type&#39;: &#39;application/json&#39;},
        &#39;body&#39;: &#39;&quot;Hi from Python lambda&quot;&#39;
    }</code></pre>
<p>and package it into zip in AWS lambda definition:</p>
<pre><code class="language-js">const pythonLambda = new aws.lambda.Function(&quot;aws-hellolambda-python&quot;, {
    runtime: aws.lambda.Python3d6Runtime,
    code: new pulumi.asset.AssetArchive({
        &quot;.&quot;: new pulumi.asset.FileArchive(&quot;./python&quot;),
    }),
    timeout: 5,
    handler: &quot;handler.handler&quot;,
    role: role.arn
});</code></pre>
<p>I&#39;m reusing the <code>role</code> definition from above. The API definition will also
be the same as for .NET.</p>
<h2 id="go">Go</h2>
<p>Golang is a compiled language, so the approach is similar to .NET: write code,
build, reference the built artifact from Pulumi.</p>
<p>My Go function looks like this:</p>
<pre><code class="language-go">func Handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) {

 return events.APIGatewayProxyResponse{
  Body:       &quot;\&quot;Hi from Golang lambda\&quot;&quot;,
  StatusCode: 200,
 }, nil

}</code></pre>
<p>Because I&#39;m on Windows but AWS Lambda runs on Linux, I had to use 
<a href="https://github.com/aws/aws-lambda-go"><code>build-lambda-zip</code></a> 
tool to make the package compatible. Here is the PowerShell build script:</p>
<pre><code class="language-powershell">$env:GOOS = &quot;linux&quot;
$env:GOARCH = &quot;amd64&quot;
go build -o main main.go
~\Go\bin\build-lambda-zip.exe -o main.zip main</code></pre>
<p>and Pulumi function definition:</p>
<pre><code class="language-js">const golangLambda = new aws.lambda.Function(&quot;aws-hellolambda-golang&quot;, {
    runtime: aws.lambda.Go1dxRuntime,
    code: new pulumi.asset.FileArchive(&quot;./go/main.zip&quot;),
    timeout: 5,
    handler: &quot;main&quot;,
    role: role.arn
});</code></pre>
<h2 id="java">Java</h2>
<p>Java class implements an interface from AWS SDK:</p>
<pre><code class="language-java">public class Hello implements RequestStreamHandler {

    public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context) throws IOException {

        JSONObject responseJson = new JSONObject();

        responseJson.put(&quot;isBase64Encoded&quot;, false);
        responseJson.put(&quot;statusCode&quot;, &quot;200&quot;);
        responseJson.put(&quot;body&quot;, &quot;\&quot;Hi from Java lambda\&quot;&quot;);  

        OutputStreamWriter writer = new OutputStreamWriter(outputStream, &quot;UTF-8&quot;);
        writer.write(responseJson.toJSONString());  
        writer.close();
    }
}</code></pre>
<p>I compiled this code with Maven (<code>mvn package</code>), which produced a <code>jar</code> file. AWS Lambda accepts
<code>jar</code> directly, but Pulumi&#39;s <code>FileArchive</code> is unfortunately crashing on trying
to read it.</p>
<p>As a workaround, I had to define a <code>zip</code> file with <code>jar</code> placed inside <code>lib</code>
folder:</p>
<pre><code class="language-js">const javaLambda = new aws.lambda.Function(&quot;aws-coldstart-java&quot;, {
    code: new pulumi.asset.AssetArchive({
        &quot;lib/lambda-java-example-1.0-SNAPSHOT.jar&quot;: new pulumi.asset.FileAsset(&quot;./java/target/lambda-java-example-1.0-SNAPSHOT.jar&quot;),
    }),
    runtime: aws.lambda.Java8Runtime,
    timeout: 5,
    handler: &quot;example.Hello&quot;,
    role: role.arn
});</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>The complete code for 5 lambda functions in 5 different programming languages
can be found in <a href="https://github.com/mikhailshilkov/pulumi-aws-serverless-examples">my github repository</a>.</p>
<p>Running <code>pulumi update</code> provisions 25 AWS resources in a matter of 1 minute,
so I can start playing with my test lambdas in no time.</p>
<p>And the best part: when I don&#39;t need them anymore, I run <code>pulumi destroy</code> and
my AWS Console is clean again!</p>
<p>Happy serverless moments!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Monads explained in C# (again)</title>
        <link href="https://mikhail.io/2018/07/monads-explained-in-csharp-again/"/>
        <updated>2018-07-05T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-07-05,/2018/07/monads-explained-in-csharp-again/</id>
        <content type="html"><![CDATA[<p>I love functional programming for the simplicity that it brings.</p>
<p>But at the same time, I realize that learning functional programming is a challenging 
process. FP comes with a baggage of unfamiliar vocabulary that can be daunting for 
somebody coming from an object-oriented language like C#.</p>
<p><img src="https://mikhail.io/2018/07/monads-explained-in-csharp-again//functional-programming-word-cloud.png" alt="Functional Programming Word Cloud"></p>
<p><em>Some of functional lingo</em></p>
<p>&quot;Monad&quot; is probably the most infamous term from the list above. Monads have reputation of being 
something very abstract and very confusing. </p>
<h2 id="the-fallacy-of-monad-tutorials">The Fallacy of Monad Tutorials</h2>
<p>Numerous attempts were made to explain monads in simple definitions; and monad tutorials have become a
genre of its own. And yet, times and times again, they fail to enlighten the readers.</p>
<p>The shortest explanation of monads looks like this:</p>
<p><img src="https://mikhail.io/2018/07/monads-explained-in-csharp-again//monoid-endofunctors.png" alt="A Monad is just a monoid in the category of endofunctors"></p>
<p>It&#39;s both mathematically correct and totally useless to anybody learning functional programming. To
understand this statement, one has to know the terms &quot;monoid&quot;, &quot;category&quot; and &quot;endofunctors&quot; and be able
to mentally compose them into something meaningful.</p>
<p>The same problem is apparent in most monad tutorials. They assume some pre-existing knowledge in
heads of their readers, and if that assumption fails, the tutorial doesn&#39;t click.</p>
<p>Focusing too much on mechanics of monads instead of explaining why they are important is another
common problem.</p>
<p>Douglas Crockford grasped this fallacy very well:</p>
<blockquote>
<p>The monadic curse is that once someone learns what monads are and how to use them, they lose the ability to explain them to other people</p>
</blockquote>
<p>The problem here is likely the following. Every person who understands monads had their own path to
this knowledge. It hasn&#39;t come all at once, instead there was a series of steps, each giving an insight,
until the last final step made the puzzle complete.</p>
<p>But they don&#39;t remember the whole path anymore. They go online and blog about that very last step as
the key to understanding, joining the club of flawed explanations.</p>
<p>There is an actual <a href="http://tomasp.net/academic/papers/monads/monads-programming.pdf">academic paper from Tomas Petricek</a> 
that studies monad tutorials.</p>
<p>I&#39;ve read that paper and a dozen of monad tutorials online. And of course, now I came up with my own.</p>
<p>I&#39;m probably doomed to fail too, at least for some readers. Yet, I know that many people found the
<a href="https://mikhail.io/2016/01/monads-explained-in-csharp/">previous version</a> of this article useful.</p>
<p>I based my explanation on examples from C# - the object-oriented language familiar to .NET developers.</p>
<h2 id="story-of-composition">Story of Composition</h2>
<p>The base element of each functional program is Function. In typed languages each function 
is just a mapping between the type of its input parameter and output parameter.
Such type can be annotated as <code>func: TypeA -&gt; TypeB</code>.</p>
<p>C# is object-oriented language, so we use methods to declare functions. There are two ways 
to define a method comparable to <code>func</code> function above. I can use static method:</p>
<pre><code class="language-csharp">static class Mapper 
{
    static ClassB func(ClassA a) { ... }
}</code></pre>
<p>... or instance method:</p>
<pre><code class="language-csharp">class ClassA 
{
    // Instance method
    ClassB func() { ... }
}</code></pre>
<p>Static form looks closer to the function annotation, but both ways are actually equivalent
for the purpose of our discussion. I will use instance methods in my examples, however all of
them could be written as static extension methods too.</p>
<p>How do we compose more complex workflows, programs and applications out of such simple
building blocks? A lot of patterns both in OOP and FP worlds revolve around this question.
And monads are one of the answers.</p>
<p>My sample code is going to be about conferences and speakers. The method implementations 
aren&#39;t really important, just watch the types carefully. There are 4 classes (types) and 
3 methods (functions):</p>
<pre><code class="language-csharp">class Speaker 
{
    Talk NextTalk() { ... }
}

class Talk 
{
    Conference GetConference() { ... }
}

class Conference 
{
    City GetCity() { ... }
}

class City { ... }</code></pre>
<p>These methods are currently very easy to compose into a workflow:</p>
<pre><code class="language-csharp">static City NextTalkCity(Speaker speaker) 
{
    Talk talk = speaker.NextTalk();
    Conference conf = talk.GetConference();
    City city = conf.GetCity();
    return city;
}</code></pre>
<p>Because the return type of the previous step always matches the input type of the next step, we can
write it even shorter:</p>
<pre><code class="language-csharp">static City NextTalkCity(Speaker speaker) 
{
    return 
        speaker
        .NextTalk()
        .GetConference()
        .GetCity();
}</code></pre>
<p>This code looks quite readable. It&#39;s concise and it flows from top to bottom, from left to right, 
similar to how we are used to read any text. There is not much noise too.</p>
<p>That&#39;s not what real codebases look like though, because there are multiple complications
along the happy composition path. Let&#39;s look at some of them.</p>
<h2 id="nulls">NULLs</h2>
<p>Any class instance in C# can be <code>null</code>. In the example above I might get runtime errors if
one of the methods ever returns <code>null</code> back.</p>
<p>Typed functional programming always tries to be explicit about types, so I&#39;ll re-write the signatures
of my methods to annotate the return types as nullables:</p>
<pre><code class="language-csharp">class Speaker 
{
    Nullable&lt;Talk&gt; NextTalk() { ... }
}

class Talk 
{
    Nullable&lt;Conference&gt; GetConference() { ... }
}

class Conference 
{
    Nullable&lt;City&gt; GetCity() { ... }
}

class City { ... }</code></pre>
<p>This is actually invalid syntax in current C# version, because <code>Nullable&lt;T&gt;</code> and its short form
<code>T?</code> are not applicable to reference types. This <a href="https://blogs.msdn.microsoft.com/dotnet/2017/11/15/nullable-reference-types-in-csharp/">might change in C# 8</a>
though, so bear with me.</p>
<p>Now, when composing our workflow, we need to take care of <code>null</code> results:</p>
<pre><code class="language-csharp">static Nullable&lt;City&gt; NextTalkCity(Speaker speaker) 
{
    Nullable&lt;Talk&gt; talk = speaker.NextTalk();
    if (talk == null) return null;

    Nullable&lt;Conference&gt; conf = talk.GetConference();
    if (conf == null) return null;

    Nullable&lt;City&gt; city = conf.GetCity();
    return city;
}</code></pre>
<p>It&#39;s still the same method, but it got more noise now. Even though I used short-circuit returns
and one-liners, it still got harder to read.</p>
<p>To fight that problem, smart language designers came up with the Null Propagation Operator:</p>
<pre><code class="language-csharp">static Nullable&lt;City&gt; NextTalkCity(Speaker speaker) 
{
    return 
        speaker
        ?.NextTalk()
        ?.GetConference()
        ?.GetCity();
}</code></pre>
<p>Now we are almost back to our original workflow code: it&#39;s clean and concise, we just got
3 extra <code>?</code> symbols around.</p>
<p>Let&#39;s take another leap.</p>
<h2 id="collections">Collections</h2>
<p>Quite often a function returns a collection of items, not just a single item. To some extent,
that&#39;s a generalization of <code>null</code> case: with <code>Nullable&lt;T&gt;</code> we might get 0 or 1 results back,
while with a collection we can get <code>0</code> to any <code>n</code> results.</p>
<p>Our sample API could look like this:</p>
<pre><code class="language-csharp">class Speaker 
{
    List&lt;Talk&gt; GetTalks() { ... }
}

class Talk 
{
    List&lt;Conference&gt; GetConferences() { ... }
}

class Conference 
{
    List&lt;City&gt; GetCities() { ... }
}</code></pre>
<p>I used <code>List&lt;T&gt;</code> but it could be any class or plain <code>IEnumerable&lt;T&gt;</code> interface.</p>
<p>How would we combine the methods into one workflow? Traditional version would look like this:</p>
<pre><code class="language-csharp">static List&lt;City&gt; AllCitiesToVisit(Speaker speaker) 
{
    var result = new List&lt;City&gt;();

    foreach (Talk talk in speaker.GetTalks())
        foreach (Conference conf in talk.GetConferences())
            foreach (City city in conf.GetCities())
                result.Add(city);

    return result;
}</code></pre>
<p>It reads ok-ish still. But the combination of nested loops and mutation with some conditionals sprinkled 
on them can get unreadable pretty soon. The exact workflow might be lost in the mechanics.</p>
<p>As an alternative, C# language designers invented LINQ extension methods. We can write code like this:</p>
<pre><code class="language-csharp">static List&lt;City&gt; AllCitiesToVisit(Speaker speaker) 
{
    return 
        speaker
        .GetTalks()
        .SelectMany(talk =&gt; talk.GetConferences())
        .SelectMany(conf =&gt; conf.GetCities())
        .ToList();
}</code></pre>
<p>Let me do one further trick and format the same code in an unusual way:</p>
<pre><code class="language-csharp">static List&lt;City&gt; AllCitiesToVisit(Speaker speaker) 
{
    return 
        speaker
        .GetTalks()           .SelectMany(x =&gt; x
        .GetConferences()    ).SelectMany(x =&gt; x
        .GetCities()         ).ToList();
}</code></pre>
<p>Now you can see the same original code on the left, combined with just a bit of technical repeatable
clutter on the right. Hold on, I&#39;ll show you where I&#39;m going.</p>
<p>Let&#39;s discuss another possible complication.</p>
<h2 id="asynchronous-calls">Asynchronous Calls</h2>
<p>What if our methods need to access some remote database or service to produce the results? This
should be shown in type signature, and C# has <code>Task&lt;T&gt;</code> for that:</p>
<pre><code class="language-csharp">class Speaker 
{
    Task&lt;Talk&gt; NextTalk() { ... }
}

class Talk 
{
    Task&lt;Conference&gt; GetConference() { ... }
}

class Conference 
{
    Task&lt;City&gt; GetCity() { ... }
}</code></pre>
<p>This change breaks our nice workflow composition again.</p>
<p>We&#39;ll get back to async-await later, but the original way to combine <code>Task</code>-based
methods was to use <code>ContinueWith</code> and <code>Unwrap</code> API:</p>
<pre><code class="language-csharp">static Task&lt;City&gt; NextTalkCity(Speaker speaker) 
{
    return 
        speaker
        .NextTalk()
        .ContinueWith(talk =&gt; talk.Result.GetConference())
        .Unwrap()
        .ContinueWith(conf =&gt; conf.Result.GetCity())
        .Unwrap();
}</code></pre>
<p>Hard to read, but let me apply my formatting trick again:</p>
<pre><code class="language-csharp">static Task&lt;City&gt; NextTalkCity(Speaker speaker) 
{
    return 
        speaker
        .NextTalk()         .ContinueWith(x =&gt; x.Result
        .GetConference()   ).Unwrap().ContinueWith(x =&gt; x.Result
        .GetCity()         ).Unwrap();
}</code></pre>
<p>You can see that, once again, it&#39;s our nice readable workflow on the left + some mechanical repeatable
junction code on the right.</p>
<h2 id="pattern">Pattern</h2>
<p>Can you see a pattern yet?</p>
<p>I&#39;ll repeat the <code>Nullable</code>-, <code>List</code>- and <code>Task</code>-based workflows again:</p>
<pre><code class="language-csharp">static Nullable&lt;City&gt; NextTalkCity(Speaker speaker) 
{
    return 
        speaker               ?
        .NextTalk()           ?
        .GetConference()      ?
        .GetCity();
}

static List&lt;City&gt; AllCitiesToVisit(Speaker speaker) 
{
    return 
        speaker
        .GetTalks()            .SelectMany(x =&gt; x
        .GetConferences()     ).SelectMany(x =&gt; x
        .GetCities()          ).ToList();
}

static Task&lt;City&gt; NextTalkCity(Speaker speaker) 
{
    return 
        speaker
        .NextTalk()            .ContinueWith(x =&gt; x.Result
        .GetConference()      ).Unwrap().ContinueWith(x =&gt; x.Result
        .GetCity()            ).Unwrap();
}</code></pre>
<p>In all 3 cases there was a complication which prevented us from sequencing method
calls fluently. In all 3 cases we found the gluing code to get back to fluent composition.</p>
<p>Let&#39;s try to generalize this approach. Given some generic container type 
<code>WorkflowThatReturns&lt;T&gt;</code>, we have a method to combine an instance of such workflow with
a function which accepts the result of that workflow and returns another workflow back:</p>
<pre><code class="language-csharp">class WorkflowThatReturns&lt;T&gt; 
{
    WorkflowThatReturns&lt;U&gt; AddStep(Func&lt;T, WorkflowThatReturns&lt;U&gt;&gt; step);
}</code></pre>
<p>In case this is hard to grasp, have a look at the picture of what is going on:</p>
<p><img src="https://mikhail.io/2018/07/monads-explained-in-csharp-again//monad-bind.png" alt="Monad Bind Internals"></p>
<ol>
<li><p>An instance of type <code>T</code> sits in a generic container.</p>
</li>
<li><p>We call <code>AddStep</code> with a function, which maps <code>T</code> to <code>U</code> sitting inside yet another
container. </p>
</li>
<li><p>We get an instance of <code>U</code> but inside two containers.</p>
</li>
<li><p>Two containers are automatically unwrapped into a single container to get back to the
original shape.</p>
</li>
<li><p>Now we are ready to add another step!</p>
</li>
</ol>
<p>In the following code, <code>NextTalk</code> returns the first instance inside the container:</p>
<pre><code class="language-csharp">WorkflowThatReturns&lt;City&gt; Workflow(Speaker speaker) 
{
    return 
        speaker
        .NextTalk()         
        .AddStep(x =&gt; x.GetConference())
        .AddStep(x =&gt; x.GetCity()); 
}</code></pre>
<p>Subsequently, <code>AddStep</code> is called two times to transfer to <code>Conference</code> and then
<code>City</code> inside the same container:</p>
<p><img src="https://mikhail.io/2018/07/monads-explained-in-csharp-again//monad-two-binds.png" alt="Monad Bind Chaining"></p>
<h2 id="finally-monads">Finally, Monads</h2>
<p>The name of this pattern is <strong>Monad</strong>.</p>
<p>In C# terms, a Monad is a generic class with two operations: constructor and bind.</p>
<pre><code class="language-csharp">class Monad&lt;T&gt; {
    Monad(T instance);
    Monad&lt;U&gt; Bind(Func&lt;T, Monad&lt;U&gt;&gt; f);
}</code></pre>
<p>Constructor is used to put an object into container, <code>Bind</code> is used to replace one
contained object with another contained object.</p>
<p>It&#39;s important that <code>Bind</code>&#39;s argument returns <code>Monad&lt;U&gt;</code> and not just <code>U</code>. We can think
of <code>Bind</code> as a combination of <code>Map</code> and <code>Unwrap</code> as defined per following signature:</p>
<pre><code class="language-csharp">class Monad&lt;T&gt; {
    Monad(T instance);
    Monad&lt;U&gt; Map(Function&lt;T, U&gt; f);
    static Monad&lt;U&gt; Unwrap(Monad&lt;Monad&lt;U&gt;&gt; nested);
}</code></pre>
<p>Even though I spent quite some time with examples, I expect you to be slightly confused
at this point. That&#39;s ok.</p>
<p>Keep going and let&#39;s have a look at several sample implementations of Monad pattern.</p>
<p><a name="maybe" href="https://mikhail.io/2018/07/monads-explained-in-csharp-again/undefined"></a></p>
<h2 id="maybe-option-">Maybe (Option)</h2>
<p>My first motivational example was with <code>Nullable&lt;T&gt;</code> and <code>?.</code>. The full pattern
containing either 0 or 1 instance of some type is called <code>Maybe</code> (it maybe has a value,
or maybe not).</p>
<p><code>Maybe</code> is another approach to dealing with &#39;no value&#39; value, alternative to the 
concept of <code>null</code>. </p>
<p>Functional-first language F# typically doesn&#39;t allow <code>null</code> for its types. Instead, F# has 
a maybe implementation built into the language: 
it&#39;s called <code>option</code> type. </p>
<p>Here is a sample implementation in C#:</p>
<pre><code class="language-csharp">public class Maybe&lt;T&gt; where T : class
{
    private readonly T value;

    public Maybe(T someValue)
    {
        if (someValue == null)
            throw new ArgumentNullException(nameof(someValue));
        this.value = someValue;
    }

    private Maybe()
    {
    }

    public Maybe&lt;U&gt; Bind&lt;U&gt;(Func&lt;T, Maybe&lt;U&gt;&gt; func) where U : class
    {
        return value != null ? func(value) : Maybe&lt;U&gt;.None();
    }

    public static Maybe&lt;T&gt; None() =&gt; new Maybe&lt;T&gt;();
}</code></pre>
<p>When <code>null</code> is not allowed, any API contract gets more explicit: either you
return type <code>T</code> and it&#39;s always going to be filled, or you return <code>Maybe&lt;T&gt;</code>.
The client will see that <code>Maybe</code> type is used, so it will be forced to handle 
the case of absent value.</p>
<p>Given an imaginary repository contract (which does something with customers and
orders):</p>
<pre><code class="language-csharp">public interface IMaybeAwareRepository
{
    Maybe&lt;Customer&gt; GetCustomer(int id);
    Maybe&lt;Address&gt; GetAddress(int id);
    Maybe&lt;Order&gt; GetOrder(int id);
}</code></pre>
<p>The client can be written with <code>Bind</code> method composition, without branching, 
in fluent style:</p>
<pre><code class="language-csharp">Maybe&lt;Shipper&gt; shipperOfLastOrderOnCurrentAddress =
    repo.GetCustomer(customerId)
        .Bind(c =&gt; c.Address)
        .Bind(a =&gt; repo.GetAddress(a.Id))
        .Bind(a =&gt; a.LastOrder)
        .Bind(lo =&gt; repo.GetOrder(lo.Id))
        .Bind(o =&gt; o.Shipper);</code></pre>
<p>As we saw above, this syntax looks very much like a LINQ query with a bunch 
of <code>SelectMany</code> statements. One of the common 
implementations of <code>Maybe</code> implements <code>IEnumerable</code> interface to enable
a more C#-idiomatic binding composition. Actually:</p>
<h2 id="enumerable-selectmany-is-a-monad-">Enumerable + SelectMany is a Monad </h2>
<p><code>IEnumerable</code> is an interface for enumerable containers.</p>
<p>Enumerable containers can be created - thus the constructor monadic operation.</p>
<p>The <code>Bind</code> operation is defined by the standard LINQ extension method, here 
is its signature:</p>
<pre><code class="language-csharp">public static IEnumerable&lt;U&gt; SelectMany&lt;T, U&gt;(
    this IEnumerable&lt;T&gt; first, 
    Func&lt;T, IEnumerable&lt;U&gt;&gt; selector)</code></pre>
<p>Direct implementation is quite straightforward:</p>
<pre><code class="language-csharp">static class Enumerable 
{
    public static IEnumerable&lt;U&gt; SelectMany(
        this IEnumerable&lt;T&gt; values, 
        Func&lt;T, IEnumerable&lt;U&gt;&gt; func) 
    { 
        foreach (var item in values)
            foreach (var subItem in func(item))
                yield return subItem;
    }
}</code></pre>
<p>And here is an example of composition:</p>
<pre><code class="language-csharp">IEnumerable&lt;Shipper&gt; shippers =
    customers
        .SelectMany(c =&gt; c.Addresses)
        .SelectMany(a =&gt; a.Orders)
        .SelectMany(o =&gt; o.Shippers);</code></pre>
<p>The query has no idea about how the collections are stored (encapsulated in
containers). We use functions <code>T -&gt; IEnumerable&lt;U&gt;</code> to produce new enumerables
(<code>Bind</code> operation).</p>
<h2 id="task-future-">Task (Future)</h2>
<p>In C# <code>Task&lt;T&gt;</code> type is used to denote asynchronous computation which will eventually
return an instance of <code>T</code>. The other names for similar concepts in other languages
are <code>Promise</code> and <code>Future</code>.</p>
<p>While the typical usage of <code>Task</code> in C# is different from the Monad pattern we
discussed, I can still come up with a <code>Future</code> class with the familiar structure:</p>
<pre><code class="language-csharp">public class Future&lt;T&gt;
{
    private readonly Task&lt;T&gt; instance;

    public Future(T instance)
    {
        this.instance = Task.FromResult(instance);
    }

    private Future(Task&lt;T&gt; instance)
    {
        this.instance = instance;
    }

    public Future&lt;U&gt; Bind&lt;U&gt;(Func&lt;T, Future&lt;U&gt;&gt; func)
    {
        var a = this.instance.ContinueWith(t =&gt; func(t.Result).instance).Unwrap();
        return new Future&lt;U&gt;(a);
    }

    public void OnComplete(Action&lt;T&gt; action)
    {
        this.instance.ContinueWith(t =&gt; action(t.Result));
    }
}</code></pre>
<p>Effectively, it&#39;s just a wrapper around the <code>Task</code> which doesn&#39;t add too much value,
but it&#39;s a useful illustration because now we can do:</p>
<pre><code class="language-csharp">repository
    .LoadSpeaker()
    .Bind(speaker =&gt; speaker.NextTalk())
    .Bind(talk =&gt; talk.GetConference())
    .Bind(conference =&gt; conference.GetCity())
    .OnComplete(city =&gt; reservations.BookFlight(city));</code></pre>
<p>We are back to the familiar structure. Time for some more complications.</p>
<h2 id="non-sequential-workflows">Non-Sequential Workflows</h2>
<p>Up until now, all the composed workflows had very liniar, sequential
structure: the output of a previous step was always the input for the next step.
That piece of data could be discarded after the first use because it was never needed 
for later steps:</p>
<p><img src="https://mikhail.io/2018/07/monads-explained-in-csharp-again//linear-workflow.png" alt="Linear Workflow"></p>
<p>Quite often though, this might not be the case. A workflow step might need data
from two or more previous steps combined.</p>
<p>In the example above, <code>BookFlight</code> method might actually need both <code>Speaker</code> and
<code>City</code> objects:</p>
<p><img src="https://mikhail.io/2018/07/monads-explained-in-csharp-again//non-linear-workflow.png" alt="Non Linear Workflow"></p>
<p>In this case, we would have to use closure to save <code>speaker</code> object until we get
a <code>talk</code> too:</p>
<pre><code class="language-csharp">repository
    .LoadSpeaker()
    .OnComplete(speaker =&gt;
        speaker
            .NextTalk()
            .Bind(talk =&gt; talk.GetConference())
            .Bind(conference =&gt; conference.GetCity())
            .OnComplete(city =&gt; reservations.BookFlight(speaker, city))
        );</code></pre>
<p>Obviously, this gets ugly very soon.</p>
<p>To solve this structural problem, C# language got its <code>async</code>-<code>await</code> feature,
which is now being reused in more languages including Javascript.</p>
<p>If we move back to using <code>Task</code> instead of our custom <code>Future</code>, we are able to
write</p>
<pre><code class="language-csharp">var speaker = await repository.LoadSpeaker();
var talk = await speaker.NextTalk();
var conference = await talk.GetConference();
var city = await conference.GetCity();
await reservations.BookFlight(speaker, city);</code></pre>
<p>Even though we lost the fluent syntax, at least the block has just one level,
which makes it easier to navigate.</p>
<h2 id="monads-in-functional-languages">Monads in Functional Languages</h2>
<p>So far we learned that</p>
<ul>
<li>Monad is a workflow composition pattern</li>
<li>This pattern is used in functional programming</li>
<li>Special syntax helps simplify the usage</li>
</ul>
<p>It should come at no surprise that functional languages support monads on syntactic
level.</p>
<p>F# is a functional-first language running on .NET framework. F# had its own way of
doing workflows comparable to <code>async</code>-<code>await</code> before C# got it. In F#, the above
code would look like this:</p>
<pre><code class="language-fsharp">let sendReservation () = async {
    let! speaker = repository.LoadSpeaker()
    let! talk = speaker.nextTalk()
    let! conf = talk.getConference()
    let! city = conf.getCity()
    do! bookFlight(speaker, city)
}</code></pre>
<p>Apart from syntax (<code>!</code> instead of <code>await</code>), the major difference to C# is that
<code>async</code> is just one possible monad type to be used this way. There are many
other monads in F# standard library (they are called Computation Expressions).</p>
<p>The best part is that any developer can create their own monads, and then use
all the power of language features.</p>
<p>Say, we want a hand-made <code>Maybe</code> computation expressoin in F#:</p>
<pre><code class="language-fsharp">let nextTalkCity (speaker: Speaker) = maybe {
    let! talk = speaker.nextTalk()
    let! conf = talk.getConference()
    let! city = conf.getCity(talk)
    return city
}</code></pre>
<p>To make this code runnable, we need to define Maybe computation expression
builder:</p>
<pre><code class="language-fsharp">type MaybeBuilder() =

    member this.Bind(x, f) = 
        match x with
        | None -&gt; None
        | Some a -&gt; f a

    member this.Return(x) = 
        Some x

let maybe = new MaybeBuilder()</code></pre>
<p>I won&#39;t explain the details of what happens here, but you can see that the code is
quite trivial. Note the presence of <code>Bind</code> operation (and <code>Return</code> operation being
the monad constructor).</p>
<p>The feature is widely used by third-party F# libraries. Here is an actor definition
in Akka.NET F# API:</p>
<pre><code class="language-fsharp">let loop () = actor {
    let! message = mailbox.Receive()
    match message with
    | Greet(name) -&gt; printfn &quot;Hello %s&quot; name
    | Hi -&gt; printfn &quot;Hello from F#!&quot;
    return! loop ()
}</code></pre>
<h2 id="monad-laws">Monad Laws</h2>
<p>There are a couple laws that constructor and <code>Bind</code> need to adhere to, so
that they produce a proper monad.</p>
<p>A typical monad tutorial will make a lot of emphasis on the laws, but I find them
less important to explain to a beginner. Nonetheless, here they are for the sake
of completeness.</p>
<p><strong>Left Identity law</strong> says that Monad constructor is a neutral operation: you can safely
run it before <code>Bind</code>, and it won&#39;t change the result of the function call:</p>
<pre><code class="language-csharp">// Given
T value;
Func&lt;T, Monad&lt;U&gt;&gt; f;

// Then (== means both parts are equivalent)
new Monad&lt;T&gt;(value).Bind(f) == f(value) </code></pre>
<p><strong>Right Identity law</strong> says that given a monadic value, wrapping its contained data
into another monad of same type and then <code>Bind</code>ing it, doesn&#39;t change the original value:</p>
<pre><code class="language-csharp">// Given
Monad&lt;T&gt; monadicValue;

// Then (== means both parts are equivalent)
monadicValue.Bind(x =&gt; new Monad&lt;T&gt;(x)) == monadicValue</code></pre>
<p><strong>Associativity law</strong> means that the order in which <code>Bind</code> operations
are composed does not matter:</p>
<pre><code class="language-csharp">// Given
Monad&lt;T&gt; m;
Func&lt;T, Monad&lt;U&gt;&gt; f;
Func&lt;U, Monad&lt;V&gt;&gt; g;

// Then (== means both parts are equivalent)
m.Bind(f).Bind(g) == m.Bind(a =&gt; f(a).Bind(g))</code></pre>
<p>The laws may look complicated, but in fact they are very natural 
expectations that any developer has when working with monads, so don&#39;t
spend too much mental effort on memorizing them.</p>
<h2 id="conclusion">Conclusion</h2>
<p>You should not be afraid of the &quot;M-word&quot; just because you are a C# programmer. </p>
<p>C# does not have a notion of monads as predefined language constructs, but 
that doesn&#39;t mean we can&#39;t borrow some ideas from the functional world. Having 
said that, it&#39;s also true that C# is lacking some powerful ways to combine 
and generalize monads that are available in functional programming
languages.</p>
<p>Go learn some more Functional Programming!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Programmable Cloud: Provisioning Azure App Service with Pulumi</title>
        <link href="https://mikhail.io/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi/"/>
        <updated>2018-06-22T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-06-22,/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi/</id>
        <content type="html"><![CDATA[<p>Modern Cloud providers offer a wide variety of services of different types
and levels. A modern cloud application would leverage multiple services
in order to be efficient in terms of developer experience, price, operations
etc.</p>
<p>For instance, a very simple Web Application deployed to Azure PaaS services
could use</p>
<ul>
<li>App Service - to host the application</li>
<li>App Service Plan - to define the instance size, price, scaling and other
hosting parameters</li>
<li>Azure SQL Database - to store relational data</li>
<li>Application Insights - to collect telemetry and logs</li>
<li>Storage Account - to store the binaries and leverage Run-as-Zip feature</li>
</ul>
<p>Provisioning such environment becomes a task on its own:</p>
<ul>
<li>How do we create the initial setup?</li>
<li>How do we make changes?</li>
<li>What if we need multiple environments?</li>
<li>How do we apply settings?</li>
<li>How do we recycle resources which aren&#39;t needed anymore?</li>
</ul>
<p>Well, there are several options.</p>
<h2 id="manually-in-azure-portal">Manually in Azure Portal</h2>
<p>We all start doing this in Azure Portal. User Interface is great for
discovering new services and features, and it&#39;s a quick way to make a single
change.</p>
<p><img src="https://mikhail.io/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi//azureportal.png" alt="Azure Portal"></p>
<p><em>Creating an App Service in Azure Portal</em></p>
<p>Clicking buttons manually doesn&#39;t scale though. After the initial setup is
complete, maintaining the environment over time poses significant challenges:</p>
<ul>
<li>Every change requires going back to the portal, finding the right resource
and doing the right change</li>
<li>People make mistakes, so if you have multiple environments, they are likely
to be different in subtle ways</li>
<li>Naming gets messy over time</li>
<li>There is no easily accessible history of environment changes</li>
<li>Cleaning up is hard: usually some leftovers will remain unnoticed</li>
<li>Skills are required from everybody involved in provisioning</li>
</ul>
<p>So, how do we streamline this process?</p>
<h2 id="azure-powershell-cli-and-management-sdks">Azure PowerShell, CLI and Management SDKs</h2>
<p>Azure comes with a powerful set of tools to manage resources with code.</p>
<p>You can use PowerShell, CLI scripts or custom code like C# to do with code
whatever is possible to do via portal.</p>
<pre><code class="language-csharp">var webApp = azure.WebApps.Define(appName)
    .WithRegion(Region.WestEurope)
    .WithNewResourceGroup(rgName)
    .WithNewFreeAppServicePlan()
    .Create();</code></pre>
<p><em>Fluent C# code creating an App Service</em></p>
<p>However, those commands are usually expressed in imperative style of 
CRUD operations. You can run the commands once, but it&#39;s hard to modify
existing resources from an arbitrary state to the desired end state.</p>
<h2 id="azure-resource-manager-templates">Azure Resource Manager Templates</h2>
<p>All services in Azure are managed by Azure Resource Manager (ARM). ARM 
has a special JSON-based format for templates. </p>
<p>Once a template is defined,
it&#39;s relatively straightforward to be deployed to Azure environment. So, if
resources are defined in JSON, they will be created automatically via
PowerShell or CLI commands.</p>
<p>It is also possible to deploy templates in incremental mode, when the tool
will compare existing environment with desired configuration and will deploy
the difference.</p>
<p>Templates can be parametrized, which enables multi-environment deployments.</p>
<p>There&#39;s a problem with templates though: they are JSON files. They get
very large very fast, they are hard to reuse, it&#39;s easy to make a typo.</p>
<p><img src="https://mikhail.io/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi//armtemplate.png" alt="ARM Template"></p>
<p><em>A fragment of auto-generated ARM Template for App Service, note the 
line numbers</em></p>
<p>Terraform is another templating tool to provision cloud resources but it uses
YAML instead of JSON. I don&#39;t have much experience with it, but the problems 
seem to be very similar.</p>
<p>Can we combine the power of SDKs and the power of JSON-/YAML-based desired state
configuration tools?</p>
<h2 id="pulumi">Pulumi</h2>
<p>One potential solution has just arrived.
A startup called Pulumi <a href="http://joeduffyblog.com/2018/06/18/hello-pulumi/">just went out of private beta to open source</a>.</p>
<p><img src="https://mikhail.io/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi//pulumi.jpg" alt="Pulumi"></p>
<p>Pulumi wants to be much more than a better version of ARM templates, aiming
to become the tool to build cloud-first distributed systems. But for today I&#39;ll 
focus on lower level of resource provisioning task.</p>
<p>With Pulumi cloud infrastructure is defined in code using full-blown general 
purpose programming languages.</p>
<p>The workflow goes like this:</p>
<ul>
<li>Define a Stack, which is a container for a group of related resources</li>
<li>Write a program in one of supported languages (I&#39;ll use TypeScript) which
references <code>pulumi</code> libraries and constructs all the resources as objects</li>
<li>Establish connection with your Azure account</li>
<li>Call <code>pulumi</code> CLI to create, update or destroy Azure resources based on
the program</li>
<li>Pulumi will first show the preview of changes, and then apply them as
requested</li>
</ul>
<h2 id="pulumi-program">Pulumi Program</h2>
<p>I&#39;m using TypeScript to define my Azure resources in Pulumi. So, the program
is a normal Node.js application with <code>index.ts</code> file, package references in 
<code>package.json</code> and one extra file <code>Pulumi.yaml</code> to define the program:</p>
<pre><code class="language-yaml">name: azure-appservice
runtime: nodejs</code></pre>
<p>Our <code>index.js</code> is as simple as a bunch of <code>import</code> statements followed by
creating TypeScript objects per desired resource. The simplest program can
look like this:</p>
<pre><code class="language-ts">import * as pulumi from &quot;@pulumi/pulumi&quot;;
import * as azure from &quot;@pulumi/azure&quot;;

const resourceGroup = new azure.core.ResourceGroup(&quot;myrg&quot;, {
    location: &quot;West Europe&quot;
});</code></pre>
<p>When executed by <code>pulumi update</code> command, this program will create a new
Resource Group in your Azure subscription.</p>
<h2 id="chaining-resources">Chaining Resources</h2>
<p>When multiple resources are created, the properties of one resource will
depend on properties of the others. E.g. I&#39;ve defined the Resource Group
above, and now I want to create an App Service Plan under this Group:</p>
<pre><code class="language-ts">const resourceGroupArgs = {
    resourceGroupName: resourceGroup.name,
    location: resourceGroup.location
};

const appServicePlan = new azure.appservice.Plan(&quot;myplan&quot;, {
    ...resourceGroupArgs,

    kind: &quot;App&quot;,

    sku: {
        tier: &quot;Basic&quot;,
        size: &quot;B1&quot;,
    },
});</code></pre>
<p>I&#39;ve assigned <code>resourceGroupName</code> and <code>location</code> of App Service Plan to
values from the Resource Group. It looks like a simple assignment of
strings but in fact it&#39;s more complicated.</p>
<p>Property <code>resourceGroup.name</code> has the type of <code>pulumi.Output&lt;string&gt;</code>.
Constructor argument <code>resourceGroupName</code> of <code>Plan</code> has the type of
<code>pulumi.Input&lt;string&gt;</code>.</p>
<p>We assigned <code>&quot;myrg&quot;</code> value to Resource Group name, but during the actual 
deployment it will change. Pulumi will append a unique identifier to the name,
so the actually provisioned group will be named e.g. <code>&quot;myrg65fb103e&quot;</code>.</p>
<p>This value will materialize inside <code>Output</code> type only at deployment time, 
and then it will get propagated to <code>Input</code> by Pulumi.</p>
<p>There is also a nice way to return the end values of <code>Output</code>&#39;s from Pulumi
program. Let&#39;s say we define an App Service:</p>
<pre><code class="language-ts">const app = new azure.appservice.AppService(&quot;mywebsite&quot;, {
    ...resourceGroupArgs,

    appServicePlanId: appServicePlan.id
});</code></pre>
<p>First, notice how we used TypeScript spread operator to reuse
properties from <code>resourceGroupArgs</code>.</p>
<p>Second, <code>Output</code>-<code>Input</code> assignment got used again to propagate App Service
Plan ID.</p>
<p>Lastly, we can now export App Service host name from our program, e.g.
for the user to be able to go to the web site immediately after deployment:</p>
<pre><code class="language-ts">exports.hostname = app.defaultSiteHostname;</code></pre>
<p><code>Output</code> can also be transformed with <code>apply</code> function. Here is the code to
format output URL:</p>
<pre><code class="language-ts">exports.endpoint = app.defaultSiteHostname.apply(n =&gt; `https://${n}`);</code></pre>
<p>Running <code>pulumi update</code> from CLI will then print the endpoint for us:</p>
<pre><code class="language-sh">---outputs:---
endpoint: &quot;https://mywebsiteb76260b5.azurewebsites.net&quot;</code></pre>
<p>Multiple outputs can be combined with <code>pulumi.all</code>, e.g. given SQL Server
and Database, we could make a connection string:</p>
<pre><code class="language-ts">const connectionString = 
    pulumi.all([sqlServer, database]).apply(([server, db]) =&gt; 
        `Server=tcp:${server}.database.windows.net;initial catalog=${db};user ID=${username};password=${pwd};Min Pool Size=0;Max Pool Size=30;Persist Security Info=true;`)</code></pre>
<h2 id="using-the-power-of-npm">Using the Power of NPM</h2>
<p>Since our program is just a TypeScript application, we are free to use any
3rd party package which exists out there in NPM.</p>
<p>For instance, we can install Azure Storage SDK. Just</p>
<pre><code class="language-sh">npm install azure-storage@2.9.0-preview</code></pre>
<p>and then we can write a function to produce SAS token for a Blob in Azure
Storage:</p>
<pre><code class="language-ts">import * as azurestorage from &quot;azure-storage&quot;;

// Given an Azure blob, create a SAS URL that can read it.
export function signedBlobReadUrl(
    blob: azure.storage.Blob | azure.storage.ZipBlob,
    account: azure.storage.Account,
    container: azure.storage.Container,
): pulumi.Output&lt;string&gt; {
    const signatureExpiration = new Date(2100, 1);

    return pulumi.all([
        account.primaryConnectionString,
        container.name,
        blob.name,
    ]).apply(([connectionString, containerName, blobName]) =&gt; {
        let blobService = new azurestorage.BlobService(connectionString);
        let signature = blobService.generateSharedAccessSignature(
            containerName,
            blobName,
            {
                AccessPolicy: {
                    Expiry: signatureExpiration,
                    Permissions: azurestorage.BlobUtilities.SharedAccessPermissions.READ,
                },
            }
        );

        return blobService.getUrl(containerName, blobName, signature);
    });
}</code></pre>
<p>I took this function from <a href="https://github.com/pulumi/examples/tree/master/azure-ts-functions">Azure Functions</a>
example, and it will probably move to Pulumi libraries at some point, but until
then you are free to leverage the package ecosystem.</p>
<h2 id="deploying-application-files">Deploying Application Files</h2>
<p>So far we provisioned Azure App Service, but we can also deploy the application
files as part of the same workflow.</p>
<p>The code below is using <a href="https://github.com/Azure/app-service-announcements/issues/84">Run from Zip</a>
feature of App Service:</p>
<ol>
<li><p>Define Storage Account and Container</p>
<pre><code class="language-ts"> const storageAccount = new azure.storage.Account(&quot;mystorage&quot;, {
     ...resourceGroupArgs,

     accountKind: &quot;StorageV2&quot;,
     accountTier: &quot;Standard&quot;,
     accountReplicationType: &quot;LRS&quot;,
 });

 const storageContainer = new azure.storage.Container(&quot;mycontainer&quot;, {
     resourceGroupName: resourceGroup.name,
     storageAccountName: storageAccount.name,
     containerAccessType: &quot;private&quot;,
 });</code></pre>
</li>
<li><p>Create a folder with application files, e.g. <code>wwwroot</code>. It may contain
some test HTML, ASP.NET application, or anything supported by App Service.</p>
</li>
<li><p>Produce a zip file from that folder in Pulumi program:</p>
<pre><code class="language-ts"> const blob = new azure.storage.ZipBlob(&quot;myzip&quot;, {
     resourceGroupName: resourceGroup.name,
     storageAccountName: storageAccount.name,
     storageContainerName: storageContainer.name,
     type: &quot;block&quot;,

     content: new pulumi.asset.FileArchive(&quot;wwwroot&quot;)
 });</code></pre>
</li>
<li><p>Produce SAS Blob URL and assign it to App Service Run-as-Zip setting:</p>
<pre><code class="language-ts"> const codeBlobUrl = signedBlobReadUrl(blob, storageAccount, storageContainer);

 const app = new azure.appservice.AppService(&quot;mywebsite&quot;, {
     ...resourceGroupArgs,

     appServicePlanId: appServicePlan.id,

     appSettings: {
         &quot;WEBSITE_RUN_FROM_ZIP&quot;: codeBlobUrl
     }
 });</code></pre>
</li>
</ol>
<p>Run the program, and your Application will start as soon as <code>pulumi update</code>
is complete.</p>
<h2 id="determinism">Determinism</h2>
<p>Pulumi programs should strive to be deterministic.
That means you should avoid using things like current date/time or random numbers.</p>
<p>The reason is incremental updates. Every time you run <code>pulumi update</code>, it
will execute the program from scratch. If your resources depend on random
values, they will not match the existing resources and thus the false
delta will be detected and deployed.</p>
<p>In the SAS generation example above we used a fixed date in the future
instead of doing today + 1 year kind of calculation.</p>
<p>Should Pulumi provide some workaround for this?</p>
<h2 id="conclusion">Conclusion</h2>
<p>My code was kindly merged to 
<a href="https://github.com/pulumi/examples/tree/master/azure-ts-appservice">Pulumi examples</a>, 
go there for the complete runnable program that provisions App Service with
Azure SQL Database and Application Insights.</p>
<p>I really see high potential in Cloud-as-Code approach suggested by Pulumi.
Today we just scratched the surface of the possibilities. We were working
with cloud services on raw level: provisioning specific services with
given parameters.</p>
<p>Pulumi&#39;s vision includes providing higher-level components to blur the line
between infrastructure and code, and to enable everybody to create such
components on their own.</p>
<p>Exciting future ahead!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Cold Starts Beyond First Request in Azure Functions</title>
        <link href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/"/>
        <updated>2018-05-18T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-05-18,/2018/05/azure-functions-cold-starts-beyond-first-load/</id>
        <content type="html"><![CDATA[<p>In my <a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">previous article</a>
I&#39;ve explored the topic of Cold Starts in Azure Functions. Particularly, I&#39;ve measured the
cold start delays per language and runtime version.</p>
<p>I received some follow-up questions that I&#39;d like to explore in today&#39;s post:</p>
<ul>
<li>Can we avoid cold starts except the very first one by keeping the instance warm?</li>
<li>Given one warm instance, if two requests come at the same time, will one request hit 
a cold start because existing instance is busy with the other?</li>
<li>In general, does a cold start happen at scale-out when a new extra instance is provisioned?</li>
</ul>
<p>Again, we are only talking Consumption Plan here.</p>
<h2 id="theory">Theory</h2>
<p>Azure Functions are running on instances provided by Azure App Service. Each instance is
able to process several requests concurrently, which is different comparing to AWS Lambda.</p>
<p>Thus, the following <em>could</em> be true:</p>
<ul>
<li>If we issue at least 1 request every 20 minutes, the first instance should stay warm for
long time</li>
<li>Simultaneous requests don&#39;t cause cold start unless the existing instance gets too busy</li>
<li>When runtime decides to scale out and spin up a new instance, it could do so in the background,
still forwarding incoming requests to the existing warm instance(s). Once the new instance
is ready, it could be added to the pool without causing cold starts</li>
<li>If so, cold starts are mitigated beyond the very first execution</li>
</ul>
<p>Let&#39;s put this theory under test!</p>
<h2 id="keeping-always-warm">Keeping Always Warm</h2>
<p>I&#39;ve tested a Function App which consists of two Functions:</p>
<ul>
<li>HTTP Function under test</li>
<li>Timer Function which runs every 10 minutes and does nothing but logging 1 line of text</li>
</ul>
<p>I then measured the cold start statistics similar to all the tests from my previous article.</p>
<p>During 2 days I was issuing infrequent requests to the same app, most of them would normally
lead to a cold start. Interestingly, even though I was regularly firing the timer, Azure 
switched instances to serve my application 2 times during the test period:</p>
<p><img src="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load//cold-starts-keep-warm.png" alt="Infrequent Requests to Azure Functions with &quot;Keep It Warm&quot; Timer"></p>
<p>I can see that most responses are fast, so timer &quot;warmer&quot; definitely helps.</p>
<p>The first request(s) to a new instance are slower than subsequent ones. Still, they are faster
than normal full cold start time, so it could be related to HTTP stack loading.</p>
<p>Anyway, keeping Functions warm seems a viable strategy.</p>
<h2 id="parallel-requests">Parallel Requests</h2>
<p>What happens when there is a warm instance, but it&#39;s already busy with processing another
request? Will the parallel request be delayed, or will it be processed by the same
warm instance?</p>
<p>I tested with a very lightweight function, which nevertheless takes some time to complete:</p>
<pre><code class="language-csharp">public static async Task&lt;HttpResponseMessage&gt; Delay500([HttpTrigger] HttpRequestMessage req)
{
    await Task.Delay(500);
    return req.CreateResponse(HttpStatusCode.OK, &quot;Done&quot;);
}</code></pre>
<p>I believe it&#39;s an OK approximation for an IO-bound function.</p>
<p>The test client then issued 2 to 10 parallel requests to this function and measured the
response time for all requests.</p>
<p>It&#39;s not the easiest chart to understand in full, but note the following:</p>
<ul>
<li><p>Each group of bars are for requests sent at the same time. Then there goes a pause about
20 seconds before the next group of requests gets sent</p>
</li>
<li><p>The bars are colored by the instance which processed that request: same instance - same
color</p>
</li>
</ul>
<p><img src="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load//cold-starts-during-simultaneous-requests.png" alt="Azure Functions Response Time to Batches of Simultaneous Requests"></p>
<p>Here are some observations from this experiment:</p>
<ul>
<li><p>Out of 64 requests, there were 11 cold starts</p>
</li>
<li><p>Same instance <em>can</em> process multiple simultaneous requests, e.g. one instance processed
7 out of 10 requests in the last batch</p>
</li>
<li><p>Nonetheless, Azure is eager to spin up new instances for multiple requests. In total
12 instances were created, which is even more than max amount of requests in any single
batch</p>
</li>
<li><p>Some of those instances were actually never reused (gray-ish bars in batched x2 and x3,
brown bar in x10)</p>
</li>
<li><p>The first request to each new instance pays the full cold start price. Runtime doesn&#39;t
provision them in background while reusing existing instances for received requests</p>
</li>
<li><p>If an instance handled more than one request at a time, response time invariably suffers,
even though the function is super lightweight (<code>Task.Delay</code>)</p>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Getting back to the experiment goals, there are several things that we learned.</p>
<p>For low-traffic apps with sporadic requests it makes sense to setup a &quot;warmer&quot; timer
function firing every 10 minutes or so to prevent the only instance from being recycled.</p>
<p>However, scale-out cold starts are real and I don&#39;t see any way to prevent them from
happening.</p>
<p>When multiple requests come in at the same time, we might expect some of them to hit
a new instance and get slowed down. The exact algorithm of instance reuse is not
entirely clear.</p>
<p>Same instance is capable of processing multiple requests in parallel, so there are
possibilities for optimization in terms of routing to warm instances during the
provisioning of cold ones. </p>
<p>If such optimizations happen, I&#39;ll be glad to re-run my tests and report any noticeable
improvements.</p>
<p>Stay tuned for more serverless perf goodness!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Azure Functions: Cold Starts in Numbers</title>
        <link href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/"/>
        <updated>2018-04-24T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-04-24,/2018/04/azure-functions-cold-starts-in-numbers/</id>
        <content type="html"><![CDATA[<p>Auto-provisioning and auto-scalability are the killer features of Function-as-a-Service
cloud offerings, and Azure Functions in particular.</p>
<p>One drawback of such dynamic provisioning is a phenomenon called &quot;Cold Start&quot;. Basically,
applications that haven&#39;t been used for a while take longer to startup and to handle the
first request.</p>
<p>The problem is nicely described in 
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2018/02/07/understanding-serverless-cold-start/">Understanding Serverless Cold Start</a>,
so I won&#39;t repeat it here. I&#39;ll just copy a picture from that article:</p>
<p><img src="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers//coldstart.jpg" alt="Cold Start"></p>
<p>Based on the 4 actions which happen during a cold start, we may guess that the following factors
might affect the cold start duration:</p>
<ul>
<li>Language / execution runtime</li>
<li>Azure Functions runtime version</li>
<li>Application size including dependencies</li>
</ul>
<p>I ran several sample functions and tried to analyze the impact of these factors on cold start time.</p>
<h2 id="methodology">Methodology</h2>
<p>All tests were run against HTTP Functions, because that&#39;s where cold start matters the most.</p>
<p>All the functions were just returning &quot;Hello, World&quot; taking the &quot;World&quot; value from the query string.
Some functions were also loading extra dependencies, see below.</p>
<p>I did not rely on execution time reported by Azure. Instead, I measured end-to-end duration from
client perspective. All calls were made from within the same Azure region, so network latency should 
have minimal impact:</p>
<p><img src="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers//test-setup.png" alt="Test Setup"></p>
<h2 id="when-does-cold-start-happen-">When Does Cold Start Happen?</h2>
<p>Obviously, cold start happens when the very first request comes in. After that request is processed,
the instance is kept alive in case subsequent requests arrive. But for how long?</p>
<p>The following chart gives the answer. It shows values of normalized request durations across
different languages and runtime versions (Y axis) depending on the time since the previous
request in minutes (X axis):</p>
<p><img src="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers//coldstart-threshold.png" alt="Cold Start Threshold"></p>
<p>Clearly, an idle instance lives for 20 minutes and then gets recycled. All requests after 20 minutes
threshold hit another cold start.</p>
<h2 id="how-do-languages-compare-">How Do Languages Compare?</h2>
<p>I&#39;ll start with version 1 of Functions runtime, which is the production-ready GA version as of today.</p>
<p>I&#39;ve written Hello World HTTP function in all GA languages: C#, F# and Javascript, and I added Python
for comparison. C#/F# were executed both in the form of script, and as a precompiled .NET assembly.</p>
<p>The following chart shows some intuition about the cold start duration per language. The languages
are ordered based on mean response time, from lowest to highest. 65% of request
durations are inside the vertical bar (1-sigma interval) and 95% are inside the vertical line (2-sigma):</p>
<p><img src="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers//coldstarts-v1.png" alt="Cold Start V1 per Language"></p>
<p>Somewhat surprisingly, precompiled .NET is exactly on par with Javascript. Javascript &quot;Hello World&quot; 
is really lightweight, so I expected it to win, but I was wrong.</p>
<p>C# Script is slower but somewhat comparable. F# Script presented a really negative surprise though: it&#39;s much
slower. It&#39;s even slower than experimental Python support where no performance optimization would
be expected at all!</p>
<h2 id="functions-runtime-v1-vs-v2">Functions Runtime: V1 vs V2</h2>
<p>Version 2 of Functions runtime is currently in preview and not suitable for production load. That
probably means they haven&#39;t done too much performance optimization, especially from cold start
standpoint.</p>
<p>Can we see this on the chart? We sure can:</p>
<p><img src="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers//coldstarts-v2.png" alt="Cold Start V1 vs V2"></p>
<p>V2 is massively slower. The fastest cold starts are around 6 seconds, but the slowest can come
up to 40-50 seconds.</p>
<p>Javascript is again on-par with precompiled .NET. </p>
<p>Java is noticeably slower, even though the
deployment package is just 33kB, so I assume I didn&#39;t overblow it.</p>
<h2 id="does-size-matter-">Does Size Matter?</h2>
<p>OK, enough of Hello World. A real-life function might be more heavy, mainly because it would
depend on other third-party libraries.</p>
<p>To simulate such scenario, I&#39;ve measured cold starts for a .NET function with references to 
Entity Framework, Automapper, Polly and Serilog.</p>
<p>For Javascript I did the same, but referenced Bluebird, lodash and AWS SDK.</p>
<p>Here are the results:</p>
<p><img src="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers//coldstarts-dependencies.png" alt="Cold Start Dependencies"></p>
<p>As expected, the dependencies slow the loading down. You should keep your Functions lean,
otherwise you will pay in seconds for every cold start.</p>
<p>An important note for Javascript developers: the above numbers are for Functions deployed
after <a href="https://github.com/Azure/azure-functions-pack"><code>Funcpack</code></a> preprocessor. The package
contained the single <code>js</code> file with Webpack-ed dependency tree. Without that, the mean
cold start time of the same function is 20 seconds!</p>
<h2 id="conclusions">Conclusions</h2>
<p>Here are some lessons learned from all the experiments above:</p>
<ul>
<li>Be prepared for 1-3 seconds cold starts even for the smallest Functions</li>
<li>Stay on V1 of runtime until V2 goes GA unless you don&#39;t care about perf</li>
<li>.NET precompiled and Javascript Functions have roughly same cold start time</li>
<li>Minimize the amount of dependencies, only bring what&#39;s needed</li>
</ul>
<p>Do you see anything weird or unexpected in my results? Do you need me to dig deeper on other aspects?
Please leave a comment below or ping me on twitter, and let&#39;s sort it all out.</p>
<p>There is a follow-up post available: 
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">Cold Starts Beyond First Request in Azure Functions</a></p>
]]></content>
    </entry>
    
    <entry>
        <title>Awesome F# Exchange 2018</title>
        <link href="https://mikhail.io/2018/04/fsharp-exchange-2018/"/>
        <updated>2018-04-07T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-04-07,/2018/04/fsharp-exchange-2018/</id>
        <content type="html"><![CDATA[<p>I&#39;m writing this post in the train to London Stensted, on my way back from F# Exchange 2018
conference.</p>
<p>F# Exchange is a yearly conference taking place in London, and 2018 edition was the first one
for me personally. I also had an honour to speak there about creating Azure Functions with
F#.</p>
<h2 id="impression">Impression</h2>
<p>F# is still relatively niche language, so the conference is not overcrowded, but that gives
it a special feeling of family gathering. There were 162 participants this year, and I have an
impression that every one of them is extremely friendly, enthusiastic and just plain awesome.</p>
<p>The conference itself had 2 tracks of 45-minute talks and 60-minute keynotes. Most talks were
of high quality, and the topics ranging from compiler internals to fun applications like 
music generation, car racing and map drawing.</p>
<p>Both Don Syme, the creator of F#, and Philip Carter, F# program manager, were there and gave
keynotes, but they were careful enough not to draw too much attention on Microsoft and let
the community speak loud.</p>
<h2 id="corridor-track">Corridor Track</h2>
<p>But the talks were just a part of the story. For me, the conference started in the evening
before the first day at the speakers drinks party, and only finished at 1 a.m. after the 
second day (the pubs in London are lovely).</p>
<p>I spoke to so many great people, I learnt a lot, and had fun too. I&#39;ve never seen so many
F# folks at the same place, and I guess there must be something about F# which attracts
the right kind of people to it.</p>
<p>And of course it&#39;s so much fun to meet face-to-face all those twitter, slack, github and 
Channel 9 persona&#39;s and to see that they are actually real people :)</p>
<h2 id="my-talk">My Talk</h2>
<p>The talk I gave was called &quot;Azure F#unctions&quot;. It was not a hard-core F# talk, but people
seemed to be genuinely interested in the topic.</p>
<p>A decent amount of attendees are already familiar with Azure Functions, and many either run 
them in production or plan to do so.</p>
<p>The reference version conflict problem is very well known and raises a lots of questions
or concerns. This even leads to workarounds like transpiling F# Functions to Javascript
with Fable. Yikes.</p>
<p>Durable Functions seem to be sparkling a lot of initial interest. I&#39;ll be definitely
spending more time to play with them, and maybe to make F# story more smooth.</p>
<p>Functions were mentioned in Philip&#39;s keynote as one of the important areas for F# 
application, which is cool. We should spend some extra effort to make the documentation
and onboarding story as smooth as possible.</p>
<h2 id="call-to-action">Call to Action</h2>
<p>Skills Matter is the company behind the conference. Carla, Nicole and others did a great
job preparing the event; everything went smooth, informal and fun.</p>
<p>The videos are already online at <a href="https://skillsmatter.com/conferences/9419-f-sharp-exchange-2018#skillscasts">Skillscasts</a> 
(requires free signup).</p>
<p><a href="https://skillsmatter.com/conferences/10869-f-sharp-exchange-2019">F# Exchange 2019</a> 
super early bird tickets are for sale now and until Monday April 9, go 
get one and join F# Exchange in London next year! </p>
<p>I&#39;m already missing you all.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Azure Durable Functions in F#</title>
        <link href="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/"/>
        <updated>2018-02-19T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2018-02-19,/2018/02/azure-durable-functions-in-fsharp/</id>
        <content type="html"><![CDATA[<p>Azure Functions are designed for stateless, fast-to-execute,
simple actions. Typically, they are triggered by an HTTP call or a queue message,
then they read something from the storage or database and return the result
to the caller or send it to another queue. All within several seconds at most.</p>
<p>However, there exists a preview of <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview">Durable Functions</a>,
an extension that lets you write stateful functions for long-running workflows.
Here is a picture of one possible workflow from the docs:</p>
<p><img src="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp//fan-out-fan-in.png" alt="Fan-out Fan-in Workflow"></p>
<p>Such workflows might take arbitrary time to complete. Instead of blocking and
waiting for all that period, Durable Functions use the combination of
Storage Queues and Tables to do all the work asynchronously.</p>
<p>The code still <em>feels</em> like one continuous thing because it&#39;s programmed
as a single orchestrator function. So, it&#39;s easier for a human to reason 
about the functionality without the complexities of low-level communication.</p>
<p>I won&#39;t describe Durable Functions any further, just go read
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview">documentation</a>,
it&#39;s nice and clean.</p>
<h2 id="language-support">Language Support</h2>
<p>As of February 2018, Durable Functions are still in preview. That also means
that language support is limited:</p>
<blockquote>
<p>Currently C# is the only supported language for Durable Functions. This 
includes orchestrator functions and activity functions. In the future, 
we will add support for all languages that Azure Functions supports.</p>
</blockquote>
<p>I was a bit disappointed that F# is not an option. But actually, since
Durable Functions support precompiled .NET assembly model, pretty much
anything doable in C# can be done in F# too.</p>
<p>The goal of this post is to show that you can write Durable Functions in F#.
I used precompiled .NET Standard 2.0 F# Function App running on 2.0 preview
runtime.</p>
<h2 id="orchestration-functions">Orchestration Functions</h2>
<p>The stateful workflows are Azure Functions with a special <code>OrchestrationTrigger</code>.
Since they are asynchronous, C# code is always based on <code>Task</code> and <code>async</code>-<code>await</code>.
Here is a simple example of orchestrator in C#:</p>
<pre><code class="language-csharp">public static async Task&lt;List&lt;string&gt;&gt; Run([OrchestrationTrigger] DurableOrchestrationContext context)
{
    var outputs = new List&lt;string&gt;();

    outputs.Add(await context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Tokyo&quot;));
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Seattle&quot;));
    outputs.Add(await context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;London&quot;));

    // returns [&quot;Hello Tokyo!&quot;, &quot;Hello Seattle!&quot;, &quot;Hello London!&quot;]
    return outputs;
}</code></pre>
<p>F# has its own preferred way of doing asynchronous code based on <code>async</code>
computation expression. The direct refactoring could look something like</p>
<pre><code class="language-fsharp">let Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = async {
  let! hello1 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Tokyo&quot;)   |&gt; Async.AwaitTask
  let! hello2 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Seattle&quot;) |&gt; Async.AwaitTask
  let! hello3 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;London&quot;)  |&gt; Async.AwaitTask
  return [hello1; hello2; hello3]
} |&gt; Async.StartAsTask   </code></pre>
<p>That would work for a normal HTTP trigger, but it blows up for the Orchestrator
trigger because multi-threading operations are not allowed:</p>
<blockquote>
<p>Orchestrator code must never initiate any async operation except by 
using the DurableOrchestrationContext API. The Durable Task Framework 
executes orchestrator code on a single thread and cannot interact with 
any other threads that could be scheduled by other async APIs.</p>
</blockquote>
<p>To solve this issue, we need to keep working with <code>Task</code> directly. This is
not very handy with standard F# libraries. So, I pulled an extra NuGet
package <code>TaskBuilder.fs</code> which provides a <code>task</code> computation expression.</p>
<p>The above function now looks very simple:</p>
<pre><code class="language-fsharp">let Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = task {
  let! hello1 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Tokyo&quot;)
  let! hello2 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;Seattle&quot;)
  let! hello3 = context.CallActivityAsync&lt;string&gt;(&quot;E1_SayHello&quot;, &quot;London&quot;)
  return [hello1; hello2; hello3]
}       </code></pre>
<p>And the best part is that it works just fine.</p>
<p><code>SayHello</code> function is Activity trigger based, and no special effort is required
to implement it in F#:</p>
<pre><code class="language-fsharp">[&lt;FunctionName(&quot;E1_SayHello&quot;)&gt;]
let SayHello([&lt;ActivityTrigger&gt;] name) =
  sprintf &quot;Hello %s!&quot; name</code></pre>
<h2 id="more-examples">More Examples</h2>
<p>Durable Functions repository comes with 
<a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/precompiled">a set of 4 samples</a>
implemented in C#. I took all of those samples and ported them over to F#.</p>
<p>You&#39;ve already seen the first <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/HelloSequence.fs"><code>Hello Sequence</code> sample</a>
above: the orchestrator calls the activity function 3 times and combines the
results. As simple as it looks, the function will actually run 3 times for each
execution, saving state before each subsequent call.</p>
<p>The second <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/BackupSiteContent.fs"><code>Backup Site Content</code> sample</a>
is using this persistence mechanism to run
a potentially slow workflow of copying all files from a given directory to
a backup location. It shows how multiple activities can be executed in
parallel:</p>
<pre><code class="language-fsharp">let tasks = Array.map (fun f -&gt; backupContext.CallActivityAsync&lt;int64&gt;(&quot;E2_CopyFileToBlob&quot;, f)) files
let! results = Task.WhenAll tasks</code></pre>
<p>The third <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/Counter.fs"><code>Counter</code> example</a>
demos a potentially infinite actor-like workflow, where state can exist and
evolve for indefinite period of time. The key API calls are based on
<code>OrchestrationContext</code>:</p>
<pre><code class="language-fsharp">let counterState = counterContext.GetInput&lt;int&gt;()
let! command = counterContext.WaitForExternalEvent&lt;string&gt;(&quot;operation&quot;)</code></pre>
<p>The final elaborate <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/PhoneVerification.fs"><code>Phone Verification</code> workflow</a>
has several twists, like output binding for activity (<code>ICollector</code> is required
instead of C#&#39;s <code>out</code> parameter), third-party integration (Twilio to send SMSs),
recursive sub-function to loop through several attempts and context-based
timers for reliable timeout implementation.</p>
<p>So, if you happen to be an F# fan, you can still give Durable Functions a try.
Be sure to leave your feedback, so that the library could get even better 
before going GA.</p>
]]></content>
    </entry>
    
</feed>