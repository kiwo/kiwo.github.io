<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Mikhail Shilkov</title>
    <link href="https://mikhail.io/feed/" rel="self"/>
    <link href="https://mikhail.io"/>
    <updated>2017-10-26T21:17:48.759Z</updated>
    <id>https://mikhail.io/</id>
    <author>
        <name>Mikhail Shilkov</name>
        <email></email>
    </author>

    
    <entry>
        <title>Azure Function Triggered by Azure Event Grid</title>
        <link href="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid/"/>
        <updated>2017-10-05T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-10-05,/2017/10/azure-function-triggered-by-azure-event-grid/</id>
        <content type="html"><![CDATA[<p><em>Update: I missed the elephant in the room. There actually exists a specialized
trigger for Event Grid binding. In the portal, just select <code>Experimental</code>
in <code>Scenario</code> drop down while creating the function. In precompiled 
functions, reference <code>Microsoft.Azure.WebJobs.Extensions.EventGrid</code> NuGet
package.</em></p>
<p><em>The rest of the article describes my original approach to trigger an
Azure Function from <a href="https://azure.microsoft.com/en-us/services/event-grid/">Azure Event Grid</a> 
with generic Web Hook trigger.</em></p>
<p>Here are the steps to follow:</p>
<h2 id="create-a-function-with-webhook-trigger">Create a Function with Webhook Trigger</h2>
<p>I&#39;m not aware of a specialized trigger type for Event Grid, so
I decided to use Generic Webhook trigger (which is essentially an
HTTP trigger).</p>
<p>I used the Azure Portal to generate a function, so here is the 
<code>function.json</code> that I got:</p>
<pre class="highlight"><code class="hljs json">{
  "<span class="hljs-attribute">bindings</span>": <span class="hljs-value">[
    {
      "<span class="hljs-attribute">type</span>": <span class="hljs-value"><span class="hljs-string">"httpTrigger"</span></span>,
      "<span class="hljs-attribute">direction</span>": <span class="hljs-value"><span class="hljs-string">"in"</span></span>,
      "<span class="hljs-attribute">webHookType</span>": <span class="hljs-value"><span class="hljs-string">"genericJson"</span></span>,
      "<span class="hljs-attribute">name</span>": <span class="hljs-value"><span class="hljs-string">"req"</span>
    </span>},
    {
      "<span class="hljs-attribute">type</span>": <span class="hljs-value"><span class="hljs-string">"http"</span></span>,
      "<span class="hljs-attribute">direction</span>": <span class="hljs-value"><span class="hljs-string">"out"</span></span>,
      "<span class="hljs-attribute">name</span>": <span class="hljs-value"><span class="hljs-string">"res"</span>
    </span>}
  ]</span>,
  "<span class="hljs-attribute">disabled</span>": <span class="hljs-value"><span class="hljs-literal">false</span>
</span>}
</code></pre>
<p>For precompiled functions, just decorate it with <code>HttpTriggerAttribute</code> with
POST method:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> Task&lt;HttpResponseMessage&gt; <span class="hljs-title">Run</span><span class="hljs-params">(
    [HttpTrigger(AuthorizationLevel.Function, <span class="hljs-string">"post"</span>)</span>] HttpRequestMessage req)
</span></code></pre>
<h2 id="parse-the-payload">Parse the Payload</h2>
<p>Events from Event Grid will arrive in a specific predefined JSON format.
Here is an example of events to expect:</p>
<pre class="highlight"><code class="hljs json">[{
  "<span class="hljs-attribute">id</span>": <span class="hljs-value"><span class="hljs-string">"0001"</span></span>,
  "<span class="hljs-attribute">eventType</span>": <span class="hljs-value"><span class="hljs-string">"MyHelloWorld"</span></span>,
  "<span class="hljs-attribute">subject</span>": <span class="hljs-value"><span class="hljs-string">"Hello World!"</span></span>,
  "<span class="hljs-attribute">eventTime</span>": <span class="hljs-value"><span class="hljs-string">"2017-10-05T08:53:07"</span></span>,
  "<span class="hljs-attribute">data</span>": <span class="hljs-value">{
    "<span class="hljs-attribute">hello</span>": <span class="hljs-value"><span class="hljs-string">"world"</span>
  </span>}</span>,
  "<span class="hljs-attribute">topic</span>": <span class="hljs-value"><span class="hljs-string">"/SUBSCRIPTIONS/GUID/RESOURCEGROUPS/NAME/PROVIDERS/MICROSOFT.EVENTGRID/TOPICS/MY-EVENTGRID-TOPIC1"</span>
</span>}]
</code></pre>
<p>To be able to parse those data more easily, I defined a C# class to deserialize
JSON to:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">GridEvent</span>
{
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Id { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> EventType { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Subject { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> DateTime EventTime { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> Dictionary&lt;<span class="hljs-keyword">string</span>, <span class="hljs-keyword">string</span>&gt; Data { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Topic { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
}
</code></pre>
<p>Now, the function can read the events (note, that they are sent in arrays)
from the body of POST request:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">async</span> Task&lt;HttpResponseMessage&gt; <span class="hljs-title">Run</span><span class="hljs-params">(HttpRequestMessage req, TraceWriter log)</span>
</span>{
    <span class="hljs-keyword">string</span> jsonContent = <span class="hljs-keyword">await</span> req.Content.ReadAsStringAsync();
    <span class="hljs-keyword">var</span> events = JsonConvert.DeserializeObject&lt;GridEvent[]&gt;(jsonContent);

    <span class="hljs-comment">// do something with events</span>

    <span class="hljs-keyword">return</span> req.CreateResponse(HttpStatusCode.OK);
}
</code></pre>
<h2 id="validate-the-endpoint">Validate the Endpoint</h2>
<p>To prevent you from sending events to endpoints that you don&#39;t own, Event
Grid requires each subsriber to validate itself. For this purpose, Event
Grid will send events of the special type <code>SubscriptionValidation</code>. </p>
<p>The validation request will contain a code, which we need to echo back in
200-OK HTTP response. </p>
<p>Here is a small piece of code to do just that:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">if</span> (req.Headers.GetValues(<span class="hljs-string">"Aeg-Event-Type"</span>).FirstOrDefault() == <span class="hljs-string">"SubscriptionValidation"</span>)
{
    <span class="hljs-keyword">var</span> code = events[<span class="hljs-number">0</span>].Data[<span class="hljs-string">"validationCode"</span>];
    <span class="hljs-keyword">return</span> req.CreateResponse(HttpStatusCode.OK,
        <span class="hljs-keyword">new</span> { validationResponse = code });
}
</code></pre>
<p>The function is ready!</p>
<h2 id="create-a-custom-event-grid-topic">Create a Custom Event Grid Topic</h2>
<p>To test it out, go to the portal and create a custom Event Grid topic.
Then click on Add Event Subscription button, give it a name and copy paste
the function URL (including key) to Subscriber Endpoint field:</p>
<p><img src="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid//function-url.png" alt="Azure Function URL"></p>
<p><img src="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid//event-subscription.png" alt="Event Grid Subscription"></p>
<p>Creating a subscription will immediately trigger a validation request to
your function, so you should see one invocation in the logs.</p>
<h2 id="send-custom-events">Send Custom Events</h2>
<p>Now, go to your favorite HTTP client (curl, Postman, etc) and send a sample
event to check how the whole setup works:</p>
<pre class="highlight"><code class="hljs http"><span class="hljs-request">POST <span class="hljs-string">/api/events</span> HTTP/1.1</span>
<span class="hljs-attribute">Host</span>: <span class="hljs-string">&lt;your-eventgrid-topic&gt;.westus2-1.eventgrid.azure.net</span>
<span class="hljs-attribute">aeg-sas-key</span>: <span class="hljs-string">&lt;key&gt;</span>
<span class="hljs-attribute">Content-Type</span>: <span class="hljs-string">application/json</span>

<span class="json">[{
  "<span class="hljs-attribute">id</span>": <span class="hljs-value"><span class="hljs-string">"001"</span></span>,
  "<span class="hljs-attribute">eventType</span>": <span class="hljs-value"><span class="hljs-string">"MyHelloWorld"</span></span>,
  "<span class="hljs-attribute">subject</span>": <span class="hljs-value"><span class="hljs-string">"Hello World!"</span></span>,
  "<span class="hljs-attribute">eventTime</span>": <span class="hljs-value"><span class="hljs-string">"2017-10-05T08:53:07"</span></span>,
  "<span class="hljs-attribute">data</span>": <span class="hljs-value">{
    "<span class="hljs-attribute">hello</span>": <span class="hljs-value"><span class="hljs-string">"world"</span>
  </span>}
</span>}]
</span></code></pre>
<p>Obviously, adjust the endpoint and key based on the data from the portal.</p>
<p>You should get a 200-OK back and then see your event in Azure Function 
invocation logs.</p>
<p>Have fun!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Wanted: Effectively-Once Processing in Azure</title>
        <link href="https://mikhail.io/2017/09/wanted-effectively-once-processing-in-azure/"/>
        <updated>2017-09-25T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-09-25,/2017/09/wanted-effectively-once-processing-in-azure/</id>
        <content type="html"><![CDATA[<p><em>This experimental post is a question. The question
is too broad for StackOverflow, so I&#39;m posting it here. Please engage in the
comments section, or forward the link to subject experts.</em></p>
<p>TL;DR: Are there any known patterns / tools / frameworks to provide 
scalable, stateful, effectively-once, end-to-end processing of messages, 
to be hosted in Azure, preferably on PaaS-level of service?</p>
<h2 id="motivational-example">Motivational Example</h2>
<p>Let&#39;s say we are making a TODO app. There is a constant flow of requests
to create a TODO in the system. Each request contains just two fields:
a title and a project ID which TODO should belong to. Here is the definition:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">TodoRequest</span> </span>= {
  ProjectId: int
  Title: string
}
</code></pre>
<p>Now, we want to process the request and assign each TODO an identifier,
which should be an auto-incremented integer. Numeration is unique per project,
so each TODO must have its own combination of <code>ProjectId</code> and <code>Id</code>:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">Todo</span> </span>= {
  ProjectId: int
  Id: int
  Title: string
}
</code></pre>
<p>Now, instead of relying on some database sequences, I want to describe this
transformation as a function. The function has the type <code>(TodoRequest, int) -&gt;
(Todo, int)</code>, i.e. it transforms a tuple of a request and current per-project
state (last generated ID) to a tuple of a TODO and post-processing state:</p>
<pre class="highlight"><code class="hljs fs"><span class="hljs-keyword">let</span> create (request: TodoRequest, state: int) =
  <span class="hljs-keyword">let</span> nextId = state + <span class="hljs-number">1</span>
  <span class="hljs-keyword">let</span> todo = {
    ProjectId = request.ProjectId
    Id = nextId
    Title = request.Title
  }
  todo, nextId
</code></pre>
<p>This is an extremely simple function, and I can use it to great success to
process local, non-durable data.</p>
<p>But if I need to make a reliable distributed application out of it, I need
to take care of lots of things:</p>
<ol>
<li><p>No request should be lost. I need to persist all the requests into 
a durable storage in case of processor crash. </p>
</li>
<li><p>Similarly, I need to persist TODO&#39;s too. Presumably, some downstream 
logic will use the persisted data later on in TODO&#39;s lifecycle.</p>
</li>
<li><p>The state (the counter) must be durable too. In case of crash of processing
function, I want to be able to restart processing after recovery. </p>
</li>
<li><p>Processing of the requests should be sequential per project ID. Otherwise
I might get a clash of ID&#39;s in case two requests belonging to the same 
project are processed concurrently.</p>
</li>
<li><p>I still want requests to different projects to be processed in parallel,
to make sure the system scales up with the growth of project count.</p>
</li>
<li><p>There must be no holes or duplicates in TODO numbering per project, even
in face of system failures. In worst case, I agree to tolerate a duplicated
entry in the output log, but it must be exactly the same entry (i.e. two 
entries with same project id, id and title).</p>
</li>
<li><p>The system should tolerate a permanent failure of any single hardware
dependency and automatically fail-over within reasonable time.</p>
</li>
</ol>
<p>It&#39;s not feasible to meet all of those requirements without relying on some
battle-tested distributed services or frameworks.</p>
<p>Which options do I know of?</p>
<h2 id="transactions">Transactions</h2>
<p>Traditionally, this kind of requirements were solved by using transactions
in something like SQL Server. If I store requests, TODO&#39;s and current ID per
project in the same relational database, I can make each processing step a
single atomic transaction. </p>
<p>This addresses all the concerns, as long as we can stay inside the single 
database. That&#39;s probably a viable option for the TODO app, but less of so
if I convert my toy example to some real applications like IoT data 
processing.</p>
<p>Can we do the same for distributed systems at scale?</p>
<h2 id="azure-event-hubs">Azure Event Hubs</h2>
<p>Since I touched IoT space, the logical choice would be to store our entries
in Azure Event Hubs. That works for many criteria, but I don&#39;t see any available
approach to make such processing consistent in the face of failures.</p>
<p>When processing is done, we need to store 3 pieces: generated TODO event,
current processing offset and current ID. Event goes to another event hub,
processing offset is stored in Blob Storage and ID can be saved to something
like Table Storage. </p>
<p>But there&#39;s no way to store those 3 pieces atomically. Whichever order we 
choose, we are bound to get anomalies in some specific failure modes.</p>
<h2 id="azure-functions">Azure Functions</h2>
<p>Azure Functions don&#39;t solve those problems. But I want to mention this
Function-as-a-Service offering because they provide an ideal programming
model for my use case.</p>
<p>I need to take just one step from my domain function to Azure Function: 
to define bindings for e.g. Event Hubs and Table Storage.</p>
<p>However, reliability guarantees will stay poor. I won&#39;t get neither sequential
processing per Event Hub partition key, nor atomic state commit.</p>
<h2 id="azure-service-fabric">Azure Service Fabric</h2>
<p>Service Fabric sounds like a good candidate service for reliable processing. 
Unfortunately, I don&#39;t have much experience with it to judge.</p>
<p>Please leave a comment if you do.</p>
<h2 id="jvm-world">JVM World</h2>
<p>There are products in JVM world which claim to solve my problem perfectly.</p>
<p>Apache Kafka was the inspiration for Event Hubs log-based messaging. The recent
Kafka release provides effectively-once processing semantics as long as
data stay inside Kafka. Kafka does that with atomic publishing to multiple
topics, and state storage based on compacted topics.</p>
<p>Apache Flink has similar guarantees for its stream processing APIs.</p>
<p>Great, but how do I get such awesomeness in .NET code, and without installing 
expensive ZooKeeper-managed clusters?</p>
<h2 id="call-for-feedback">Call for Feedback</h2>
<p>Do you know a solution, product or service?</p>
<p>Have you developed effectively-once processing on .NET / Azure stack?</p>
<p>Are you in touch with somebody who works on such framework?</p>
<p>Please leave a comment, or ping me on Twitter.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Azure Functions: Are They Really Infinitely Scalable and Elastic?</title>
        <link href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/"/>
        <updated>2017-08-31T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-08-31,/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/</id>
        <content type="html"><![CDATA[<p>Automatic elastic scaling is a built-in feature of Serverless computing
paradigm. One doesn&#39;t have to provision servers anymore, they just need to
write code that will be provisioned on as many servers as needed based on the
actual load. That&#39;s the theory.</p>
<p>In particular, Azure Functions can be hosted on the Consumption plan:</p>
<blockquote>
<p>The Consumption plan automatically allocates compute power when your 
code is running, scales out as necessary to handle load, and then scales 
down when code is not running.</p>
</blockquote>
<p>In this post I will run a simple stress test to get a feel of how such
automatic allocation works in practice and what kind of characteristics 
we can rely on.</p>
<h2 id="setup">Setup</h2>
<p>Here are the parameters that I chose for my test of today:</p>
<ul>
<li>Azure Function written in C# and hosted on Consumption plan</li>
<li>Triggered by Azure Storage Queue binding</li>
<li>Workload is strictly CPU-bound, no I/O is executed</li>
</ul>
<p>Specifically, each queue item represents one password that I need to hash.
Each function call performs 12-round <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hashing. Bcrypt is a slow algorithm recommended for
password hashing, because it makes potential hash collision attacks really 
hard and costly.</p>
<p>My function is based on <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.Net</a>
implementation, and it&#39;s extremely simple:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Run</span><span class="hljs-params">([QueueTrigger(<span class="hljs-string">"bcrypt-password"</span>)</span>] <span class="hljs-keyword">string</span> password)
</span>{
    BCrypt.Net.BCrypt.HashPassword(password, <span class="hljs-number">12</span>);
}
</code></pre>
<p>It turns out that a single execution of this function takes approximately
1 second on an instance of Consumption plan, and consumes 100% CPU during
that second.</p>
<p>Now, the challenge is simple. I send 100,000 passwords
to the queue and see how long it will take to hash them, and also how the
autoscaling will behave. I will run it two times, with different pace of
sending messages to the queue.</p>
<p>That sounds like a perfect job for a Function App on Consumption plan:</p>
<ul>
<li>Needs to scale based on load</li>
<li>CPU intensive - easy to see how busy each server is</li>
<li>Queue-based - easy to see the incoming vs outgoing rate</li>
</ul>
<p>Let&#39;s see how it went.</p>
<h2 id="experiment-1-steady-load">Experiment 1: Steady Load</h2>
<p>In my first run, I was sending messages at constant rate. 100,000 messages
were sent within 2 hours, without spikes or drops in the pace.</p>
<p>Sounds like an easy job for autoscaling facilities. But here is the actual 
chart of data processing:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//FunctionAppScaling.png" alt="Function App Scaling"></p>
<p>The horizontal axis is time in minutes since the first message came in.</p>
<p>The orange line shows the queue backlog - the amount of messages sitting in
the queue at a given moment.</p>
<p>The blue area represents the amount of instances (virtual servers) allocated
to the function by Azure runtime (see the numbers at the right side).</p>
<p>We can divide the whole process into 3 logical segments, approximately 
40 minutes each:</p>
<p><strong>Laging behind</strong>. Runtime starts with 0 instances, and immediately switches
to 1 when the first message comes in. However it&#39;s reluctant to add any more
servers for the next 20 (!) minutes. The scaling heuristic is probably based
on the past history for this queue/function, and it wasn&#39;t busy at all during
the hours before.</p>
<p>After 20 minutes, the runtime starts adding more instances: it goes up to 2, 
then jumps to 4, then reaches 5 at minute 40. The CPU is constantly at 
100% and the queue backlog grows linearly.</p>
<p><strong>Rapid scale up</strong>. After minute 40, it looks like the runtime realizes 
that it needs more power. Much more power! The growth speeds up real quick
and by minute 54 the backlog stops growing, even though the messages are still
coming in. But there are now 21 instances working, which is enough to
finally match and beat the rate of incoming messages.</p>
<p>The runtime doesn&#39;t stop growing though. CPU&#39;s are still at 100%, and the backlog
is still very high, so the scaling goes up and up. The amount of instances
reaches astonishing 55, at which point all the backlog is processed and
there are no messages in the queue.</p>
<p><strong>Searching for balance</strong>. When queue is almost empty and CPU drops below
100% for the first time, the runtime decides to scale down. It does that quickly
and aggressively, switching from 55 to 21 instances in just 2 minutes.</p>
<p>From there it keeps slowly reducing the number of instances until the backlog 
starts growing again. The runtime allows the backlog to grow a bit, but
then figures out a balanced number of servers (17) to keep the backlog flat 
at around 2,000 messages. </p>
<p>It stays at 17 until the producer stops sending new messages. The backlog 
goes to 0, and the amount of instances gradually drops to 0 within 10 minutes.</p>
<p>The second chart from the same experiment looks very similar, but it shows
different metrics:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//FunctionAppDelay.png" alt="Function App Delay"></p>
<p>The gray line is the delay in minutes since the currently processed message
got enqueued (message &quot;age&quot;, in-queue latency). The blue line is the 
total processing rate, measured in messages per minute.</p>
<p>Due to perfect scalability and stability of my function, both charts are almost
exactly the same. I&#39;ve put it here so that you could see that the slowest
message spent more than 40 minutes sitting inside the queue.</p>
<h2 id="experiment-2-spiky-load">Experiment 2: Spiky Load</h2>
<p>With the second run, I tried to emulate a spiky load profile. I was sending
my 100,000 messages throughout 6 hours at lower pace than during the first
run. But sometimes the producer switched to fast mode and sent a bigger bunch
of messages in just several minutes. Here is the actual chart of incoming
message rate:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//SpikyLoad.png" alt="Spiky Load"></p>
<p>It&#39;s easy to imagine some service which has a usage pattern like that, when
spikes of the events happen from time to time, or in rush hours.</p>
<p>This is how the Function App managed to process the messages:</p>
<p><img src="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic//SpikyLoadProcessing.png" alt="Spiky Load Processing Result"></p>
<p>The green line still shows the amount of incoming messages per minute. The 
blue line denotes how many messages were actually processed at that minute.
And the orange bars are queue backlogs - the amount of messages pending.</p>
<p>Here are several observations:</p>
<ul>
<li><p>Obviously, processing latency is way too far from real time. There is
constantly quite a significant backlog in the queue, and processing delay
reaches 20 minutes at peak.</p>
</li>
<li><p>It took the runtime 2 hours to clean the backlog for the first time. Even
without any spikes during the first hour, the autoscaling algorithm needs
time to get up to speed.</p>
</li>
<li><p>Function App runtime is able to scale up quite fast (look at the reaction
on the fourth spike), but it&#39;s not really willing to do that most of the time.</p>
</li>
<li><p>The growth of the backlog after minute 280 is purely caused by wrong
decision of runtime. While the load is completely steady, the runtime
decided to shut down most workers after 20 minutes of empty backlog, and could
not recover for the next hour.</p>
</li>
</ul>
<h2 id="conclusions">Conclusions</h2>
<p>I tried to get a feeling about the ability of Azure Functions to scale
on demand, adapting to the workload. The function under test was purely CPU-bound,
and for that I can give two main conclusions:</p>
<ul>
<li><p>Function Apps are able to scale to high amount of instances running at the
same time, and to eventually process large parallel jobs (at least up to 55
instances).</p>
</li>
<li><p>Significant processing delays are to be expected for heavy loads. Function
App runtime has quite some inertia, and the resulting processing latency can
easily go up to tens of minutes.</p>
</li>
</ul>
<p>If you know how these results can be improved, or why they are less than 
optimal, please leave a comment or contact me directly.</p>
<p>I look forward to conducting more tests in the future!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Authoring a Custom Binding for Azure Functions</title>
        <link href="https://mikhail.io/2017/07/authoring-custom-binding-azure-functions/"/>
        <updated>2017-07-26T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-26,/2017/07/authoring-custom-binding-azure-functions/</id>
        <content type="html"><![CDATA[<p>In my <a href="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/">previous post</a>
I described how I used Durable Functions extensions
in Azure Function App. Durable Functions are using several binding types
that are not part of the standard suite: <code>OrchestrationClient</code>,
<code>OrchestrationTrigger</code>, <code>ActivityTrigger</code>. These custom bindings 
<a href="https://azure.github.io/azure-functions-durable-extension/articles/installation.html">are installed</a>
by copying the corresponding assemblies to a special Extensions folder.</p>
<p>Although Bring-Your-Own-Binding (BYOB) feature hasn&#39;t been released yet, I
decided to follow the path of Durable Functions and create my own 
custom binding.</p>
<h2 id="configuration-binding">Configuration Binding</h2>
<p>I&#39;ve picked a really simple use case for my first experiments with custom
bindings: reading configuration values.</p>
<p>Azure Functions store their configuration values in App Settings (local
runtime uses <code>local.settings.json</code> file for that).</p>
<p>That means, when you need a configuration value inside your C# code,
you normally do</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">string</span> setting = ConfigurationManager.AppSettings[<span class="hljs-string">"MySetting"</span>];
</code></pre>
<p>Alternatively, <code>Environment.GetEnvironmentVariable()</code> method can be used.</p>
<p>When I <a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">needed to collect</a> 
service bus subscription metrics, I wrote this kind of bulky code:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> resourceToScale = ConfigurationManager.AppSettings[<span class="hljs-string">"ResourceToScale"</span>];

<span class="hljs-keyword">var</span> connectionString = ConfigurationManager.AppSettings[<span class="hljs-string">"ServiceBusConnection"</span>];
<span class="hljs-keyword">var</span> topic = ConfigurationManager.AppSettings[<span class="hljs-string">"Topic"</span>];
<span class="hljs-keyword">var</span> subscription = ConfigurationManager.AppSettings[<span class="hljs-string">"Subscription"</span>];
</code></pre>
<p>The code is no rocket science, but it&#39;s tedious to write, so instead I came
up with this idea to define Functions:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">MyFunction</span><span class="hljs-params">(
    [TimerTrigger(<span class="hljs-string">"0 */1 * * * *"</span>)</span>] TimerInfo timer,
    [<span class="hljs-title">Configuration</span><span class="hljs-params">(Key = <span class="hljs-string">"ResourceToScale"</span>)</span>] <span class="hljs-keyword">string</span> resource,
    [Configuration] ServiceBusSubscriptionConfig config)
</span></code></pre>
<p>Note two usages of <code>Configuration</code> attribute. The first one defines the 
specific configuration key, and binds its value to a string parameter. The 
other one binds <em>multiple</em> configuration values to a POCO parameter. I defined
the config class as</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ServiceBusSubscriptionConfig</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">ServiceBusSubscriptionConfig</span><span class="hljs-params">(<span class="hljs-keyword">string</span> serviceBusConnection, <span class="hljs-keyword">string</span> topic, <span class="hljs-keyword">string</span> subscription)</span>
    </span>{
        ServiceBusConnection = serviceBusConnection;
        Topic = topic;
        Subscription = subscription;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> ServiceBusConnection { <span class="hljs-keyword">get</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Topic { <span class="hljs-keyword">get</span>; }
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Subscription { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>The immutable class is a bit verbose, but I still prefer it over get-set
container in this scenario.</p>
<p>The binding behavior is convention-based in this case: the binding engine
should load configuration values based on the names of class properties.</p>
<h2 id="motivation">Motivation</h2>
<p>So, why do I need such binding?</p>
<p>As I said, it&#39;s a simple use case to play with BYOB feature, and overall,
<strong>understand</strong> the internals of Function Apps a bit better.</p>
<p>But apart from that, I removed 4 lines of garbage from the function body
(at the cost of two extra parameters). <strong>Less noise</strong> means more readable code,
especially when I put this code on a webpage.</p>
<p>As a bonus, the <strong>testability</strong> of the function immediately increased. It&#39;s so
much easier for the test just to accept the configuration as input parameter,
instead of fine-tuning the configuration files inside test projects, or
hiding <code>ConfigurationManager</code> usage behind a mockable facade.</p>
<p>Such approach does seem to be the strength of Azure Functions code in
general. It&#39;s often possible to reduce imperative IO-related code to 
attribute-decorated function parameters.</p>
<h2 id="implementing-a-custom-binding">Implementing a Custom Binding</h2>
<p>The actual implementation process of a custom non-trigger binding is quite
simple:</p>
<p><strong>Create a class library</strong> with the word &quot;Extension&quot; in its name. Import
<code>Microsoft.Azure.WebJobs</code> and <code>Microsoft.Azure.WebJobs.Extensions</code> NuGet
packages (at the time of writing I used <code>2.1.0-beta1</code> version).</p>
<p><strong>Define</strong> a class for binding attribute:</p>
<pre class="highlight"><code class="hljs cs">[AttributeUsage(AttributeTargets.Parameter)]
[Binding]
<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ConfigurationAttribute</span> : <span class="hljs-title">Attribute</span>
{
    [AutoResolve]
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Key { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
}
</code></pre>
<p>The attribute is marked as <code>Binding</code> and the <code>Key</code> property is marked as
resolvable from <code>function.json</code>.</p>
<p><strong>Implement</strong> <code>IExtensionConfigProvider</code> which will tell the function runtime
how to use your binding correctly.</p>
<p>The interface has just one method to implement:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ConfigurationExtensionConfigProvider</span> : <span class="hljs-title">IExtensionConfigProvider</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Initialize</span><span class="hljs-params">(ExtensionConfigContext context)</span>
    </span>{
        <span class="hljs-comment">// ... see below</span>
    }
}
</code></pre>
<p>The first step of the implementation is to define a rule for our new
<code>ConfigurationAttribute</code> and tell this rule how to get a string value out
of any attribute instance:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> rule = context.AddBindingRule&lt;ConfigurationAttribute&gt;();
rule.BindToInput&lt;<span class="hljs-keyword">string</span>&gt;(a =&gt; ConfigurationManager.AppSettings[a.Key]);
</code></pre>
<p>That&#39;s really all that needs to happen to bind <code>string</code> parameters.</p>
<p>To make our binding work with any POCO, we need a more elaborate construct:</p>
<pre class="highlight"><code class="hljs cs">rule.BindToInput&lt;Env&gt;(_ =&gt; <span class="hljs-keyword">new</span> Env());
<span class="hljs-keyword">var</span> cm = context.Config.GetService&lt;IConverterManager&gt;();
cm.AddConverter&lt;Env, OpenType, ConfigurationAttribute&gt;(<span class="hljs-keyword">typeof</span>(PocoConverter&lt;&gt;));
</code></pre>
<p>I instruct the rule to bind to my custom class <code>Env</code>, and then I say that
this class <code>Env</code> is convertable to any type (denoted by special <code>OpenType</code>
type argument) with a generic converter called <code>PocoConverter</code>.</p>
<p>The <code>Env</code> class is a bit dummy (it exists just because I need <em>some</em> class):</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">private</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Env</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> <span class="hljs-title">GetValue</span><span class="hljs-params">(<span class="hljs-keyword">string</span> key)</span> </span>=&gt; ConfigurationManager.AppSettings[key];
}
</code></pre>
<p>And <code>PocoConverter</code> is a piece of reflection, that loops through property
names and reads configuration values out of them. Then it calls a constructor
which matches the property count:</p>
<pre class="highlight"><code class="hljs undefined">private class PocoConverter&lt;T&gt; : IConverter&lt;Env, T&gt;
{
    public T Convert(Env env)
    {
        var values = typeof(T)
            .GetProperties()
            .Select(p =&gt; p.Name)
            .Select(env.GetValue)
            .Cast&lt;object&gt;()
            .ToArray();

        var constructor = typeof(T).GetConstructor(values.Select(v =&gt; v.GetType()).ToArray());
        if (constructor == null)
        {
            throw new Exception("We tried to bind to your C# class, but it looks like there's no constructor which accepts all property values");
        }

        return (T)constructor.Invoke(values);
    }
}
</code></pre>
<p>This piece of code is not particularly robust, but it is good enough to
illustrate the concept.</p>
<p>And that&#39;s it, the binding it ready! You can find the complete example in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/tree/master/custom-binding-azure-functions">my github repo</a>.</p>
<h2 id="deploying-custom-bindings">Deploying Custom Bindings</h2>
<p>Since BYOB feature is in early preview, there is no tooling for automated
deployment, and we need to do everything manually. But the process is not
too sophisticated:</p>
<ol>
<li><p>Create a folder for custom bindings, e.g. <code>D:\BindingExtensions</code>.</p>
</li>
<li><p>Set <code>AzureWebJobs_ExtensionsPath</code> parameter in your app settings
to that folder&#39;s path. For local development add a line to <code>local.settings.json</code>:</p>
<pre class="highlight"><code class="hljs undefined"> "AzureWebJobs_ExtensionsPath": "D:\\BindingExtensions",
</code></pre>
</li>
<li><p>Create a subfolder for your extension, e.g. 
<code>D:\BindingExtensions\ConfigurationExtension</code>.</p>
</li>
<li><p>Copy the contents of <code>bin\Debug\</code> of your extension&#39;s class library
to that folder.</p>
</li>
<li><p>Reference your extension library from your Function App.</p>
</li>
</ol>
<p>You are good to go! Decorate your function parameters with the new attribute.</p>
<p>Run the function app locally to try it out. In the console output you should
be able to see something like</p>
<pre class="highlight"><code class="hljs oxygene">Loaded custom <span class="hljs-keyword">extension</span>: ConfigurationExtensionConfigProvider <span class="hljs-keyword">from</span> 
<span class="hljs-string">'D:\BindingExtensions\ConfigurationExtension\MyExtensions.dll'</span>
</code></pre><p>You will be able to debug your extension if needed.</p>
<h2 id="useful-links">Useful Links</h2>
<p>Use the following links to find out more about custom bindings, see more
examples and walkthroughs, and get fresh updates:</p>
<ul>
<li><a href="https://github.com/Azure/azure-webjobs-sdk/wiki/Extensibility">Extensibility in Azure WebJobs SDK</a></li>
<li><a href="https://github.com/Azure/WebJobsExtensionSamples/tree/master/SampleExtension">Sample Extension for Azure Functions</a>,
<a href="https://github.com/Azure/WebJobsExtensionSamples/blob/master/FunctionApp/ReaderFunction.cs">Sample Usage in Precompiled App</a> and
<a href="https://github.com/Azure/WebJobsExtensionSamples/tree/master/ScriptRuntimeSample/Reader">Sample Usage in Script Runtime</a></li>
<li><a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/src/WebJobs.Extensions.DurableTask">Custom Bindings of Durable Functions</a></li>
<li><a href="https://azure.github.io/azure-functions-durable-extension/articles/installation.html">Installation Guide for Durable Functions</a></li>
</ul>
<p>Have a good binding!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Custom Autoscaling with Durable Functions</title>
        <link href="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/"/>
        <updated>2017-07-24T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-24,/2017/07/custom-autoscaling-with-durable-functions/</id>
        <content type="html"><![CDATA[<p>In my previous post 
<a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">Custom Autoscaling of Azure App Service with a Function App</a>
I&#39;ve created a Function App which watches a Service Bus Subscription
backlog and adjusts the scale of App Service based on the observed load.</p>
<p>It works fine but there are two minor issues that I would like to address
in this article:</p>
<ul>
<li><p><strong>Scaling Logic</strong> function from that workflow needs to preserve state
between calls. I used Table Storage bindings for that, which proved to
be a bit verbose and low level: I needed to manage conversion to entity and 
JSON serialization myself;</p>
</li>
<li><p>There is no feedback from <strong>Scaler</strong> function (which executes the change)
back to <strong>Scaling Logic</strong> function. Thus, if scaling operation is slow or
fails, there is no easy way to notify the logic about that.</p>
</li>
</ul>
<p>Let&#39;s see how these issues can be solved with Azure Durable Functions.</p>
<h2 id="meet-durable-functions">Meet Durable Functions</h2>
<p>Microsoft has recently announced the preview of 
<a href="https://azure.github.io/azure-functions-durable-extension/">Durable Functions</a>:</p>
<blockquote>
<p>Durable Functions is an Azure Functions extension for building long-running, 
stateful function orchestrations in code using C# in a serverless environment.</p>
</blockquote>
<p>The library is built on top of <a href="https://github.com/Azure/durabletask">Durable Task Framework</a>
and introduces several patterns for Function coordination and stateful
processing. Please go read the <a href="https://azure.github.io/azure-functions-durable-extension/">documentation</a>,
it&#39;s great and has some very useful examples.</p>
<p>I decided to give Durable Functions a try for my autoscaling workflow. Feel
free to refer to <a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">the first part</a>
to understand my goals and the previous implementation.</p>
<h2 id="architecture">Architecture</h2>
<p>The flow of metric collection, scaling logic and scaling action stays the
same. The state and cross-function communication aspects are now delegated
to Durable Functions, so the diagram becomes somewhat simpler:</p>
<p><img src="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions//AutoscalingArchitecture.png" alt="Autoscaling Architecture"></p>
<p>The blue sign on <strong>Scaling Logic</strong> function denotes its statefulness.</p>
<p>Let&#39;s walk through the functions implementation to see how the workflow
plays out.</p>
<p>This time I&#39;ll start with <strong>Scaler</strong> function and then flow from right to left
to make the explanation more clear.</p>
<h2 id="scaler">Scaler</h2>
<p><strong>Scaler</strong> function applies the scaling decisions to the Azure resource, App
Service Plan in my case. I&#39;ve extracted App Service related code to a helper, 
to keep the function minimal and clean. You can see the full code in 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/durable-functions/DurableScaling.cs">my github repo</a>.</p>
<p><strong>Scaler</strong> function is triggered by Durable Function&#39;s <code>ActivityTrigger</code>. That
basically means that it&#39;s ready to be called from other functions. Here is
the code:</p>
<pre class="highlight"><code class="hljs cs">[FunctionName(nameof(Scaler))]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> <span class="hljs-title">Scaler</span><span class="hljs-params">([ActivityTrigger] DurableActivityContext context)</span>
</span>{
    <span class="hljs-keyword">var</span> action = context.GetInput&lt;ScaleAction&gt;();

    <span class="hljs-keyword">var</span> newCapacity = ScalingHelper.ChangeAppServiceInstanceCount(
        action.ResourceName,
        action.Type == ScaleActionType.Down ? -<span class="hljs-number">1</span> : +<span class="hljs-number">1</span>);

    <span class="hljs-keyword">return</span> newCapacity;
}
</code></pre>
<p>In order to receive an input value, I utilize <code>context.GetInput()</code> method.
I believe that the team is working on support of custom classes 
(<code>ScaleAction</code> in my case) directly as function parameters.</p>
<p>The function then executes the scale change and returns back the new capacity
of App Service Plan. Note that this is new: we were not able to return
values in the previous implementation.</p>
<h2 id="scaling-logic">Scaling Logic</h2>
<p><strong>Scaling Logic</strong> is using <a href="https://azure.github.io/azure-functions-durable-extension/articles/samples/counter.html">Stateful Actor pattern</a>.
One instance of such actor is created for each scalable resource (I only use
1 now). Here is the implementation (again, simplified for readability):</p>
<pre class="highlight"><code class="hljs cs">[FunctionName(nameof(ScalingLogic))]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">async</span> Task&lt;ScalingState&gt; <span class="hljs-title">ScalingLogic</span><span class="hljs-params">(
    [OrchestrationTrigger] DurableOrchestrationContext context, 
    TraceWriter log)</span>
</span>{
    <span class="hljs-keyword">var</span> state = context.GetInput&lt;ScalingState&gt;();

    <span class="hljs-keyword">var</span> metric = <span class="hljs-keyword">await</span> context.WaitForExternalEvent&lt;Metric&gt;(nameof(Metric));

    UpdateHistory(state.History, metric.Value);
    ScaleAction action = CalculateScalingAction(state);

    <span class="hljs-keyword">if</span> (action != <span class="hljs-keyword">null</span>)
    {
        <span class="hljs-keyword">var</span> result = <span class="hljs-keyword">await</span> context.CallFunctionAsync&lt;<span class="hljs-keyword">int</span>&gt;(nameof(Scaler), action);
        log.Info($<span class="hljs-string">"Scaling logic: Scaled to {result} instances."</span>);
        state.LastScalingActionTime = context.CurrentUtcDateTime;
    }

    context.ContinueAsNew(state);
    <span class="hljs-keyword">return</span> state;
}
</code></pre>
<p>Here is how it works:</p>
<ul>
<li><p>Function is bound to <code>OrchestrationTrigger</code>, yet another trigger type from
Durable Functions;</p>
</li>
<li><p>It loads durable state from the received <code>context</code>;</p>
</li>
<li><p>It then waits for an external event called Metric (to be sent by <strong>Collector</strong>
function, see the next section);</p>
</li>
<li><p>When an event is received, the function updates its state and calculates
if a scaling action is warranted;</p>
</li>
<li><p>If yes, it calls <strong>Scaler</strong> function and sends the scale action. It expects
an integer result, denoting the new amount of instances;</p>
</li>
<li><p>It then calls <code>ContinueAsNew</code> method to start a new iteration of the actor
loop, providing the updated state.</p>
</li>
</ul>
<p>One important note: the orchestrated function 
<a href="https://azure.github.io/azure-functions-durable-extension/articles/topics/checkpointing-and-replay.html">has to be deterministic</a>. 
That means, for example, that <code>DateTime.Now</code> is not allowed to be used. 
I use <code>context.CurrentUtcDateTime</code> instead for time-related calculations.</p>
<p>The implementation of this function solves both problems that I mentioned 
in the introduction. We do not manage state storage and serialization manually,
and we now have the ability to get feedback from <strong>Scaler</strong> function.</p>
<h2 id="metrics-collector">Metrics Collector</h2>
<p>I&#39;ve extracted Service Bus related code to a helper to keep the code sample
minimal and clean. You can see the full code in 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/durable-functions/DurableScaling.cs">my github repo</a>.</p>
<p>Here is the remaining implementation of <strong>Metric Collector</strong>:</p>
<pre class="highlight"><code class="hljs cs">[FunctionName(nameof(MetricCollector))]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">async</span> Task <span class="hljs-title">MetricCollector</span><span class="hljs-params">(
    [TimerTrigger(<span class="hljs-string">"0 */1 * * * *"</span>)</span>] TimerInfo myTimer,
    [OrchestrationClient] DurableOrchestrationClient client,
    TraceWriter log)
</span>{
    <span class="hljs-keyword">var</span> resource = Environment.GetEnvironmentVariable(<span class="hljs-string">"ResourceToScale"</span>);

    <span class="hljs-keyword">var</span> status = <span class="hljs-keyword">await</span> client.GetStatusAsync(resource);
    <span class="hljs-keyword">if</span> (status == <span class="hljs-keyword">null</span>)
    {
        <span class="hljs-keyword">await</span> client.StartNewAsync(nameof(ScalingLogic), resource, <span class="hljs-keyword">new</span> ScalingState());
    }
    <span class="hljs-keyword">else</span>
    {
        <span class="hljs-keyword">var</span> metric = ServiceBusHelper.GetSubscriptionMetric(resource);
        log.Info($<span class="hljs-string">"Collector: Current metric value is {metric.Value.Value} at {DateTime.Now}"</span>);
        <span class="hljs-keyword">await</span> client.RaiseEventAsync(resource, nameof(Metric), metric);
    }
}
</code></pre>
<p>It&#39;s still a timer-triggered &quot;normal&quot; (non-durable) function, but now it 
also has an additional binding to <code>OrchestrationClient</code>. This client is used 
to communicate metric data to the <strong>Scaling Logic</strong>.</p>
<p>With the current implementation, <strong>Metric Collector</strong> also has a second
responsibility: actor instance management. At every iteration, it queries
for the current status of corresponding actor. If that is <code>null</code>, Collector
creates a new instance with initial empty state.</p>
<p>To my liking, this aspect is a bit unfortunate, but it seems to be required
with the current implementation of Durable Functions framework. See 
<a href="https://github.com/Azure/azure-functions-durable-extension/issues/21">my related question on github</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>I adjusted the initial flow of autoscaling functions to use Durable Functions
library. It made the state management look more straightforward, and also
allowed direct communication between two functions in strongly-typed
request-response manner.</p>
<p>The resulting code is relatively clear and resembles the typical structure
of async-await code that C# developers are used to.</p>
<p>There are some downsides that I found about Durable Functions too:</p>
<ul>
<li><p>This is a very early preview, so there are some implementation issues.
A couple times I managed to put my functions into a state where they were stuck
and no calls could be made anymore. The only way I could get out of there is by
clearing some blobs in the Storage Account;</p>
</li>
<li><p>The actor instance management story feels raw. The function, which needs to
send events to actors, has to manage their lifecycle and instance IDs. I would
need to add some more checks to make the code production ready, e.g. to 
restart actors if they end up in faulty state;</p>
</li>
<li><p>There are some concurrency issues in function-to-function communication
to be resolved;</p>
</li>
<li><p>Some discipline is required to keep Durable functions side-effect free
and deterministic. The multiple executions caused by awaits and replays are
counter-intuitive (at least for novice devs), and thus error-prone.</p>
</li>
</ul>
<p>Having said that, I believe Durable Functions can be a very useful abstraction
to simplify some of the more advanced scenarios and workflows. I look
forward to further iterations of the library, and I will keep trying it out
for more scenarios.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Custom Autoscaling of Azure App Service with a Function App</title>
        <link href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/"/>
        <updated>2017-07-17T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-17,/2017/07/custom-auto-scaling-in-azure/</id>
        <content type="html"><![CDATA[<p>The power of cloud computing comes from its elasticity and ability to adapt to changing
load. Most Azure services can be scaled up or down manually: by human interaction in the
portal, or by running a command or a script.</p>
<p>Some services in Azure also support Autoscaling, i.e. they may change the resource 
allocation dynamically, based on predefined rules and current operational metrics.</p>
<p>Azure App Service is one example of such service: it supports 
<a href="https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/insights-how-to-scale#scaling-based-on-a-pre-set-metric">Scaling based on a pre-set metric</a>.
This is a powerful option that enables website or webjobs to react on varying load,
e.g. based on CPU utilization.</p>
<p>At the same time, the flexibility of the built-in autoscaling is somewhat
limited:</p>
<ul>
<li><p>Only a handful of metrics is supported: for instance, Service Bus Queues 
are supported as metric source, while Service Bus Subscriptions are not;</p>
</li>
<li><p>It&#39;s not possible to combine several metrics in one rule: e.g. scale down only if
several queues are empty at the same time, not just one of them;</p>
</li>
<li><p>Thresholds are the same for any number of instances: I can&#39;t define
a scale down rule threshold to be 60% for 8 instances but 30% for 2 instances;</p>
</li>
<li><p>The minimum time of reaction is limited to 5 minutes.</p>
</li>
</ul>
<p>Other services, like SQL Database and Cosmos DB, don&#39;t have the built-in autoscaling
functionality at all.</p>
<p>This post starts the series of articles about custom implementation 
of autoscaling. The implementation will be based on Azure Functions as building 
blocks of scaling workflows.</p>
<h2 id="goal">Goal</h2>
<p>To keep the task very specific for now, I want the following from my first 
custom autoscaling implementation:</p>
<ul>
<li><p>Be able to scale the amount of instances up and down in a given App Service 
Plan;</p>
</li>
<li><p>Do so based on the given Service Bus Subscription backlog (amount of messages 
pending to be processed);</p>
</li>
<li><p>Scale up, if the average backlog during any 10 minutes is above a threshold;</p>
</li>
<li><p>Scale down, if the maximum backlog during any 10 minutes is below another 
(lower) threshold;</p>
</li>
<li><p>After scaling up or down, take a cooldown period of 10 minutes;</p>
</li>
<li><p>Have a log of scaling decisions and numbers behind;</p>
</li>
<li><p>Scaling rules should be extensible to allow more complex calculation later 
on.</p>
</li>
</ul>
<h2 id="architecture">Architecture</h2>
<p>I decided that the scaling rules should be written in a general-purpose programming language
(C# for this post), instead of just picking from a limited list of configurations.</p>
<p>I chose Azure Functions as the mechanism to host and run this logic in Azure cloud. </p>
<p>Here is a diagram of Functions that I ended up creating:</p>
<p><img src="https://mikhail.io/2017/07/custom-auto-scaling-in-azure//AutoscalingArchitecture.png" alt="Autoscaling Architecture"></p>
<p>The components of my autoscaling app are:</p>
<ul>
<li><p><strong>Metric Collector</strong> function is based on Timer trigger: it fires every minute and collects
the subscription backlog metric from a given Service Bus Subscription;</p>
</li>
<li><p>Collector then sends this metric to the <strong>Metrics</strong> storage queue;</p>
</li>
<li><p><strong>Scaling Logic</strong> function pulls the metric from the queue. It maintains the 
metric values for 10 minutes, calculates average/maximum value, and if they hit 
thresholds - issues a command to scale App Service Plan up or down;</p>
</li>
<li><p>The command is sent to <strong>Actions</strong> storage queue;</p>
</li>
<li><p><strong>Scaler</strong> function receives the commands from the queue and executes 
the re-scaling action on App Service Plan using Azure Management SDK.</p>
</li>
</ul>
<p>The implementation of this workflow is discussed below. I am using Visual Studio 2017 Version 15.3 
Preview 4.0 to author pre-compiled Azure Functions with nice built-in tooling.</p>
<h2 id="metric-collector">Metric Collector</h2>
<p>First, let&#39;s define <code>MetricValue</code> class, which simply holds time and value:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">MetricValue</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">MetricValue</span><span class="hljs-params">(DateTime time, <span class="hljs-keyword">int</span> <span class="hljs-keyword">value</span>)</span>
    </span>{
        <span class="hljs-keyword">this</span>.Time = time;
        <span class="hljs-keyword">this</span>.Value = <span class="hljs-keyword">value</span>;
    }

    <span class="hljs-keyword">public</span> DateTime Time { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> Value { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>and <code>Metric</code> class which extends the value with resource name (e.g. App Service
Plan name) and measured parameter name:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Metric</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Metric</span><span class="hljs-params">(<span class="hljs-keyword">string</span> resourceName, <span class="hljs-keyword">string</span> name, MetricValue <span class="hljs-keyword">value</span>)</span>
    </span>{
        <span class="hljs-keyword">this</span>.ResourceName = resourceName;
        <span class="hljs-keyword">this</span>.Name = name;
        <span class="hljs-keyword">this</span>.Value = <span class="hljs-keyword">value</span>;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> ResourceName { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> Name { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> MetricValue Value { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>The function definition has two associated bindings: timer trigger (runs every
minute) and return binding to the storage queue:</p>
<pre class="highlight"><code class="hljs cs">[FunctionName(<span class="hljs-string">"MetricCollector"</span>)]
[<span class="hljs-keyword">return</span>: Queue(<span class="hljs-string">"Metrics"</span>)]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> Metric <span class="hljs-title">MetricCollector</span><span class="hljs-params">([TimerTrigger(<span class="hljs-string">"0 */1 * * * *"</span>)</span>] TimerInfo myTimer, TraceWriter log)
</span>{
    <span class="hljs-keyword">var</span> connectionString = Environment.GetEnvironmentVariable(<span class="hljs-string">"ServiceBusConnection"</span>);
    <span class="hljs-keyword">var</span> topic = Environment.GetEnvironmentVariable(<span class="hljs-string">"Topic"</span>);
    <span class="hljs-keyword">var</span> subscription = Environment.GetEnvironmentVariable(<span class="hljs-string">"Subscription"</span>);

    <span class="hljs-keyword">var</span> nsmgr = NamespaceManager.CreateFromConnectionString(connectionString);
    <span class="hljs-keyword">var</span> subscriptionClient = nsmgr.GetSubscription(topic, subscription);
    <span class="hljs-keyword">var</span> backlog = subscriptionClient.MessageCountDetails.ActiveMessageCount;

    log.Info($<span class="hljs-string">"Collector: Current metric value is {backlog}"</span>);

    <span class="hljs-keyword">var</span> resource = Environment.GetEnvironmentVariable(<span class="hljs-string">"ResourceToScale"</span>);
    <span class="hljs-keyword">var</span> <span class="hljs-keyword">value</span> = <span class="hljs-keyword">new</span> MetricValue(DateTime.Now, (<span class="hljs-keyword">int</span>)backlog);
    <span class="hljs-function"><span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title">Metric</span><span class="hljs-params">(resource, $<span class="hljs-string">"{topic}-{subscription}-backlog"</span>, <span class="hljs-keyword">value</span>)</span></span>;
}
</code></pre>
<p>The function executes the following steps:</p>
<ul>
<li>Reads configuration value for Service Bus parameters;</li>
<li>Connects to Service Bus and retrieves <code>ActiveMessageCount</code> for the given 
subscription;</li>
<li>Logs the value for tracing and debugging;</li>
<li>Returns the metric value mentioning which resource it&#39;s intended for.</li>
</ul>
<h2 id="scaling-logic">Scaling Logic</h2>
<p>The core of autoscaling implementation resides in <code>ScalingLogic</code> function. </p>
<p>The function defines 4 (oh my!) bindings:</p>
<ul>
<li>Queue trigger to react on messages from the collector;</li>
<li>Output queue binding to send commands with action to execute;</li>
<li>Combination of input and output bindings to the same row in Table Storage to 
keep the state in between function calls.</li>
</ul>
<p>The bindings are illustrated on the following picture:</p>
<p><img src="https://mikhail.io/2017/07/custom-auto-scaling-in-azure//ScalingLogicBindings.png" alt="Binding of Scaling Logic Function"></p>
<p>And here is the corresponding Function signature:</p>
<pre class="highlight"><code class="hljs cs">[FunctionName(<span class="hljs-string">"ScalingLogic"</span>)]
[<span class="hljs-keyword">return</span>: Queue(<span class="hljs-string">"Actions"</span>)]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> ScaleAction <span class="hljs-title">ScalingLogic</span><span class="hljs-params">(
    [QueueTrigger(<span class="hljs-string">"Metrics"</span>)</span>] Metric metric, 
    [<span class="hljs-title">Table</span><span class="hljs-params">(<span class="hljs-string">"Scaling"</span>, <span class="hljs-string">"{ResourceName}"</span>, <span class="hljs-string">"{Name}"</span>)</span>] ScalingStateEntity stateEntity, 
    [<span class="hljs-title">Table</span><span class="hljs-params">(<span class="hljs-string">"Scaling"</span>, <span class="hljs-string">"{ResourceName}"</span>, <span class="hljs-string">"{Name}"</span>)</span>] <span class="hljs-keyword">out</span> ScalingStateEntity newStateEntity,
    TraceWriter log)
</span></code></pre>
<p>Table storage is partitioned per scalable resource, and state is stored per metric;
thus multiple resources and metrics are supported out of the box.</p>
<p>The function implementation is relatively complex, so I&#39;ll describe it in parts.</p>
<p><code>ScaleAction</code> is a simple message class:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">enum</span> ScaleActionType
{
    Up,
    Down
}

<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ScaleAction</span>
{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">ScaleAction</span><span class="hljs-params">(<span class="hljs-keyword">string</span> resourceName, ScaleActionType type)</span>
    </span>{
        <span class="hljs-keyword">this</span>.ResourceName = resourceName;
        <span class="hljs-keyword">this</span>.Type = type;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> ResourceName { <span class="hljs-keyword">get</span>; }

    <span class="hljs-keyword">public</span> ScaleActionType Type { <span class="hljs-keyword">get</span>; }
}
</code></pre>
<p>Table Storage only allows primitive types for its columns, like strings. 
So I had to create a separate Table Storage entity class:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ScalingStateEntity</span> : <span class="hljs-title">TableEntity</span>
{
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">string</span> SerializedState { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }
}
</code></pre>
<p>which stores serialized state, from the state class itself:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">ScalingState</span>
{
    <span class="hljs-keyword">public</span> List&lt;MetricValue&gt; History { <span class="hljs-keyword">get</span>; } = <span class="hljs-keyword">new</span> List&lt;MetricValue&gt;();

    <span class="hljs-keyword">public</span> DateTime LastScalingActionTime { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; } = DateTime.MinValue;
}
</code></pre>
<p>Now let&#39;s look at the function body. It consists of four blocks. </p>
<p>The first block retrieves the previous values of the metric and logs it too:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 1. Deserialize state</span>
<span class="hljs-keyword">var</span> state = stateEntity?.SerializedState != <span class="hljs-keyword">null</span> 
    ? JsonConvert.DeserializeObject&lt;ScalingState&gt;(stateEntity.SerializedState) 
    : <span class="hljs-keyword">new</span> ScalingState();
<span class="hljs-keyword">var</span> history = state.History;
log.Info($<span class="hljs-string">"Scaling logic: Received {metric.Name}, previous state is {string.Join("</span>, <span class="hljs-string">", history)}"</span>);
</code></pre>
<p>The second block adds the current metric value and removes all metrics which are
not in the target period of 10 minutes anymore:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 2. Add current metric value, remove old values</span>
history.Add(metric.Value);
history.RemoveAll(e =&gt; e.Time &lt; metric.Value.Time.Substract(period));
</code></pre>
<p>Now, the actual logic finally kicks in and produces the scaling action if average
or maximum value is above or below respective thresholds. For my implementation I also
chose to apply this rule after 5th data point. Cooldown period is also respected:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 3. Compare the aggregates to thresholds, produce scaling action if needed</span>
ScaleAction action = <span class="hljs-keyword">null</span>;
<span class="hljs-keyword">if</span> (history.Count &gt;= <span class="hljs-number">5</span>
    &amp;&amp; DateTime.Now - state.LastScalingActionTime &gt; cooldownPeriod)
{
    <span class="hljs-keyword">var</span> average = (<span class="hljs-keyword">int</span>)history.Average(e =&gt; e.Value);
    <span class="hljs-keyword">var</span> maximum = (<span class="hljs-keyword">int</span>)history.Max(e =&gt; e.Value);
    <span class="hljs-keyword">if</span> (average &gt; thresholdUp)
    {
        log.Info($<span class="hljs-string">"Scaling logic: Value {average} is too high, scaling {metric.ResourceName} up..."</span>);
        state.LastScalingActionTime = DateTime.Now;
        action = <span class="hljs-keyword">new</span> ScaleAction(metric.ResourceName, ScaleActionType.Up);
    }
    <span class="hljs-function"><span class="hljs-keyword">else</span> <span class="hljs-title">if</span> <span class="hljs-params">(maximum &lt; thresholdDown)</span>
    </span>{
        log.Info($<span class="hljs-string">"Scaling logic: Value {maximum} is low, scaling {metric.ResourceName} down..."</span>);
        state.LastScalingActionTime = DateTime.Now;
        action = <span class="hljs-keyword">new</span> ScaleAction(metric.ResourceName, ScaleActionType.Down);
    }
}
</code></pre>
<p>Finally, the state is serialized back to table entity and action is returned: </p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// 4. Serialize the state back and return the action</span>
newStateEntity = stateEntity != <span class="hljs-keyword">null</span> 
    ? stateEntity 
    : <span class="hljs-keyword">new</span> ScalingStateEntity { PartitionKey = metric.ResourceName, RowKey = metric.Name };
newStateEntity.SerializedState = JsonConvert.SerializeObject(state);
<span class="hljs-keyword">return</span> action;
</code></pre>
<p>Note, that if no scaling action is warranted, the function simply returns <code>null</code> and no message 
gets sent to the output queue.</p>
<h2 id="scaler">Scaler</h2>
<p>The last function of the workflow is called <code>Scaler</code>: it listens for scaling commands and executes them.
I am using Azure Management Fluent SDK to scale the App Service Plan capacity:</p>
<pre class="highlight"><code class="hljs cs">[FunctionName(<span class="hljs-string">"Scaler"</span>)]
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Scaler</span><span class="hljs-params">([QueueTrigger(<span class="hljs-string">"Actions"</span>)</span>] ScaleAction action, TraceWriter log)
</span>{
    <span class="hljs-keyword">var</span> secrets = Environment.GetEnvironmentVariable(<span class="hljs-string">"ServicePrincipal"</span>).Split(<span class="hljs-string">','</span>);
    <span class="hljs-keyword">var</span> credentials = SdkContext.AzureCredentialsFactory
        .FromServicePrincipal(secrets[<span class="hljs-number">0</span>], secrets[<span class="hljs-number">1</span>], secrets[<span class="hljs-number">2</span>], AzureEnvironment.AzureGlobalCloud);
    <span class="hljs-keyword">var</span> azure = Azure.Configure()
        .Authenticate(credentials)
        .WithDefaultSubscription();

    <span class="hljs-keyword">var</span> plan = azure.AppServices
        .AppServicePlans
        .List()
        .First(p =&gt; p.Name.Contains(action.ResourceName));

    <span class="hljs-keyword">var</span> newCapacity = action.Type == ScaleActionType.Down ? plan.Capacity - <span class="hljs-number">1</span> : plan.Capacity + <span class="hljs-number">1</span>;
    log.Info($<span class="hljs-string">"Scaler: Switching {action.ResourceName} from {plan.Capacity} {action.Type} to {newCapacity}"</span>);

    plan.Update()
        .WithCapacity(newCapacity)
        .Apply();
}
</code></pre>
<p>The functionality is pretty straightforward. Here are some links where you can read more about 
<a href="https://docs.microsoft.com/en-us/dotnet/azure/dotnet-sdk-azure-authenticate?view=azure-dotnet#a-namemgmt-authaazure-management-libraries-for-net-authentication">Authentication in Azure Management libraries</a> 
and <a href="https://github.com/Azure-Samples/app-service-dotnet-scale-web-apps">Managing Web App with Fluent SDK</a>.</p>
<h2 id="conclusion-and-further-steps">Conclusion and Further Steps</h2>
<p>This was quite a lot of code for a single blog post, but most of it was
fairly straightforward. You can find the full implemenation in 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/servicebussubscription-to-appserviceplan/MetricCollector.cs">my github</a>.</p>
<p>Overall, I&#39;ve established an application based on Azure Functions, which
watches the predefined metrics and scales the specified resource up and down
based on target metric values.</p>
<p>The current example works only for the combination of Service Bus Subscription
and App Service Plan, but it is clear how to extend it to more scenarios.</p>
<p>The flexibility of such autoscaling solution exceeds the built-in functionality
that is available in Azure Portal.</p>
<p>The most complex part of my Autoscaling application is the Scaling Logic
function. In the next article of the series, I will refactor it to use
<a href="https://azure.github.io/azure-functions-durable-extension/index.html">Durable Functions</a> - 
the upcoming Orchestration framework for Function Apps.</p>
<p>Stay tuned, and happy scaling!</p>
]]></content>
    </entry>
    
    <entry>
        <title>Sending Large Batches to Azure Service Bus</title>
        <link href="https://mikhail.io/2017/07/sending-large-batches-to-azure-service-bus/"/>
        <updated>2017-07-04T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-07-04,/2017/07/sending-large-batches-to-azure-service-bus/</id>
        <content type="html"><![CDATA[<p>Azure Service Bus client supports sending messages in batches (<code>SendBatch</code>
and <code>SendBatchAsync</code> methods of <code>QueueClient</code> and <code>TopicClient</code>). However,
the size of a single batch must stay below 256k bytes, otherwise the whole
batch will get rejected.</p>
<p>How do we make sure that the batch-to-be-sent is going to fit? The rest 
of this article will try to answer this seemingly simple question.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Given a list of messages of arbitrary type <code>T</code>, we want to send them to Service
Bus in batches. The amount of batches should be close to minimal, but
obviously each one of them must satisfy the restriction of 256k max size.</p>
<p>So, we want to implement a method with the following signature:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages);
</code></pre>
<p>which would work for collections of any size.</p>
<p>To limit the scope, I will restrict the article to the following assumptions:</p>
<ul>
<li><p>Each individual message is less than 256k serialized. If that wasn&#39;t true,
we&#39;d have to put the body into external blob storage first, and then send
the reference. It&#39;s not directly related to the topic of discussion.</p>
</li>
<li><p>I&#39;ll use <code>public BrokeredMessage(object serializableObject)</code> constructor.
Custom serialization could be used, but again, it&#39;s not related to batching,
so I&#39;ll ignore it.</p>
</li>
<li><p>We won&#39;t care about transactions, i.e. if connectivity dies in the middle
of sending the big batch, we might end up with partially sent batch.</p>
</li>
</ul>
<h2 id="messages-of-known-size">Messages of Known Size</h2>
<p>Let&#39;s start with a simple use case: the size of each message is known to us. 
It&#39;s defined by hypothetical <code>Func&lt;T, long&gt; getSize</code> function. Here is a 
helpful extension method that will split an arbitrary collection based on
a metric function and maximum chunk size:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> List&lt;List&lt;T&gt;&gt; ChunkBy&lt;T&gt;(<span class="hljs-keyword">this</span> IEnumerable&lt;T&gt; source, Func&lt;T, <span class="hljs-keyword">long</span>&gt; metric, <span class="hljs-keyword">long</span> maxChunkSize)
{
    <span class="hljs-keyword">return</span> source
        .Aggregate(
            <span class="hljs-keyword">new</span>
            {
                Sum = <span class="hljs-number">0</span>L,
                Current = (List&lt;T&gt;)<span class="hljs-keyword">null</span>,
                Result = <span class="hljs-keyword">new</span> List&lt;List&lt;T&gt;&gt;()
            },
            (agg, item) =&gt;
            {
                <span class="hljs-keyword">var</span> <span class="hljs-keyword">value</span> = metric(item);
                <span class="hljs-keyword">if</span> (agg.Current == <span class="hljs-keyword">null</span> || agg.Sum + <span class="hljs-keyword">value</span> &gt; maxChunkSize)
                {
                    <span class="hljs-keyword">var</span> current = <span class="hljs-keyword">new</span> List&lt;T&gt; { item };
                    agg.Result.Add(current);
                    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> { Sum = <span class="hljs-keyword">value</span>, Current = current, agg.Result };
                }

                agg.Current.Add(item);
                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> { Sum = agg.Sum + <span class="hljs-keyword">value</span>, agg.Current, agg.Result };
            })
        .Result;
}
</code></pre>
<p>Now, the implementation of <code>SendBigBatchAsync</code> is simple:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task <span class="hljs-title">SendBigBatchAsync</span><span class="hljs-params">(IEnumerable&lt;T&gt; messages, Func&lt;T, <span class="hljs-keyword">long</span>&gt; getSize)</span>
</span>{
    <span class="hljs-keyword">var</span> chunks = messages.ChunkBy(getSize, MaxServiceBusMessage);
    <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> chunk <span class="hljs-keyword">in</span> chunks)
    {
        <span class="hljs-keyword">var</span> brokeredMessages = chunk.Select(m =&gt; <span class="hljs-keyword">new</span> BrokeredMessage(m));
        <span class="hljs-keyword">await</span> client.SendBatchAsync(brokeredMessages);
    }
}

<span class="hljs-keyword">private</span> <span class="hljs-keyword">const</span> <span class="hljs-keyword">long</span> MaxServiceBusMessage = <span class="hljs-number">256000</span>;
<span class="hljs-keyword">private</span> <span class="hljs-keyword">readonly</span> QueueClient client;
</code></pre>
<p>Note that I do <code>await</code> for each chunk sequentially to preserve message ordering.
Another thing to notice is that we lost all-or-nothing guarantee: we might
be able to send the first chunk, and then get an exception from subsequent
parts. Some sort of retry mechanism is probably needed.</p>
<h2 id="brokeredmessage-size">BrokeredMessage.Size</h2>
<p>OK, how do we determine the size of each message? How do we implement 
<code>getSize</code> function? </p>
<p><code>BrokeredMessage</code> class exposes <code>Size</code> property, so it might be tempting to
rewrite our method the following way:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages)
{
    <span class="hljs-keyword">var</span> brokeredMessages = messages.Select(m =&gt; <span class="hljs-keyword">new</span> BrokeredMessage(m));
    <span class="hljs-keyword">var</span> chunks = brokeredMessages.ChunkBy(bm =&gt; bm.Size, MaxServiceBusMessage);
    <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> chunk <span class="hljs-keyword">in</span> chunks)
    {
        <span class="hljs-keyword">await</span> client.SendBatchAsync(chunk);
    }
}
</code></pre>
<p>Unfortunately, this won&#39;t work properly. A quote from documentation:</p>
<blockquote>
<p>The value of Size is only accurate after the BrokeredMessage 
instance is sent or received.</p>
</blockquote>
<p>My experiments show that <code>Size</code> of a draft message returns the size of 
the message body, ignoring headers. If the message bodies are large, and
each chunk has just a handful of them, the code might work ok-ish. </p>
<p>But it will significantly underestimate the size of large batches of messages
with small payload.</p>
<p>So, for the rest of this article I&#39;ll try to adjust the calculation for headers.</p>
<h2 id="fixed-header-size">Fixed Header Size</h2>
<p>It could be that the header size of each message is always the same.
Quite often people will set the same headers for all their messages,
or set no custom headers at all. </p>
<p>In this case, you might just measure this size once, and then put this
fixed value inside a configuration file.</p>
<p>Here is how you measure the headers of a <code>BrokeredMessage</code> message:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> sizeBefore = message.Size;
client.Send(message);
<span class="hljs-keyword">var</span> sizeAfter = message.Size;
<span class="hljs-keyword">var</span> headerSize = sizeAfter - sizeBefore;
</code></pre>
<p>Now you just need to adjust one line from the previous version of 
<code>SendBigBatchAsync</code> method</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-keyword">var</span> chunks = brokeredMessages.ChunkBy(bm =&gt; FixedHeaderSize + bm.Size, MaxServiceBusMessage);
</code></pre>
<p><code>FixedHeaderSize</code> might be simply hard-coded, or taken from configuration
per application.</p>
<h2 id="measuring-of-header-size-per-message">Measuring of Header Size per Message</h2>
<p>If the size of headers varies per message, you need a way to adjust batching
algorithm accordingly. </p>
<p>Unfortunately, I haven&#39;t found a straightforward way to accomplish that. It looks like
you&#39;d have to serialize the headers yourself, and then measure the size of
resulting binary. This is not a trivial operation to do correctly,
and also implies some performance penalty.</p>
<p>Sean Feldman <a href="https://weblogs.asp.net/sfeldman/asb-batching-brokered-messages">came up</a> 
with a way to <em>estimate</em> the size of headers. That might be a good way to go,
though the estimation tends to err on the safe side for messages with small
payload.</p>
<h2 id="heuristics-retry">Heuristics &amp; Retry</h2>
<p>The last possibility that I want to consider is actually allow yourself
violating the max size of the batch, but then handle the exception, retry
the send operation and adjust future calculations based on actual measured size
of the failed messages. The size is known after trying to <code>SendBatch</code>, even if
operation failed, so we can use this information.</p>
<p>Here is a sketch of how to do that in code:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-comment">// Sender is reused across requests</span>
<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">BatchSender</span>
{
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">readonly</span> QueueClient queueClient;
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">long</span> batchSizeLimit = <span class="hljs-number">262000</span>;
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">long</span> headerSizeEstimate = <span class="hljs-number">54</span>; <span class="hljs-comment">// start with the smallest header possible</span>

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">BatchSender</span><span class="hljs-params">(QueueClient queueClient)</span>
    </span>{
        <span class="hljs-keyword">this</span>.queueClient = queueClient;
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages)
    {
        <span class="hljs-keyword">var</span> packets = (<span class="hljs-keyword">from</span> m <span class="hljs-keyword">in</span> messages
                     <span class="hljs-keyword">let</span> bm = <span class="hljs-keyword">new</span> BrokeredMessage(m)
                     <span class="hljs-keyword">select</span> <span class="hljs-keyword">new</span> { Source = m, Brokered = bm, BodySize = bm.Size }).ToList();
        <span class="hljs-keyword">var</span> chunks = packets.ChunkBy(p =&gt; <span class="hljs-keyword">this</span>.headerSizeEstimate + p.Brokered.Size, <span class="hljs-keyword">this</span>.batchSizeLimit);
        <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> chunk <span class="hljs-keyword">in</span> chunks)
        {
            <span class="hljs-keyword">try</span>
            {
                <span class="hljs-keyword">await</span> <span class="hljs-keyword">this</span>.queueClient.SendBatchAsync(chunk.Select(p =&gt; p.Brokered));
            }
            <span class="hljs-keyword">catch</span> (MessageSizeExceededException)
            {
                <span class="hljs-keyword">var</span> maxHeader = packets.Max(p =&gt; p.Brokered.Size - p.BodySize);
                <span class="hljs-keyword">if</span> (maxHeader &gt; <span class="hljs-keyword">this</span>.headerSizeEstimate)
                {
                    <span class="hljs-comment">// If failed messages had bigger headers, remember this header size </span>
                    <span class="hljs-comment">// as max observed and use it in future calculations</span>
                    <span class="hljs-keyword">this</span>.headerSizeEstimate = maxHeader;
                }
                <span class="hljs-keyword">else</span>
                {
                    <span class="hljs-comment">// Reduce max batch size to 95% of current value</span>
                    <span class="hljs-keyword">this</span>.batchSizeLimit = (<span class="hljs-keyword">long</span>)(<span class="hljs-keyword">this</span>.batchSizeLimit * <span class="hljs-number">.95</span>);
                }

                <span class="hljs-comment">// Re-send the failed chunk</span>
                <span class="hljs-keyword">await</span> <span class="hljs-keyword">this</span>.SendBigBatchAsync(packets.Select(p =&gt; p.Source));
            }

        }
    }
}
</code></pre>
<p>The code example is quite involved, here is what actually happens:</p>
<ol>
<li><p>Create a brokered message for each message object, but also save the
corresponding source message. This is critical to be able to re-send items:
there&#39;s no way to send the same <code>BrokeredMessage</code> instance twice.</p>
</li>
<li><p>Also save the body size of the brokered message. We&#39;ll use it for retry
calculation.</p>
</li>
<li><p>Start with some guess of header size estimate. I start with 54 bytes, 
which seems to be the minimal header size possible.</p>
</li>
<li><p>Split the batch into chunks the same way we did before.</p>
</li>
<li><p>Try sending chunks one by one.</p>
</li>
<li><p>If send operation fails with <code>MessageSizeExceededException</code>, iterate
through failed items and find out the actual header size of the message.</p>
</li>
<li><p>If that actual size is bigger than our known estimate, increase the estimate
to the newly observed value. Retry sending the chunk (not the whole batch) with
this new setting.</p>
</li>
<li><p>If the header is small, but message size is still too big - reduce the 
allowed total size of the chunk. Retry again.</p>
</li>
</ol>
<p>The combination of checks of steps 7 and 8 should make the mechanism reliable
and self-adopting to message header payloads.</p>
<p>Since we reuse the sender between send operations, the size parameters will
also converge quite quickly and no more retries will be needed. Thus the 
performance overhead should be minimal.</p>
<h2 id="conclusion">Conclusion</h2>
<p>It seems like there is no &quot;one size fits all&quot; solution for this problem at 
the moment. The best implementation might depend on your messaging 
requirements.</p>
<p>But if you have the silver bullet solution, please leave a comment under
this post and answer <a href="https://stackoverflow.com/questions/44779707/split-batch-of-messages-to-be-sent-to-azure-service-bus">my StackOverflow question</a>!</p>
<p>Otherwise, let&#39;s hope that the new 
<a href="https://github.com/azure/azure-service-bus-dotnet">.NET Standard-compatible Service Bus client</a>
will solve this issue for us. Track <a href="https://github.com/Azure/azure-service-bus-dotnet/issues/109">this github issue</a>
for status updates.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Finding Lost Events in Azure Application Insights</title>
        <link href="https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights/"/>
        <updated>2017-06-07T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-06-07,/2017/06/finding-lost-events-in-azure-application-insights/</id>
        <content type="html"><![CDATA[<p>One of the ways we use Azure Application Insights is tracking custom
application-specific events. For instance, every time a data point from an
IoT device comes in, we log an AppInsights event. Then we are able to
aggregate the data and plot charts to derive trends and detect possible 
anomalies.</p>
<p>And recently we found such anomaly, which looked like this:</p>
<p><img src="https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights//dashboard-chart.png" alt="Amount of Events on Dashboard Chart"></p>
<p>This is a chart from our Azure dashboard, which shows the total amount of
events of specific type received per day. </p>
<p>The first two &quot;hills&quot; are two weeks, so we can clearly see that we get more
events on business days compared to weekends.</p>
<p>But then something happened on May 20: we started getting much less events,
and the hill pattern disappeared, days looks much more alike.</p>
<p>We haven&#39;t noticed any other problems in the system, but the trend looked
quite bothering. Are we loosing data?</p>
<p>I headed towards Analytics console of Application Insights to dig deeper.
Here is the query that reproduces the problem:</p>
<pre class="highlight"><code class="hljs stylus">customEvents
| where name == <span class="hljs-string">"EventXReceived"</span>
| where timestamp &gt;= <span class="hljs-function"><span class="hljs-title">ago</span><span class="hljs-params">(<span class="hljs-number">22</span>d)</span></span>
| project PointCount = <span class="hljs-function"><span class="hljs-title">todouble</span><span class="hljs-params">(customMeasurements[<span class="hljs-string">"EventXReceived_Count"</span>])</span></span>, timestamp
| summarize EventXReceived = <span class="hljs-function"><span class="hljs-title">sum</span><span class="hljs-params">(PointCount)</span></span> by <span class="hljs-function"><span class="hljs-title">bin</span><span class="hljs-params">(timestamp, <span class="hljs-number">1</span>d)</span></span>
| render timechart
</code></pre><p>and I got the same chart as before:</p>
<p><img src="https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights//analytics1.png" alt="Trend on Application Insights Analytics"></p>
<p>I checked the history of our source code repository and deployments and I
figured out that we upgraded the version of Application Insights SDK from 
version 2.1 to version 2.3.</p>
<p>My guess at this point was that Application Insights started sampling our
data instead of sending all events to the server. After reading 
<a href="https://docs.microsoft.com/en-us/azure/application-insights/app-insights-sampling">Sampling in Application Insights</a>
article, I came up with the following query to see the sampling rate:</p>
<pre class="highlight"><code class="hljs stylus">customEvents
| where name == <span class="hljs-string">"EventXReceived"</span>
| where timestamp &gt;= <span class="hljs-function"><span class="hljs-title">ago</span><span class="hljs-params">(<span class="hljs-number">22</span>d)</span></span>
| summarize <span class="hljs-number">100</span>/<span class="hljs-function"><span class="hljs-title">avg</span><span class="hljs-params">(itemCount)</span></span> by <span class="hljs-function"><span class="hljs-title">bin</span><span class="hljs-params">(timestamp, <span class="hljs-number">1</span>d)</span></span> 
| render areachart
</code></pre><p>and the result is self-explanatory:</p>
<p><img src="https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights//sampling-rate.png" alt="Sampling Rate"></p>
<p>Clearly, the sampling rate dropped from 100% down to about 30% right when
the anomaly started. The sampling-adjusted query (note <code>itemCount</code> multiplication)</p>
<pre class="highlight"><code class="hljs 1c">customEvents
<span class="hljs-string">| where name == "</span>EventXReceived<span class="hljs-string">"</span>
<span class="hljs-string">| where timestamp &gt;= ago(22d)</span>
<span class="hljs-string">| project PointCount = todouble(customMeasurements["</span>EventXReceived_Count<span class="hljs-string">"]) * itemCount, timestamp</span>
<span class="hljs-string">| summarize EventXReceived = sum(PointCount) by bin(timestamp, 1d)</span>
<span class="hljs-string">| render timechart</span>
</code></pre><p>puts us back to the point when results make sense:</p>
<p><img src="https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights//analytics2.png" alt="Adjusted Trend on Application Insights Analytics"></p>
<p>The third week&#39;s Thursday was bank holiday in several European countries, so 
we got a drop there.</p>
<p>Should Azure dashboard items take sampling into account - to avoid
confusing people and to show more useful charts?</p>
]]></content>
    </entry>
    
    <entry>
        <title>Mikhail.io Upgraded to HTTPS and HTTP/2</title>
        <link href="https://mikhail.io/2017/06/mikhail-io-upgraded-to-https-and-http2/"/>
        <updated>2017-06-06T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-06-06,/2017/06/mikhail-io-upgraded-to-https-and-http2/</id>
        <content type="html"><![CDATA[<p>Starting today, this blog has switched to HTTPS secure protocol:</p>
<p><img src="https://mikhail.io/2017/06/mikhail-io-upgraded-to-https-and-http2//mikhailio-https.png" alt="HTTPS"></p>
<p>While there&#39;s not that much to secure on my blog, HTTPS is still considered
to be a good practice for any site in 2017. One of the benefits that we can
get from it is the usage of HTTP/2 protocol:</p>
<p><img src="https://mikhail.io/2017/06/mikhail-io-upgraded-to-https-and-http2//mikhailio-http2.png" alt="HTTP/2"></p>
<p>This should be beneficial to any reader which uses a modern browser!</p>
<p>Thanks to <a href="https://cloudflare.com">CloudFlare</a> for providing me with free
HTTPS and HTTP/2 support.</p>
]]></content>
    </entry>
    
    <entry>
        <title>Reliable Consumer of Azure Event Hubs</title>
        <link href="https://mikhail.io/2017/05/reliable-consumer-of-azure-event-hubs/"/>
        <updated>2017-05-29T00:00:00.000Z</updated>
        <id>tag:mikhail.io,2017-05-29,/2017/05/reliable-consumer-of-azure-event-hubs/</id>
        <content type="html"><![CDATA[<p><a href="https://azure.microsoft.com/en-us/services/event-hubs/">Azure Event Hubs</a> is
a log-based messaging system-as-a-service in Azure cloud. It&#39;s designed to be able to handle huge
amount of data, and naturally supports multiple consumers.</p>
<h2 id="event-hubs-and-service-bus">Event Hubs and Service Bus</h2>
<p>While Event Hubs are formally part of Azure Service Bus family of products,
in fact its model is quite different.</p>
<p>&quot;Traditional&quot; Service Bus service is organized around queues (subscriptions
are just queues with the topic being the source of messages). Each consumer 
can peek messages from the queue, do the required processing and then
complete the message to remove it from the queue, or abort the processing.
Abortion will leave the message at the queue, or will move it to the Dead Letter
Queue. Completion/abortion are granular per message; and the status of each
message is managed by the Service Bus broker.</p>
<p><img src="https://mikhail.io/2017/05/reliable-consumer-of-azure-event-hubs//service-bus-processors.png" alt="Service Bus Processors"></p>
<p>Event Hubs service is different. Each Hub represnts a log of messages.
Event producer appends data to the end of the log, and consumers can read this log,
but they can&#39;t remove or change the status of events there.</p>
<p>Each event has an offset associated with it. And the only operation that is
supported for consumers is &quot;give me some messages starting at the offset X&quot;.</p>
<p><img src="https://mikhail.io/2017/05/reliable-consumer-of-azure-event-hubs//event-hub-processors.png" alt="Event Hub Processors"></p>
<p>While this approach might seem simplistic, it actually  makes consumers 
more powerful:</p>
<ul>
<li><p>The messages do not disappear from the Hub after being processed for the
first time. So, if needed, the consumer can go back and re-process older
events again;</p>
</li>
<li><p>Multiple consumers are always supported, out of the box. They just read
the same log, after all;</p>
</li>
<li><p>Each consumer can go at its own pace, drop and resume processing whenever
needed, with no effect on other consumers.</p>
</li>
</ul>
<p>There are some disadvantages too:</p>
<ul>
<li><p>Consumers have to manage their own state of the processing progress, i.e.
they have to save the offset of the last processed event;</p>
</li>
<li><p>There is no way to mark any specific event as failed to be able to reprocess
it later. There&#39;s no notion of Dead Letter Queue either.</p>
</li>
</ul>
<h2 id="event-processor-host">Event Processor Host</h2>
<p>To overcome the first complication, Microsoft provides the consumer API called
<a href="https://github.com/Microsoft/azure-docs/blob/master/articles/event-hubs/event-hubs-programming-guide.md">EventProcessorHost</a>.
This API has an implementation of consumers based on checkpointing. All you
need to do is to provide a callback to process a batch of events, and then call
<code>CheckpointAsync</code> method, which saves the current offset of the last message
into Azure Blob Storage. If the consumer restarts at any point in time, it will
read the last checkpoint to find the current offset, and will then continue
processing from that point on.</p>
<p>It works great for some scenarios, but the event delivery/processing guarantees
are relatively low in this case:</p>
<ul>
<li><p>Any failures are ignored: there&#39;s no retry or Dead Letter Queue</p>
</li>
<li><p>There are no transactions between event hub checkpoints and the data sinks
that the processor works with (i.e. data stores where processed messages 
end up at)</p>
</li>
</ul>
<p>In this post I want to focus on a way to process events with higher consistency
requirements, in particular:</p>
<ul>
<li><p>Event Hub processor modifies data in a SQL Database, and such 
processing is transactional per batch of messages</p>
</li>
<li><p>Each event should be (successfully) processed exactly once</p>
</li>
<li><p>If event processing failed, it should be marked as failed and kept 
available to be reprocessed at later point in time</p>
</li>
</ul>
<p>While end-to-end exactly-once processing would require changes of the 
producers too, we will only focus on consumer side in this post.</p>
<h2 id="transactional-checkpoints-in-sql">Transactional Checkpoints in SQL</h2>
<p>If checkpoint information is stored in Azure Blobs, there is no obvious way to
implement distributed transactions between SQL Database and Azure Storage.</p>
<p>However, we can override the default checkpointing mechanism and 
implement our own checkpoints based on a SQL table. This way each 
checkpoint update can become part of a SQL transaction and be committed
or rolled back with normal guarantees provided by SQL Server.</p>
<p>Here is a table that I created to hold my checkpoints:</p>
<pre class="highlight"><code class="hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> EventHubCheckpoint (
  Topic <span class="hljs-built_in">varchar</span>(<span class="hljs-number">100</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  PartitionID <span class="hljs-built_in">varchar</span>(<span class="hljs-number">100</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  SequenceNumber <span class="hljs-built_in">bigint</span> <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  Offset <span class="hljs-built_in">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  <span class="hljs-keyword">CONSTRAINT</span> PK_EventHubCheckpoint <span class="hljs-keyword">PRIMARY</span> <span class="hljs-keyword">KEY</span> CLUSTERED (Topic, PartitionID)
)
</span></code></pre>
<p>For each topic and partition of Event Hubs, we store two values: sequence
number and offset, which together uniquely identify the consumer position.</p>
<p>Conveniently, Event Host Processor provides an extensibility point to
override the default checkpoint manager with a custom one. For that we
need to implement <code>ICheckpointManager</code> interface to work with our SQL
table.</p>
<p>The implementation mainly consists of 3 methods: <code>CreateCheckpointIfNotExistsAsync</code>,
<code>GetCheckpointAsync</code> and <code>UpdateCheckpointAsync</code>. The names are pretty
much self-explanatory, and my Dapper-based implementation is quite trivial.
You can find the code <a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/eventhubs-sqlcheckpoints/SQLCheckpointManager.cs">here</a>.</p>
<p>For now, I&#39;m ignoring the related topic of lease management and corresponding
interface <code>ILeaseManager</code>. It&#39;s quite a subject on its own; for the sake
of simplicity I&#39;ll assume we have just one consumer process per partition,
which makes proper lease manager redundand.</p>
<h2 id="dead-letter-queue">Dead Letter Queue</h2>
<p>Now, we want to be able to mark some messages as failed and to 
re-process them later. To make Dead Letters transactional, we need another
SQL table to hold the failed events:</p>
<pre class="highlight"><code class="hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> EventHubDeadLetter (
  Topic <span class="hljs-built_in">varchar</span>(<span class="hljs-number">100</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  PartitionID <span class="hljs-built_in">varchar</span>(<span class="hljs-number">100</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  SequenceNumber <span class="hljs-built_in">bigint</span> <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  Offset <span class="hljs-built_in">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  FailedAt datetime <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  Error <span class="hljs-keyword">nvarchar</span>(<span class="hljs-keyword">max</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,
  <span class="hljs-keyword">CONSTRAINT</span> PK_EventHubDeadLetter <span class="hljs-keyword">PRIMARY</span> <span class="hljs-keyword">KEY</span> CLUSTERED (Topic, PartitionID)
)
</span></code></pre>
<p>This table looks very similar to <code>EventHubCheckpoint</code> that I
defined above. That is because they are effectively storing pointers to
events in a hub. Dead Letters have two additional columns to store error
timestamp and text.</p>
<p>There is no need to store the message content, because failed events still
sit in the event hub anyway. You could still log it for diagnostics purpose - just make an extra
<code>varbinary</code> column.</p>
<p>There&#39;s no notion of dead letters in Event Hubs SDK, so I defined my own
interface <code>IDeadLetterManager</code> with a single <code>AddFailedEvents</code> method:</p>
<pre class="highlight"><code class="hljs undefined">public interface IDeadLetterManager
{
    Task AddFailedEvents(IEnumerable&lt;DeadLetter&lt;EventData&gt;&gt; deadLetters);
}

public class DeadLetter&lt;T&gt;
{
    public T Data { get; set; }
    public DateTime FailureTime { get; set; }
    public Exception Exception { get; set; }
}
</code></pre>
<p>Dapper-based implementation is trivial again, you can find the code 
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/eventhubs-sqlcheckpoints/SQLDeadLetterManager.cs">here</a>.</p>
<h2 id="putting-it-together-event-host">Putting It Together: Event Host</h2>
<p>My final solution is still using <code>EventHostProcessor</code>. I pass <code>SQLCheckpointManager</code> 
into its constructor, and then I implement <code>IEventProcessor</code>&#39;s 
<code>ProcessEventsAsync</code> method in the following way:</p>
<ol>
<li>Instantiate a list of items to store failed events</li>
<li>Start a SQL transaction</li>
<li>Loop through all the received events in the batch</li>
<li>Process each item inside a try-catch block</li>
<li>If exception happens, add the current event to the list of failed events</li>
<li>After all items are processed, save failed events to Dead Letter table</li>
<li>Update the checkpoint pointer</li>
<li>Commit the transaction</li>
</ol>
<p>The code block that illustrates this workflow:</p>
<pre class="highlight"><code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task <span class="hljs-title">ProcessEventsAsync</span><span class="hljs-params">(
    PartitionContext context, 
    IEnumerable&lt;EventData&gt; eventDatas)</span>
</span>{
    <span class="hljs-comment">// 1. Instantiate a list of items to store failed events</span>
    <span class="hljs-keyword">var</span> failedItems = <span class="hljs-keyword">new</span> List&lt;DeadLetter&lt;EventData&gt;&gt;();

    <span class="hljs-comment">// 2. Start a SQL transaction</span>
    <span class="hljs-keyword">using</span> (<span class="hljs-keyword">var</span> scope = <span class="hljs-keyword">new</span> TransactionScope())
    {
        <span class="hljs-comment">// 3. Loop through all the received events in the batch</span>
        <span class="hljs-keyword">foreach</span> (<span class="hljs-keyword">var</span> eventData <span class="hljs-keyword">in</span> eventDatas)
        {
            <span class="hljs-keyword">try</span>
            {
                <span class="hljs-comment">// 4. Process each item inside a try-catch block</span>
                <span class="hljs-keyword">var</span> item = <span class="hljs-keyword">this</span>.Deserialize(eventData);
                <span class="hljs-keyword">await</span> <span class="hljs-keyword">this</span>.DoWork(item);
            }
            <span class="hljs-keyword">catch</span> (Exception ex)
            {
                <span class="hljs-comment">// 5. Add a failed event to the list</span>
                failedItems.Add(<span class="hljs-keyword">new</span> DeadLetter&lt;EventData&gt;(eventData, DateTime.UtcNow, ex));
            }
        }

        <span class="hljs-keyword">if</span> (failedItems.Any())
        {
            <span class="hljs-comment">// 6. Save failed items to Dead Letter table</span>
            <span class="hljs-keyword">await</span> <span class="hljs-keyword">this</span>.dlq.AddFailedEvents(failedItems);
        }

        <span class="hljs-comment">// 7. Update the checkpoint pointer</span>
        <span class="hljs-keyword">await</span> context.CheckpointAsync();

        <span class="hljs-comment">// 8. Commit the transaction</span>
        scope.Complete();
    }
}
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>My implementation of Event Hubs consumer consists of 3 parts: checkpoint 
manager that saves processing progress per partition into a SQL table;
dead letter manager that persists information about processing errors;
and an event host which uses both to provide transactional processing
of events.</p>
<p>The transaction scope is limited to SQL Server databases, but it might be
sufficient for many real world scenarios.</p>
]]></content>
    </entry>
    
</feed>